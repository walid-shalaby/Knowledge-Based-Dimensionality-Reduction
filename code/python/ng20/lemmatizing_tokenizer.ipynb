{
 "metadata": {
  "name": "",
  "signature": "sha256:36e5a57644c03fffaf4dd52a23427b396898b3cd84c3cc28c1b8c8b31202e2b2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Tokenizer with Lemmatizer"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "A tokenizer that lemmatize tokenized documents text using nltk wordnet lemmatizer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# alphanumeric tokenizer\n",
      "from nltk import RegexpTokenizer\n",
      "class RawTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
      "    def __call__(self, doc):\n",
      "        return [t for t in self.tokenizer.tokenize(doc)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# alphanumeric tokenizer\n",
      "from nltk import RegexpTokenizer\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "class RawLemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# alphabetic tokenizer\n",
      "from nltk import RegexpTokenizer\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b[a-z]+\\\\-*[a-z]+|\\\\b(?u)\\\\b[a-z]\\\\b')\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}