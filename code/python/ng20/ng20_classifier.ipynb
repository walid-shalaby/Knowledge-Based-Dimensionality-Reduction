{
 "metadata": {
  "name": "",
  "signature": "sha256:14068b9806d5b9fab65b6f28a76c529b1fcdea6adf2ccd1ae8d7b9deea711071"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classify 20ng docs"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Classifiy 20ng docs using unigrams and bigrams features with different classifiers and report classification results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size):\n",
      "    # tokenize text\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "    from sklearn.feature_extraction.text import TfidfTransformer\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    # generate corpus vectors\n",
      "    vectorizer = CountVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),vocabulary=vocabulary,stop_words={})\n",
      "    corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "    \n",
      "    print 'vectorizing done, {0} terms vocabulary tokenized'.format(len(vectorizer.vocabulary_))\n",
      "    \n",
      "    # generate tfidf vectors\n",
      "    transformer = TfidfTransformer()\n",
      "    corpus_tfidf_vectors = transformer.fit_transform(corpus_vectors)\n",
      "\n",
      "    return corpus_tfidf_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-151-743655218ee8>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size):\n"
       ]
      }
     ],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize_corpus_new(corpus,tokenizer,vocabulary,max_ngram_size):\n",
      "    # tokenize text\n",
      "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    # generate corpus vectors\n",
      "    vectorizer = TfidfVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),vocabulary=vocabulary,stop_words={})\n",
      "    corpus_tfidf_vectors = vectorizer.fit_transform(corpus)\n",
      "    \n",
      "    print 'vectorizing done, {0} terms vocabulary tokenized'.format(len(vectorizer.vocabulary_))\n",
      "    \n",
      "    return corpus_tfidf_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-152-f88374f887f0>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def vectorize_corpus_new(corpus,tokenizer,vocabulary,max_ngram_size):\n"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# given classifier predictions probabilities, return predictions with top n probabilities > 0.5 for each instance or greatest one if all are <=0.5\n",
      "def get_max_n_pred(pred_proba, n_pred, threshold):\n",
      "    import heapq\n",
      "    import numpy\n",
      "    max_n_pred = numpy.ndarray(shape=pred_proba.shape)\n",
      "    for i in range(len(pred_proba)):\n",
      "        largest_n_proba = heapq.nlargest(n_pred,pred_proba[i])\n",
      "        max_n_pred[i] = numpy.array(((pred_proba[i]>threshold) & (pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1))\n",
      "        if max_n_pred[i].sum(axis=0)==0: # at least one label should be returned\n",
      "            max_n_pred[i] = numpy.array(((pred_proba[i]>=max(pred_proba[i])) & 1))\n",
      "    return max_n_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reference: http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification\n",
      "def classify(x_train,y_train,x_test,y_test,max_labels):\n",
      "    from sklearn.preprocessing import MultiLabelBinarizer\n",
      "    from sklearn.multiclass import OneVsRestClassifier\n",
      "    from sklearn.svm import SVC\n",
      "    from sklearn.svm import LinearSVC\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "    from sklearn.naive_bayes import MultinomialNB\n",
      "    from sklearn.naive_bayes import BernoulliNB\n",
      "    from sklearn.preprocessing import binarize\n",
      "    from sklearn.cross_validation import train_test_split\n",
      "    from sklearn import metrics\n",
      "    import numpy\n",
      "        \n",
      "    # binarize the labels\n",
      "    #mlb = MultiLabelBinarizer()\n",
      "    #y_train_binarized = mlb.fit_transform(y_train)\n",
      "    \n",
      "    # train/test split\n",
      "    #corpus_tfidf_vectors, labels_binarized = shuffle(corpus_tfidf_vectors, labels_binarized)\n",
      "    #x_train, x_test, y_train, y_test = train_test_split(corpus_tfidf_vectors, labels_binarized, test_size=test_size, random_state=1)\n",
      "    \n",
      "    # classify\n",
      "    #cls = OneVsRestClassifier(LogisticRegression(class_weight='auto'))\n",
      "    #cls = OneVsRestClassifier(LogisticRegression())\n",
      "    #cls = MultinomialNB(alpha=0.01)\n",
      "    #cls = OneVsRestClassifier(BernoulliNB()need binarize(x_train and x_test))\n",
      "    #cls = OneVsRestClassifier(SVC(kernel='linear',probability=True,max_iter=1000))\n",
      "    #cls = OneVsRestClassifier(SVC(kernel='linear'))\n",
      "    cls = LinearSVC()\n",
      "    cls.fit(x_train, y_train)\n",
      "    y_train_pred = cls.predict(x_train)\n",
      "    print 'training F1: ', metrics.f1_score(y_train, y_train_pred, average='micro')\n",
      "    print 'training accuracy: ', metrics.accuracy_score(y_train, y_train_pred)\n",
      "    #pred_proba = 1/(1+numpy.exp(-1*cls.decision_function(x_test)))\n",
      "    #threshold = 0.45#1/(1+numpy.exp(-1))\n",
      "    #y_pred = mlb.inverse_transform(get_max_n_pred(pred_proba, max_labels,threshold))\n",
      "    #y_pred = mlb.inverse_transform(cls.predict(x_test))\n",
      "    y_pred = cls.predict(x_test)\n",
      "    print 'testing accuracy: ', metrics.f1_score(y_test, y_pred)\n",
      "    # evaluate\n",
      "    #pred_proba = cls.predict_proba(x_test)\n",
      "    #print len(pred_proba[0]) # make sure it is 121\n",
      "    #actual_labels = mlb.inverse_transform(y_test)\n",
      "    #precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred, pos_label=0)\n",
      "    #print precision\n",
      "    #print recall\n",
      "    #print thresholds\n",
      "    return {'precision':metrics.precision_score(y_test, y_pred, average='micro'),\n",
      "            'recall':metrics.recall_score(y_test, y_pred, average='micro'),\n",
      "            'f1':metrics.f1_score(y_test, y_pred, average='micro')}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn\n",
      "from sklearn.svm import LinearSVC\n",
      "c = LinearSVC()\n",
      "print sklearn.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.15.2\n"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_all_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import RawTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 1\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = None\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}{1}_unigrams{2}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-156-5bcc0f6a848c>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_all_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 1\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-157-094e349cf724>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_lemmatized_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = None\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}{1}_bigrams{2}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-158-c28207c92f80>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-159-781cd4f52dc5>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_lemmatized_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_bigrams_with_LSA(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens,num_components):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    from sklearn.decomposition import TruncatedSVD\n",
      "    from scipy import sparse\n",
      "    import numpy\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # apply LSA\n",
      "    #print numpy.max(corpus_train_tfidf_vectors)\n",
      "    #print numpy.min(corpus_train_tfidf_vectors)\n",
      "    lsa = TruncatedSVD(n_components=num_components)\n",
      "    lsa.fit(corpus_train_tfidf_vectors)\n",
      "    #corpus_train_tfidf_vectors = numpy.dot(corpus_train_tfidf_vectors,pca.components_.transpose())\n",
      "    corpus_train_tfidf_vectors = lsa.transform(corpus_train_tfidf_vectors)\n",
      "    corpus_test_tfidf_vectors = lsa.transform(corpus_test_tfidf_vectors)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print 'LSA ^' , vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-160-0ae0e12f66cc>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_lemmatized_bigrams_with_LSA(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens,num_components):\n"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer    \n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 1\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()    \n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()    \n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)    \n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-161-d27651528d70>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_stemmed_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 161
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer    \n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-162-8129eac551f8>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_stemmed_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_wiki_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')\n",
      "    print 'done loading vocabulary'\n",
      "    \n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-163-180bdc332cbe>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_lemmatized_wiki_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 163
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary_extend_unigrams\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    if len(bigrams_src)==1:\n",
      "        vocabulary_tbl_intersect = '{0}_bigrams'.format(bigrams_src[0])\n",
      "    else:\n",
      "        vocabulary_tbl_intersect = '{0}_'.format(bigrams_src[0])\n",
      "        for i in range(len(bigrams_src)-1):\n",
      "            vocabulary_tbl_intersect = '{0}{1}_'.format(vocabulary_tbl_intersect,bigrams_src[i+1])\n",
      "        vocabulary_tbl_intersect = '{0}bigrams_vw'.format(vocabulary_tbl_intersect)\n",
      "        \n",
      "    vocabulary = load_common_vocabulary_extend_unigrams(vocabulary_tbl_name,vocabulary_tbl_intersect,'lemma')\n",
      "    print 'done loading vocabulary'\n",
      "    \n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name,'^',vocabulary_tbl_intersect,'(extended unigrams) --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-164-cd1ea993b277>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_lemmatized_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()    \n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()    \n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiktionary_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')\n",
      "    print 'done loading vocabulary'\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-165-98c5bdcd6b4f>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_lemmatized_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'google_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')    \n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-166-1c5602c28486>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_lemmatized_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 166
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_wiktionary_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-167-c5ecc72d6e7c>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_lemmatized_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_wiki_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_google_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-168-60ce875247ce>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_lemmatized_wiki_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 168
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiktionary_google_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-169-2010f39a430e>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_lemmatized_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 169
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_wiktionary_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-170-a08ffa055f62>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_stemmed_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_wiki_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_google_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-171-781d3020ecf2>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_stemmed_wiki_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 171
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiktionary_google_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-172-69e08236a4c4>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_stemmed_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_wiktionary_google_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-173-bdd713c1f2ac>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_lemmatized_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 173
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_wiki_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()    \n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()    \n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')    \n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-174-6b79430f2923>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_stemmed_wiki_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 174
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary_extend_unigrams\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    if len(bigrams_src)==1:\n",
      "        vocabulary_tbl_intersect = '{0}_bigrams'.format(bigrams_src[0])\n",
      "    else:\n",
      "        vocabulary_tbl_intersect = '{0}_'.format(bigrams_src[0])\n",
      "        for i in range(len(bigrams_src)-1):\n",
      "            vocabulary_tbl_intersect = '{0}{1}_'.format(vocabulary_tbl_intersect,bigrams_src[i+1])\n",
      "        vocabulary_tbl_intersect = '{0}bigrams_vw'.format(vocabulary_tbl_intersect)\n",
      "        \n",
      "    vocabulary = load_common_vocabulary_extend_unigrams(vocabulary_tbl_name,vocabulary_tbl_intersect,'stem')\n",
      "    print 'done loading vocabulary'\n",
      "    \n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name,'^',vocabulary_tbl_intersect,'(extended unigrams) --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-175-a650503cc609>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_stemmed_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiktionary_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-176-3444b80fb7e1>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_stemmed_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 176
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'google_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-177-dd4a635e1842>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_stemmed_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 177
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_wiktionary_google_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<input>:1: SyntaxWarning: import * only allowed at module level\n",
        "<ipython-input-178-ce43e0732809>:1: SyntaxWarning: import * only allowed at module level\n",
        "  def test_stemmed_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):\n"
       ]
      }
     ],
     "prompt_number": 178
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test():\n",
      "    from ng20_corpus_loader import load_corpus_and_labels\n",
      "    from ng20_corpus_loader import load_corpus_with_labels_mappings\n",
      "    \n",
      "    # load 20ng docs with class lables from DB\n",
      "    corpus_train_data = load_corpus_and_labels('train')\n",
      "    print 'done loading {0} train records and {1} labels.'.format(len(corpus_train_data['corpus']),len(corpus_train_data['labels_dic']))\n",
      "\n",
      "    corpus_test_data = load_corpus_with_labels_mappings('test',corpus_train_data['labels_dic'])\n",
      "    print 'done loading {0} test records.'.format(len(corpus_test_data['corpus']))\n",
      "    \n",
      "    stopwords_removal_mask = 1\n",
      "    chi_features_mask = 2\n",
      "    raw_tokens_mask = 4\n",
      "    for i in range(4,6): # test w/o stopword removal and w/o chi square features and w/o raw tokens\n",
      "        stopwords_removal = i&stopwords_removal_mask==stopwords_removal_mask\n",
      "        use_chi_features = i&chi_features_mask==chi_features_mask\n",
      "        use_raw_tokens = i&raw_tokens_mask==raw_tokens_mask\n",
      "        \n",
      "        test_all_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_all_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        #test_lemmatized_bigrams_with_LSA({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "        #                         {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "        #                         stopwords_removal,use_chi_features,use_raw_tokens,113240)\n",
      "        \n",
      "        test_lemmatized_wiki_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['wiki'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_wiki_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['wiki','wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_wiki_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['wiki','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_wiktionary_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_all_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['wiki','wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_wiki_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['wiki'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_wiki_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['wiki','wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_wiki_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['wiki','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_wiktionary_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_all_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['wiki','wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 stopwords_removal,use_chi_features,use_raw_tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 179
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test using all vocabulary\n",
      "test()\n",
      "\n",
      "print 'done!'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loaded 11314 records.\n",
        "done loading 11314 train records and 20 labels.\n",
        "loaded 7532 records."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading 7532 test records.\n",
        "loaded (173735) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 173735 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 173735 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy:  0.85152031107\n",
        "ng20_raw_unigrams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  -->  precision  0.852761550717 recall  0.852761550717 f1  0.852761550717\n",
        "loaded (1664448) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 1664448 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 1664448 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999646455719\n",
        "training accuracy:  0.999646455719\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.854250223123\n",
        "ng20_raw_bigrams  -->  precision  0.855549654806 recall  0.855549654806 f1  0.855549654806\n",
        "loaded (166463) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 166463 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 166463 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998939367156\n",
        "training accuracy:  0.998939367156\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.851900381097\n",
        "ng20_raw_lemmas_unigrams_df1_tf1  -->  precision  0.85302708444 recall  0.85302708444 f1  0.85302708444\n",
        "loaded (1593710) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 1593710 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 1593710 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999558069648\n",
        "training accuracy:  0.999558069648\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.856587356745\n",
        "ng20_raw_lemmas_bigrams_df1_tf1  -->  precision  0.85780669145 recall  0.85780669145 f1  0.85780669145\n",
        "loaded (221958) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 221958 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 221958 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.854968003236\n",
        "ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_bigrams  -->  precision  0.856213489113 recall  0.856213489113 f1  0.856213489113\n",
        "loaded (55495) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (74491) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 74491 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 74491 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998762595015\n",
        "training accuracy:  0.998762595015\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.831122773171\n",
        "ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_bigrams (extended unigrams) -->  precision  0.832182687201 recall  0.832182687201 f1  0.832182687201\n",
        "loaded (177564) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 177564 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 177564 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.853232255337\n",
        "ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiktionary_bigrams  -->  precision  0.854487519915 recall  0.854487519915 f1  0.854487519915\n",
        "loaded (11101) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (17030) terms\n",
        "done loading vocabulary\n",
        "vectorizing done, 17030 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 17030 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.993989747216\n",
        "training accuracy:  0.993989747216\n",
        "testing accuracy:  0.757930826192\n",
        "ng20_raw_lemmas_bigrams_df1_tf1 ^ wiktionary_bigrams (extended unigrams) -->  precision  0.759426447159 recall  0.759426447159 f1  0.759426447159\n",
        "loaded (197617) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 197617 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 197617 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999027753226\n",
        "training accuracy:  0.999027753226\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.853684205673\n",
        "ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ google_bigrams  -->  precision  0.854753053638 recall  0.854753053638 f1  0.854753053638\n",
        "loaded (31154) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (39297) terms\n",
        "done loading vocabulary\n",
        "vectorizing done, 39297 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 39297 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.996376171115\n",
        "training accuracy:  0.996376171115\n",
        "testing accuracy:  0.795363129736\n",
        "ng20_raw_lemmas_bigrams_df1_tf1 ^ google_bigrams (extended unigrams) -->  precision  0.796999468933 recall  0.796999468933 f1  0.796999468933\n",
        "loaded (224956) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 224956 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 224956 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.855419040974\n",
        "ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_wiktionary_bigrams_vw  -->  precision  0.856611789697 recall  0.856611789697 f1  0.856611789697\n",
        "loaded (58493) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (77918) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 77918 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 77918 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998939367156\n",
        "training accuracy:  0.998939367156\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.831642117863\n",
        "ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_wiktionary_bigrams_vw (extended unigrams) -->  precision  0.832713754647 recall  0.832713754647 f1  0.832713754647\n",
        "loaded (238169) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 238169 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 238169 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.855343761508\n",
        "ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_google_bigrams_vw  -->  precision  0.856611789697 recall  0.856611789697 f1  0.856611789697\n",
        "loaded (71706) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (91464) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 91464 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 91464 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998762595015\n",
        "training accuracy:  0.998762595015\n",
        "testing accuracy:  0.834753231208\n",
        "ng20_raw_lemmas_bigrams_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ wiki_google_bigrams_vw (extended unigrams) -->  precision  0.83590015932 recall  0.83590015932 f1  0.83590015932\n",
        "loaded (204528) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 204528 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 204528 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.854363093896\n",
        "ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiktionary_google_bigrams_vw  -->  precision  0.855549654806 recall  0.855549654806 f1  0.855549654806\n",
        "loaded (38065) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (48205) terms\n",
        "done loading vocabulary\n",
        "vectorizing done, 48205 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 48205 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.997260031819\n",
        "training accuracy:  0.997260031819\n",
        "testing accuracy:  0.802482914993\n",
        "ng20_raw_lemmas_bigrams_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.804036112586 recall  0.804036112586 f1  0.804036112586\n",
        "loaded (241012) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 241012 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 241012 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy:  0.854499692007"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw  -->  precision  0.855682421668 recall  0.855682421668 f1  0.855682421668\n",
        "loaded (74549) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (94709) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 94709 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 94709 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998939367156\n",
        "training accuracy:  0.998939367156\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.834318646413\n",
        "ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.835369091875 recall  0.835369091875 f1  0.835369091875\n",
        "loaded (149349) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 149349 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 149349 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999027753226\n",
        "training accuracy:  0.999027753226\n",
        "testing accuracy:  0.849878666344\n",
        "ng20_raw_stems_unigrams_df1_tf1  -->  precision  0.851301115242 recall  0.851301115242 f1  0.851301115242\n",
        "loaded (1491727) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 1491727 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 1491727 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999558069648\n",
        "training accuracy:  0.999558069648\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.857898856235\n",
        "ng20_raw_stems_bigrams_df1_tf1  -->  precision  0.859134360064 recall  0.859134360064 f1  0.859134360064\n",
        "loaded (208752) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 208752 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 208752 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.852631551636\n",
        "ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiki_bigrams  -->  precision  0.853823685608 recall  0.853823685608 f1  0.853823685608\n",
        "loaded (59403) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (76544) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 76544 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 76544 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998850981085\n",
        "training accuracy:  0.998850981085\n",
        "testing accuracy:  0.83151661676\n",
        "ng20_raw_stems_bigrams_df1_tf1 ^ wiki_bigrams (extended unigrams) -->  precision  0.832580987785 recall  0.832580987785 f1  0.832580987785\n",
        "loaded (160445) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 160445 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 160445 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy:  0.851792934038\n",
        "ng20_raw_stems_unigrams_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiktionary_bigrams  -->  precision  0.85302708444 recall  0.85302708444 f1  0.85302708444\n",
        "loaded (11096) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (16351) terms\n",
        "done loading vocabulary\n",
        "vectorizing done, 16351 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 16351 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.993371044723\n",
        "training accuracy:  0.993371044723\n",
        "testing accuracy:  0.761199316267\n",
        "ng20_raw_stems_bigrams_df1_tf1 ^ wiktionary_bigrams (extended unigrams) -->  precision  0.762347318109 recall  0.762347318109 f1  0.762347318109\n",
        "loaded (185015) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 185015 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 185015 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999027753226\n",
        "training accuracy:  0.999027753226\n",
        "testing accuracy:  0.851367177532\n",
        "ng20_raw_stems_unigrams_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ ng20_raw_stems_bigrams_df1_tf1 ^ google_bigrams  -->  precision  0.852628783856 recall  0.852628783856 f1  0.852628783856\n",
        "loaded (35666) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (43305) terms\n",
        "done loading vocabulary\n",
        "vectorizing done, 43305 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 43305 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.997083259678\n",
        "training accuracy:  0.997083259678\n",
        "testing accuracy:  0.801081937773\n",
        "ng20_raw_stems_bigrams_df1_tf1 ^ google_bigrams (extended unigrams) -->  precision  0.802310143388 recall  0.802310143388 f1  0.802310143388\n",
        "loaded (211394) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 211394 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 211394 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.853341182005\n",
        "ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiki_wiktionary_bigrams_vw  -->  precision  0.854487519915 recall  0.854487519915 f1  0.854487519915\n",
        "loaded (62045) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (79424) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 79424 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 79424 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998850981085\n",
        "training accuracy:  0.998850981085\n",
        "testing accuracy:  0.83259097254\n",
        "ng20_raw_stems_bigrams_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ wiki_wiktionary_bigrams_vw (extended unigrams) -->  precision  0.833643122677 recall  0.833643122677 f1  0.833643122677\n",
        "loaded (227804) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 227804 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 227804 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.854128670739\n",
        "ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiki_google_bigrams_vw  -->  precision  0.855284121083 recall  0.855284121083 f1  0.855284121083\n",
        "loaded (78455) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (96203) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 96203 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 96203 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998850981085\n",
        "training accuracy:  0.998850981085\n",
        "testing accuracy:  0.834021435262\n",
        "ng20_raw_stems_bigrams_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ wiki_google_bigrams_vw (extended unigrams) -->  precision  0.835103558152 recall  0.835103558152 f1  0.835103558152\n",
        "loaded (191637) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 191637 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 191637 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy:  0.852058757566\n",
        "ng20_raw_stems_unigrams_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiktionary_google_bigrams_vw  -->  precision  0.853292618163 recall  0.853292618163 f1  0.853292618163\n",
        "loaded (42288) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (51312) terms\n",
        "done loading vocabulary\n",
        "vectorizing done, 51312 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 51312 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.99752519003\n",
        "training accuracy:  0.99752519003\n",
        "testing accuracy:  0.803740632413\n",
        "ng20_raw_stems_bigrams_df1_tf1 ^ wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.805098247477 recall  0.805098247477 f1  0.805098247477\n",
        "loaded (230266) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 230266 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 230266 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.85388795935\n",
        "ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw  -->  precision  0.855018587361 recall  0.855018587361 f1  0.855018587361\n",
        "loaded (80917) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (98882) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 98882 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 98882 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998850981085\n",
        "training accuracy:  0.998850981085\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.834469264275\n",
        "ng20_raw_stems_bigrams_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.835501858736 recall  0.835501858736 f1  0.835501858736\n",
        "loaded (173377) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 173377 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 173377 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999027753226\n",
        "training accuracy:  0.999027753226\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.849918141902\n",
        "ng20_raw_unigrams_stopwords  -->  precision  0.85116834838 recall  0.85116834838 f1  0.85116834838\n",
        "loaded (1726872) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 1726872 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 1726872 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999558069648\n",
        "training accuracy:  0.999558069648\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.853838849212\n",
        "ng20_raw_bigrams_stopwords  -->  precision  0.854885820499 recall  0.854885820499 f1  0.854885820499\n",
        "loaded (166113) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 166113 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 166113 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998939367156\n",
        "training accuracy:  0.998939367156\n",
        "testing accuracy:  0.850863938269\n",
        "ng20_raw_lemmas_unigrams_stopwords_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  -->  precision  0.85209771641 recall  0.85209771641 f1  0.85209771641\n",
        "loaded (1659720) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 1659720 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 1659720 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999469683578\n",
        "training accuracy:  0.999469683578\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.85782979594\n",
        "ng20_raw_lemmas_bigrams_stopwords_df1_tf1  -->  precision  0.858868826341 recall  0.858868826341 f1  0.858868826341\n",
        "loaded (217211) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 217211 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 217211 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999027753226\n",
        "training accuracy:  0.999027753226\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.854699570117\n",
        "ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_bigrams  -->  precision  0.855815188529 recall  0.855815188529 f1  0.855815188529\n",
        "loaded (51098) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (68835) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 68835 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 68835 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998585822874\n",
        "training accuracy:  0.998585822874\n",
        "testing accuracy:  0.829849569546\n",
        "ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_bigrams (extended unigrams) -->  precision  0.830987785449 recall  0.830987785449 f1  0.830987785449\n",
        "loaded (174244) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 174244 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 174244 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999027753226\n",
        "training accuracy:  0.999027753226\n",
        "testing accuracy:  0.85250174613\n",
        "ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiktionary_bigrams  -->  precision  0.853690918747 recall  0.853690918747 f1  0.853690918747\n",
        "loaded (8131) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (13305) terms\n",
        "done loading vocabulary\n",
        "vectorizing done, 13305 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 13305 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.989393671557\n",
        "training accuracy:  0.989393671557\n",
        "testing accuracy:  0.749631311532\n",
        "ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiktionary_bigrams (extended unigrams) -->  precision  0.751194901753 recall  0.751194901753 f1  0.751194901753\n",
        "loaded (206782) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 206782 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 206782 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999027753226\n",
        "training accuracy:  0.999027753226\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.853606222734\n",
        "ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ google_bigrams  -->  precision  0.854753053638 recall  0.854753053638 f1  0.854753053638\n",
        "loaded (40669) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (49080) terms\n",
        "done loading vocabulary\n",
        "vectorizing done, 49080 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 49080 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.996552943256\n",
        "training accuracy:  0.996552943256\n",
        "testing accuracy:  0.794613185016\n",
        "ng20_raw_lemmas_bigrams_stopwords_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ google_bigrams (extended unigrams) -->  precision  0.79580456718 recall  0.79580456718 f1  0.79580456718\n",
        "loaded (218249) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 218249 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 218249 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999027753226\n",
        "training accuracy:  0.999027753226\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.854988740849\n",
        "ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_bigrams_vw  -->  precision  0.856080722252 recall  0.856080722252 f1  0.856080722252\n",
        "loaded (52136) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (70081) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 70081 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 70081 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998585822874\n",
        "training accuracy:  0.998585822874\n",
        "testing accuracy:  0.829504180158\n",
        "ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_bigrams_vw (extended unigrams) -->  precision  0.830722251726 recall  0.830722251726 f1  0.830722251726\n",
        "loaded (241441) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 241441 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 241441 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999027753226\n",
        "training accuracy:  0.999027753226\n",
        "testing accuracy:  0.854614327616\n",
        "ng20_raw_lemmas_unigrams_stopwords_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_google_bigrams_vw  -->  precision  0.855682421668 recall  0.855682421668 f1  0.855682421668\n",
        "loaded (75328) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (94090) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 94090 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 94090 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998585822874\n",
        "training accuracy:  0.998585822874\n",
        "testing accuracy:  0.833328935059\n",
        "ng20_raw_lemmas_bigrams_stopwords_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ wiki_google_bigrams_vw (extended unigrams) -->  precision  0.834572490706 recall  0.834572490706 f1  0.834572490706\n",
        "loaded (210546) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 210546 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 210546 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999027753226\n",
        "training accuracy:  0.999027753226\n",
        "testing accuracy:  0.853497449751\n",
        "ng20_raw_lemmas_unigrams_stopwords_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiktionary_google_bigrams_vw  -->  precision  0.854620286776 recall  0.854620286776 f1  0.854620286776\n",
        "loaded (44433) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (54188) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 54188 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 54188 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.997171645749\n",
        "training accuracy:  0.997171645749\n",
        "testing accuracy:  0.799654846349\n",
        "ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.800982474774 recall  0.800982474774 f1  0.800982474774\n",
        "loaded (242282) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 242282 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 242282 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999027753226\n",
        "training accuracy:  0.999027753226\n",
        "testing accuracy:  0.854637019318\n",
        "ng20_raw_lemmas_unigrams_stopwords_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw  -->  precision  0.855682421668 recall  0.855682421668 f1  0.855682421668\n",
        "loaded (76169) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (95111) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 95111 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 95111 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998674208945\n",
        "training accuracy:  0.998674208945\n",
        "testing accuracy:  0.832815199713\n",
        "ng20_raw_lemmas_bigrams_stopwords_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ wiki_wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.834041423261 recall  0.834041423261 f1  0.834041423261\n",
        "loaded (149113) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 149113 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 149113 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998939367156\n",
        "training accuracy:  0.998939367156\n",
        "testing accuracy:  0.848088379258\n",
        "ng20_raw_stems_unigrams_stopwords_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  -->  precision  0.849442379182 recall  0.849442379182 f1  0.849442379182\n",
        "loaded (1587711) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 1587711 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 1587711 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999469683578\n",
        "training accuracy:  0.999469683578\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.854999406034\n",
        "ng20_raw_stems_bigrams_stopwords_df1_tf1  -->  precision  0.856080722252 recall  0.856080722252 f1  0.856080722252\n",
        "loaded (206785) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 206785 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 206785 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy:  0.852745397829\n",
        "ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_bigrams  -->  precision  0.853956452469 recall  0.853956452469 f1  0.853956452469\n",
        "loaded (57672) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (73779) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 73779 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 73779 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998674208945\n",
        "training accuracy:  0.998674208945\n",
        "testing accuracy:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.831033865366\n",
        "ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_bigrams (extended unigrams) -->  precision  0.832182687201 recall  0.832182687201 f1  0.832182687201\n",
        "loaded (157884) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 157884 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 157884 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998939367156\n",
        "training accuracy:  0.998939367156\n",
        "testing accuracy:  0.849978642814\n",
        "ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiktionary_bigrams  -->  precision  0.851301115242 recall  0.851301115242 f1  0.851301115242\n",
        "loaded (8771) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (13555) terms\n",
        "done loading vocabulary\n",
        "vectorizing done, 13555 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 13555 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.990277532261\n",
        "training accuracy:  0.990277532261\n",
        "testing accuracy:  0.752614349629\n",
        "ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiktionary_bigrams (extended unigrams) -->  precision  0.753451938396 recall  0.753451938396 f1  0.753451938396\n",
        "loaded (196881) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 196881 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 196881 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy:  0.85034641162\n",
        "ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ google_bigrams  -->  precision  0.851566648964 recall  0.851566648964 f1  0.851566648964\n",
        "loaded (47768) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (55661) terms\n",
        "done loading vocabulary\n",
        "vectorizing done, 55661 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 55661 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.997171645749\n",
        "training accuracy:  0.997171645749\n",
        "testing accuracy:  0.796985755978\n",
        "ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ google_bigrams (extended unigrams) -->  precision  0.798194370685 recall  0.798194370685 f1  0.798194370685\n",
        "loaded (207882) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 207882 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 207882 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy:  0.852609590719\n",
        "ng20_raw_stems_unigrams_stopwords_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_bigrams_vw  -->  precision  0.853823685608 recall  0.853823685608 f1  0.853823685608\n",
        "loaded (58769) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (75015) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 75015 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 75015 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998674208945\n",
        "training accuracy:  0.998674208945\n",
        "testing accuracy:  0.830058912054\n",
        "ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_bigrams_vw (extended unigrams) -->  precision  0.831253319172 recall  0.831253319172 f1  0.831253319172\n",
        "loaded (235766) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 235766 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 235766 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.853807289936\n",
        "ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_google_bigrams_vw  -->  precision  0.855018587361 recall  0.855018587361 f1  0.855018587361\n",
        "loaded (86653) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (103595) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 103595 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 103595 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998762595015\n",
        "training accuracy:  0.998762595015\n",
        "testing accuracy:  0.831120821597\n",
        "ng20_raw_stems_bigrams_stopwords_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ wiki_google_bigrams_vw (extended unigrams) -->  precision  0.832182687201 recall  0.832182687201 f1  0.832182687201\n",
        "loaded (200920) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 200920 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 200920 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999204525367\n",
        "training accuracy:  0.999204525367\n",
        "testing accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.850500430917\n",
        "ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiktionary_google_bigrams_vw  -->  precision  0.851699415826 recall  0.851699415826 f1  0.851699415826\n",
        "loaded (51807) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (60709) terms\n",
        "done loading vocabulary\n",
        "vectorizing done, 60709 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 60709 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.99743680396\n",
        "training accuracy:  0.99743680396\n",
        "testing accuracy:  0.801298420246\n",
        "ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.80244291025 recall  0.80244291025 f1  0.80244291025\n",
        "loaded (236636) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 236636 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 236636 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999116139296\n",
        "training accuracy:  0.999116139296\n",
        "testing accuracy:  0.853636332415\n",
        "ng20_raw_stems_unigrams_stopwords_df1_tf1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw  -->  precision  0.854885820499 recall  0.854885820499 f1  0.854885820499\n",
        "loaded (87523) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extended to (104579) terms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done loading vocabulary\n",
        "vectorizing done, 104579 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vectorizing done, 104579 terms vocabulary tokenized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training F1: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.998762595015\n",
        "training accuracy:  0.998762595015\n",
        "testing accuracy:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.831049851064\n",
        "ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.832182687201 recall  0.832182687201 f1  0.832182687201\n",
        "done!\n"
       ]
      }
     ],
     "prompt_number": 180
    }
   ],
   "metadata": {}
  }
 ]
}