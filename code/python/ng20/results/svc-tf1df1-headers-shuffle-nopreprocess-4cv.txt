IPython Notebookng20_classifier-4cv (autosaved)
File
Edit
View
Insert
Cell
Kernel
Help
 Cell Toolbar:
Classify 20ng docs
Classifiy 20ng docs using unigrams and bigrams features with different classifiers and report classification results
In [61]:

def vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size):
    # tokenize text
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer
    from ng20_globals import *
    
    # generate corpus vectors
    vectorizer = CountVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),vocabulary=vocabulary,stop_words={})
    corpus_vectors = vectorizer.fit_transform(corpus)
    
    print 'vectorizing done, {0} terms vocabulary tokenized'.format(len(vectorizer.vocabulary_))
    
    # generate tfidf vectors
    transformer = TfidfTransformer()
    corpus_tfidf_vectors = transformer.fit_transform(corpus_vectors)
 
    return corpus_tfidf_vectors
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-61-743655218ee8>:1: SyntaxWarning: import * only allowed at module level
  def vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size):
In [62]:

def vectorize_corpus_new(corpus,tokenizer,vocabulary,max_ngram_size):
    # tokenize text
    from sklearn.feature_extraction.text import TfidfVectorizer
    from ng20_globals import *
    
    # generate corpus vectors
    vectorizer = TfidfVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),vocabulary=vocabulary,stop_words={})
    corpus_tfidf_vectors = vectorizer.fit_transform(corpus)
    
    print 'vectorizing done, {0} terms vocabulary tokenized'.format(len(vectorizer.vocabulary_))
    
    return corpus_tfidf_vectors
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-62-f88374f887f0>:1: SyntaxWarning: import * only allowed at module level
  def vectorize_corpus_new(corpus,tokenizer,vocabulary,max_ngram_size):
In [63]:

# given classifier predictions probabilities, return predictions with top n probabilities > 0.5 for each instance or greatest one if all are <=0.5
def get_max_n_pred(pred_proba, n_pred, threshold):
    import heapq
    import numpy
    max_n_pred = numpy.ndarray(shape=pred_proba.shape)
    for i in range(len(pred_proba)):
        largest_n_proba = heapq.nlargest(n_pred,pred_proba[i])
        max_n_pred[i] = numpy.array(((pred_proba[i]>threshold) & (pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1))
        if max_n_pred[i].sum(axis=0)==0: # at least one label should be returned
            max_n_pred[i] = numpy.array(((pred_proba[i]>=max(pred_proba[i])) & 1))
    return max_n_pred
In [64]:

# reference: http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification
def classify(x_train,y_train,x_test,y_test,max_labels):
    from sklearn.preprocessing import MultiLabelBinarizer
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.svm import SVC
    from sklearn.svm import LinearSVC
    from sklearn.linear_model import LogisticRegression
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.naive_bayes import BernoulliNB
    from sklearn.preprocessing import binarize
    from sklearn.cross_validation import train_test_split
    from sklearn import metrics
    import numpy
    from numpy import mean
    import scipy
    from sklearn.cross_validation import cross_val_score
    from sklearn.metrics import make_scorer
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import f1_score
    
    # combine train and test vectors
    #print 'merging training and testing samples x({0}),x({1})'.format(x_train.shape,x_test.shape)
    #print 'merging training and testing samples y({0}),y({1})'.format(len(y_train),len(y_test))
    x = scipy.sparse.vstack((x_train,x_test))
    y = y_train + y_test
    #print 'merged into x({0})'.format(x.shape)
    #print 'merged into y({0})'.format(len(y))
    # binarize the labels
    #mlb = MultiLabelBinarizer()
    #y_train_binarized = mlb.fit_transform(y_train)
    
    # train/test split
    #corpus_tfidf_vectors, labels_binarized = shuffle(corpus_tfidf_vectors, labels_binarized)
    #x_train, x_test, y_train, y_test = train_test_split(corpus_tfidf_vectors, labels_binarized, test_size=test_size, random_state=1)
    
    # classify
    #cls = OneVsRestClassifier(LogisticRegression(class_weight='auto'))
    #cls = OneVsRestClassifier(LogisticRegression())
    #cls = MultinomialNB(alpha=0.01)
    #cls = OneVsRestClassifier(BernoulliNB()need binarize(x_train and x_test))
    #cls = OneVsRestClassifier(SVC(kernel='linear',probability=True,max_iter=1000))
    cls = LinearSVC()
    acc_scores = cross_val_score(cls,x,y,scoring='accuracy',cv=4,n_jobs=-1)
    print 'accuracy scores = {0},{1}'.format(acc_scores,mean(acc_scores))
    macro_p_scores = cross_val_score(cls,x,y,scoring=make_scorer(precision_score,average='macro'),cv=4,n_jobs=-1)
    print 'macro precision scores = {0},{1}'.format(macro_p_scores,mean(macro_p_scores))
    macro_r_scores = cross_val_score(cls,x,y,scoring=make_scorer(recall_score,average='macro'),cv=4,n_jobs=-1)
    print 'macro recall scores = {0},{1}'.format(macro_r_scores,mean(macro_r_scores))
    macro_f1_scores = cross_val_score(cls,x,y,scoring=make_scorer(f1_score,average='macro'),cv=4,n_jobs=-1)
    print 'macro f1 scores = {0},{1}'.format(macro_f1_scores,mean(macro_f1_scores))
    p_scores = cross_val_score(cls,x,y,scoring=make_scorer(precision_score,average='weighted'),cv=4,n_jobs=-1)
    print 'weighted average precision scores = {0},{1}'.format(p_scores,mean(p_scores))
    r_scores = cross_val_score(cls,x,y,scoring=make_scorer(precision_score,average='weighted'),cv=4,n_jobs=-1)
    print 'weighted average recall scores = {0},{1}'.format(r_scores,mean(r_scores))
    f1_scores = cross_val_score(cls,x,y,scoring=make_scorer(f1_score,average='weighted'),cv=4,n_jobs=-1)
    print 'weighted f1 scores = {0},{1}'.format(f1_scores,mean(f1_scores))
   
    return {'precision':mean(macro_p_scores),
            'recall':mean(macro_r_scores),
            'f1':mean(macro_f1_scores)}
In [65]:

import sklearn
from sklearn.svm import LinearSVC
c = LinearSVC()
print sklearn.__version__
0.15.2
In [66]:

def test_all_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import RawTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    
    max_ngram_size = 1
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = None
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}{1}_unigrams{2}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern)
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-66-5bcc0f6a848c>:1: SyntaxWarning: import * only allowed at module level
  def test_all_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [67]:

def test_lemmatized_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    
    max_ngram_size = 1
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-67-094e349cf724>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [68]:

def test_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    
    max_ngram_size = 2
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = None
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}{1}_bigrams{2}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern)
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-68-c28207c92f80>:1: SyntaxWarning: import * only allowed at module level
  def test_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [69]:

def test_lemmatized_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    
    max_ngram_size = 2
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-69-781cd4f52dc5>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [70]:

def test_lemmatized_bigrams_with_LSA(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens,num_components):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    from sklearn.decomposition import TruncatedSVD
    from scipy import sparse
    import numpy
    
    max_ngram_size = 2
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # apply LSA
    #print numpy.max(corpus_train_tfidf_vectors)
    #print numpy.min(corpus_train_tfidf_vectors)
    lsa = TruncatedSVD(n_components=num_components)
    lsa.fit(corpus_train_tfidf_vectors)
    #corpus_train_tfidf_vectors = numpy.dot(corpus_train_tfidf_vectors,pca.components_.transpose())
    corpus_train_tfidf_vectors = lsa.transform(corpus_train_tfidf_vectors)
    corpus_test_tfidf_vectors = lsa.transform(corpus_test_tfidf_vectors)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print 'LSA ^' , vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-70-0ae0e12f66cc>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_bigrams_with_LSA(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens,num_components):
In [71]:

def test_stemmed_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer    
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    
    max_ngram_size = 1
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()    
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()    
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)    
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-71-d27651528d70>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [72]:

def test_stemmed_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer    
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-72-8129eac551f8>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [73]:

def test_lemmatized_wiki_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_bigrams'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')
    print 'done loading vocabulary'
    
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-73-180bdc332cbe>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_wiki_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [74]:

def test_lemmatized_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary_extend_unigrams
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    if len(bigrams_src)==1:
        vocabulary_tbl_intersect = '{0}_bigrams'.format(bigrams_src[0])
    else:
        vocabulary_tbl_intersect = '{0}_'.format(bigrams_src[0])
        for i in range(len(bigrams_src)-1):
            vocabulary_tbl_intersect = '{0}{1}_'.format(vocabulary_tbl_intersect,bigrams_src[i+1])
        vocabulary_tbl_intersect = '{0}bigrams_vw'.format(vocabulary_tbl_intersect)
        
    vocabulary = load_common_vocabulary_extend_unigrams(vocabulary_tbl_name,vocabulary_tbl_intersect,'lemma')
    print 'done loading vocabulary'
    
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,'^',vocabulary_tbl_intersect,'(extended unigrams) --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-74-cd1ea993b277>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [75]:

def test_lemmatized_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()    
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()    
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiktionary_bigrams'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')
    print 'done loading vocabulary'
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-75-98c5bdcd6b4f>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [76]:

def test_lemmatized_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'google_bigrams'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')    
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-76-1c5602c28486>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [77]:

def test_lemmatized_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_wiktionary_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-77-c5ecc72d6e7c>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [78]:

def test_lemmatized_wiki_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_google_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-78-60ce875247ce>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_wiki_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [79]:

def test_lemmatized_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiktionary_google_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-79-2010f39a430e>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [80]:

def test_stemmed_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_wiktionary_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-80-a08ffa055f62>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [81]:

def test_stemmed_wiki_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_google_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-81-781d3020ecf2>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_wiki_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [82]:

def test_stemmed_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiktionary_google_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-82-69e08236a4c4>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [83]:

def test_lemmatized_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_wiktionary_google_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-83-bdd713c1f2ac>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [84]:

def test_stemmed_wiki_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()    
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()    
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_bigrams'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')    
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-84-6b79430f2923>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_wiki_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [85]:

def test_stemmed_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary_extend_unigrams
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    if len(bigrams_src)==1:
        vocabulary_tbl_intersect = '{0}_bigrams'.format(bigrams_src[0])
    else:
        vocabulary_tbl_intersect = '{0}_'.format(bigrams_src[0])
        for i in range(len(bigrams_src)-1):
            vocabulary_tbl_intersect = '{0}{1}_'.format(vocabulary_tbl_intersect,bigrams_src[i+1])
        vocabulary_tbl_intersect = '{0}bigrams_vw'.format(vocabulary_tbl_intersect)
        
    vocabulary = load_common_vocabulary_extend_unigrams(vocabulary_tbl_name,vocabulary_tbl_intersect,'stem')
    print 'done loading vocabulary'
    
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,'^',vocabulary_tbl_intersect,'(extended unigrams) --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-85-a650503cc609>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [86]:

def test_stemmed_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiktionary_bigrams'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-86-3444b80fb7e1>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [87]:

def test_stemmed_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'google_bigrams'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-87-dd4a635e1842>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [88]:

def test_stemmed_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_wiktionary_google_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-88-ce43e0732809>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [89]:

def test():
    from ng20_corpus_loader import load_corpus_and_labels
    from ng20_corpus_loader import load_corpus_with_labels_mappings
    
    # load 20ng docs with class lables from DB
    corpus_train_data = load_corpus_and_labels('train')
    print 'done loading {0} train records and {1} labels.'.format(len(corpus_train_data['corpus']),len(corpus_train_data['labels_dic']))
 
    corpus_test_data = load_corpus_with_labels_mappings('test',corpus_train_data['labels_dic'])
    print 'done loading {0} test records.'.format(len(corpus_test_data['corpus']))
    
    stopwords_removal_mask = 1
    chi_features_mask = 2
    raw_tokens_mask = 4
    for i in range(4,6): # test w/o stopword removal and w/o chi square features and w/o raw tokens
        stopwords_removal = i&stopwords_removal_mask==stopwords_removal_mask
        use_chi_features = i&chi_features_mask==chi_features_mask
        use_raw_tokens = i&raw_tokens_mask==raw_tokens_mask
        
        test_all_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_all_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        #test_lemmatized_bigrams_with_LSA({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
        #                         {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
        #                         stopwords_removal,use_chi_features,use_raw_tokens,113240)
        
        test_lemmatized_wiki_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['wiki'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_wiki_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['wiki','wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_wiki_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['wiki','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_wiktionary_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_all_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['wiki','wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_wiki_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['wiki'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_wiki_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['wiki','wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_wiki_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['wiki','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_wiktionary_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_all_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['wiki','wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
In [90]:

# test using all vocabulary
test()
 
print 'done!'
loaded 11314 records.
done loading 11314 train records and 20 labels.
loaded 7532 records.
done loading 7532 test records.
loaded (173735) terms
vectorizing done, 173735 terms vocabulary tokenized
vectorizing done, 173735 terms vocabulary tokenized
accuracy scores = [ 0.87643069  0.90858961  0.91316348  0.87561131],0.893448773122
macro precision scores = [ 0.87630091  0.90834528  0.9126091   0.87547754],0.893183207356
macro recall scores = [ 0.87012027  0.90660112  0.91040986  0.87141702],0.889637067423
macro f1 scores = [ 0.87126558  0.90728366  0.91077307  0.87247349],0.890448949959
weighted average precision scores = [ 0.87703862  0.90868201  0.9140162   0.87716395],0.894225195013
weighted average recall scores = [ 0.87703862  0.90868201  0.9140162   0.87716395],0.894225195013
weighted f1 scores = [ 0.87524205  0.90846963  0.91291992  0.87548518],0.893029196831
ng20_raw_unigrams  -->  precision  0.893183207356 recall  0.889637067423 f1  0.890448949959
loaded (1664448) terms
vectorizing done, 1664448 terms vocabulary tokenized
vectorizing done, 1664448 terms vocabulary tokenized
accuracy scores = [ 0.88406104  0.91495228  0.91995754  0.8811397 ],0.900027639498
macro precision scores = [ 0.88502293  0.91584792  0.92038406  0.88182897],0.900770970261
macro recall scores = [ 0.8769893   0.91335715  0.91738391  0.87721406],0.8962361036
macro f1 scores = [ 0.87820266  0.91415864  0.91820036  0.8782911 ],0.897213189458
weighted average precision scores = [ 0.88504599  0.91570202  0.92082322  0.88280048],0.901092928729
weighted average recall scores = [ 0.88504599  0.91570202  0.92082322  0.88280048],0.901092928729
weighted f1 scores = [ 0.88244514  0.91489567  0.91979004  0.8808279 ],0.899489684994
ng20_raw_bigrams  -->  precision  0.900770970261 recall  0.8962361036 f1  0.897213189458
loaded (166463) terms
vectorizing done, 166463 terms vocabulary tokenized
vectorizing done, 166463 terms vocabulary tokenized
accuracy scores = [ 0.87664265  0.90986214  0.91443737  0.87454816],0.893872578835
macro precision scores = [ 0.87607542  0.90959614  0.91312375  0.87421817],0.893253369273
macro recall scores = [ 0.86986235  0.90759137  0.91191303  0.87034675],0.889928376593
macro f1 scores = [ 0.87100826  0.90836893  0.91202409  0.87134877],0.890687514208
weighted average precision scores = [ 0.87708952  0.9099395   0.91495855  0.87628371],0.894567819227
weighted average recall scores = [ 0.87708952  0.9099395   0.91495855  0.87628371],0.894567819227
weighted f1 scores = [ 0.87535026  0.90970925  0.91423999  0.87454107],0.89346013941
ng20_raw_lemmas_unigrams_df1_tf1  -->  precision  0.893253369273 recall  0.889928376593 f1  0.890687514208
loaded (1593710) terms
vectorizing done, 1593710 terms vocabulary tokenized
vectorizing done, 1593710 terms vocabulary tokenized
accuracy scores = [ 0.88236541  0.91664899  0.92059448  0.87965129],0.899815041973
macro precision scores = [ 0.88297182  0.91759784  0.92102695  0.88001483],0.900402860802
macro recall scores = [ 0.87487167  0.9146147   0.91757303  0.87593375],0.895748289027
macro f1 scores = [ 0.87616053  0.91563691  0.91849487  0.87693818],0.89680762422
weighted average precision scores = [ 0.88319086  0.91768277  0.92141793  0.88123474],0.900881572901
weighted average recall scores = [ 0.88319086  0.91768277  0.92141793  0.88123474],0.900881572901
weighted f1 scores = [ 0.88071504  0.91671615  0.92032526  0.87946906],0.899306375916
ng20_raw_lemmas_bigrams_df1_tf1  -->  precision  0.900402860802 recall  0.895748289027 f1  0.89680762422
loaded (221958) terms
done loading vocabulary
vectorizing done, 221958 terms vocabulary tokenized
vectorizing done, 221958 terms vocabulary tokenized
accuracy scores = [ 0.87876219  0.91431601  0.91847134  0.87582394],0.896843369959
macro precision scores = [ 0.87816909  0.91410389  0.91731495  0.87587392],0.89636546448
macro recall scores = [ 0.87179501  0.91237553  0.91584599  0.87140981],0.89285658462
macro f1 scores = [ 0.87289316  0.91306938  0.91611057  0.87253814],0.893652812707
weighted average precision scores = [ 0.87917797  0.91470473  0.9189514   0.87778438],0.897654622035
weighted average recall scores = [ 0.87917797  0.91470473  0.9189514   0.87778438],0.897654622035
weighted f1 scores = [ 0.87737817  0.91435204  0.91829337  0.87578024],0.89645095399
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_bigrams  -->  precision  0.89636546448 recall  0.89285658462 f1  0.893652812707
loaded (55495) terms
extended to (74491) terms
done loading vocabulary
vectorizing done, 74491 terms vocabulary tokenized
vectorizing done, 74491 terms vocabulary tokenized
accuracy scores = [ 0.85777872  0.89480382  0.90127389  0.84818201],0.875509608558
macro precision scores = [ 0.85749112  0.89400827  0.89960588  0.84682236],0.874481906626
macro recall scores = [ 0.85030329  0.89237993  0.89873551  0.84309904],0.871129440774
macro f1 scores = [ 0.85154658  0.89303297  0.89865466  0.84382966],0.871765966054
weighted average precision scores = [ 0.85860492  0.8950943   0.90136526  0.84992415],0.876247158078
weighted average recall scores = [ 0.85860492  0.8950943   0.90136526  0.84992415],0.876247158078
weighted f1 scores = [ 0.85630621  0.89479644  0.90084954  0.84798069],0.874983219556
ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_bigrams (extended unigrams) -->  precision  0.874481906626 recall  0.871129440774 f1  0.871765966054
loaded (177564) terms
done loading vocabulary
vectorizing done, 177564 terms vocabulary tokenized
vectorizing done, 177564 terms vocabulary tokenized
accuracy scores = [ 0.87833828  0.91304348  0.91316348  0.87539868],0.89498598021
macro precision scores = [ 0.87770751  0.91294254  0.91183135  0.87472047],0.894300467903
macro recall scores = [ 0.87140505  0.91069754  0.91047869  0.8711297 ],0.890927743652
macro f1 scores = [ 0.87246287  0.91158221  0.91057232  0.872037  ],0.891663601223
weighted average precision scores = [ 0.8787769   0.91320778  0.91372336  0.87678637],0.895623600533
weighted average recall scores = [ 0.8787769   0.91320778  0.91372336  0.87678637],0.895623600533
weighted f1 scores = [ 0.87692846  0.91292352  0.9129072   0.87527227],0.894507862504
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiktionary_bigrams  -->  precision  0.894300467903 recall  0.890927743652 f1  0.891663601223
loaded (11101) terms
extended to (17030) terms
done loading vocabulary
vectorizing done, 17030 terms vocabulary tokenized
vectorizing done, 17030 terms vocabulary tokenized
accuracy scores = [ 0.78613819  0.83626723  0.8373673   0.77950245],0.809818793811
macro precision scores = [ 0.78320864  0.83504143  0.83596369  0.77745049],0.807916060858
macro recall scores = [ 0.77817573  0.8331399   0.83456651  0.77423277],0.805028727216
macro f1 scores = [ 0.77867305  0.83370316  0.83457087  0.77444514],0.805348053421
weighted average precision scores = [ 0.78578947  0.83684253  0.83697161  0.78053564],0.810034814518
weighted average recall scores = [ 0.78578947  0.83684253  0.83697161  0.78053564],0.810034814518
weighted f1 scores = [ 0.78436027  0.8362014   0.83652658  0.77866311],0.808937840482
ng20_raw_lemmas_bigrams_df1_tf1 ^ wiktionary_bigrams (extended unigrams) -->  precision  0.807916060858 recall  0.805028727216 f1  0.805348053421
loaded (197617) terms
vectorizing done, 197617 terms vocabulary tokenized
vectorizing done, 197617 terms vocabulary tokenized
accuracy scores = [ 0.87770242  0.91155885  0.91422505  0.8783755 ],0.895465457268
macro precision scores = [ 0.87634854  0.91126368  0.91274695  0.87842685],0.894696503376
macro recall scores = [ 0.87073842  0.90934482  0.91119817  0.87429492],0.891394082124
macro f1 scores = [ 0.87170864  0.91012275  0.91139185  0.87530062],0.892130964434
weighted average precision scores = [ 0.87779501  0.91179866  0.91465546  0.88038786],0.896159246693
weighted average recall scores = [ 0.87779501  0.91179866  0.91465546  0.88038786],0.896159246693
weighted f1 scores = [ 0.8763172   0.9115196   0.91393636  0.87838718],0.895040085706
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ google_bigrams  -->  precision  0.894696503376 recall  0.891394082124 f1  0.892130964434
loaded (31154) terms
extended to (39297) terms
done loading vocabulary
vectorizing done, 39297 terms vocabulary tokenized
vectorizing done, 39297 terms vocabulary tokenized
accuracy scores = [ 0.82259432  0.86765642  0.87154989  0.82245375],0.846063595522
macro precision scores = [ 0.82158414  0.86660419  0.86850974  0.82034678],0.844261212819
macro recall scores = [ 0.81550964  0.8651078   0.86745145  0.81653228],0.841150292298
macro f1 scores = [ 0.8167787   0.86558763  0.86734213  0.81725314],0.841740401079
weighted average precision scores = [ 0.82336428  0.86766768  0.87170438  0.82417824],0.846728648183
weighted average recall scores = [ 0.82336428  0.86766768  0.87170438  0.82417824],0.846728648183
weighted f1 scores = [ 0.82147916  0.86740661  0.87106059  0.82218225],0.845532152874
ng20_raw_lemmas_bigrams_df1_tf1 ^ google_bigrams (extended unigrams) -->  precision  0.844261212819 recall  0.841150292298 f1  0.841740401079
loaded (224956) terms
vectorizing done, 224956 terms vocabulary tokenized
vectorizing done, 224956 terms vocabulary tokenized
accuracy scores = [ 0.88003391  0.9145281   0.91740977  0.87688709],0.897214718569
macro precision scores = [ 0.8796338   0.91454014  0.91633327  0.87650467],0.896752970179
macro recall scores = [ 0.87290936  0.91262824  0.91464768  0.8725257 ],0.893177744836
macro f1 scores = [ 0.87403886  0.91339526  0.91492485  0.87354043],0.893974849751
weighted average precision scores = [ 0.88049911  0.91498982  0.91792007  0.87856357],0.897993141759
weighted average recall scores = [ 0.88049911  0.91498982  0.91792007  0.87856357],0.897993141759
weighted f1 scores = [ 0.87855995  0.91458378  0.91717084  0.87682127],0.896783961314
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_wiktionary_bigrams_vw  -->  precision  0.896752970179 recall  0.893177744836 f1  0.893974849751
loaded (58493) terms
extended to (77918) terms
done loading vocabulary
vectorizing done, 77918 terms vocabulary tokenized
vectorizing done, 77918 terms vocabulary tokenized
accuracy scores = [ 0.85862654  0.89416755  0.90127389  0.85009568],0.876040913999
macro precision scores = [ 0.85847409  0.89367136  0.89926242  0.84845731],0.874966296519
macro recall scores = [ 0.85094101  0.89174237  0.8983584   0.84521103],0.87156320428
macro f1 scores = [ 0.85219332  0.89254066  0.89829089  0.84580656],0.872207856995
weighted average precision scores = [ 0.85939164  0.89456351  0.9012967   0.85190433],0.876789044474
weighted average recall scores = [ 0.85939164  0.89456351  0.9012967   0.85190433],0.876789044474
weighted f1 scores = [ 0.85704082  0.89421146  0.90081433  0.85000572],0.875518081436
ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_wiktionary_bigrams_vw (extended unigrams) -->  precision  0.874966296519 recall  0.87156320428 f1  0.872207856995
loaded (238169) terms
vectorizing done, 238169 terms vocabulary tokenized
vectorizing done, 238169 terms vocabulary tokenized
accuracy scores = [ 0.88045782  0.91389183  0.91868365  0.87709972],0.897533257767
macro precision scores = [ 0.87956794  0.91355209  0.91736866  0.87701401],0.89687567345
macro recall scores = [ 0.87341867  0.91183807  0.91599999  0.87280682],0.893515887221
macro f1 scores = [ 0.87445837  0.91252249  0.91626478  0.87381888],0.894266128969
weighted average precision scores = [ 0.88074659  0.91422809  0.91913924  0.87904192],0.898288960678
weighted average recall scores = [ 0.88074659  0.91422809  0.91913924  0.87904192],0.898288960678
weighted f1 scores = [ 0.87903913  0.91390126  0.91853823  0.87705329],0.897132976939
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_google_bigrams_vw  -->  precision  0.89687567345 recall  0.893515887221 f1  0.894266128969
loaded (71706) terms
extended to (91464) terms
done loading vocabulary
vectorizing done, 91464 terms vocabulary tokenized
vectorizing done, 91464 terms vocabulary tokenized
accuracy scores = [ 0.8615939   0.89565217  0.90403397  0.85243462],0.878428664028
macro precision scores = [ 0.86153155  0.89507402  0.90247795  0.85116231],0.877561455961
macro recall scores = [ 0.85388539  0.89332147  0.90138762  0.84755588],0.874037587464
macro f1 scores = [ 0.85524517  0.89405624  0.90140021  0.84829293],0.874748636248
weighted average precision scores = [ 0.86243458  0.89602686  0.90428868  0.85429702],0.879261785207
weighted average recall scores = [ 0.86243458  0.89602686  0.90428868  0.85429702],0.879261785207
weighted f1 scores = [ 0.86007774  0.89570882  0.90368176  0.85235011],0.87795460589
ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_google_bigrams_vw (extended unigrams) -->  precision  0.877561455961 recall  0.874037587464 f1  0.874748636248
loaded (204528) terms
vectorizing done, 204528 terms vocabulary tokenized
vectorizing done, 204528 terms vocabulary tokenized
accuracy scores = [ 0.87855023  0.9145281   0.91380042  0.87731235],0.896047778349
macro precision scores = [ 0.87757345  0.91445763  0.91269114  0.87710483],0.895456761433
macro recall scores = [ 0.87177487  0.91222969  0.91082631  0.87312416],0.891988757146
macro f1 scores = [ 0.87280154  0.91312042  0.9111186   0.87412637],0.892791730372
weighted average precision scores = [ 0.87866451  0.91475015  0.91446743  0.8790821 ],0.896741048336
weighted average recall scores = [ 0.87866451  0.91475015  0.91446743  0.8790821 ],0.896741048336
weighted f1 scores = [ 0.87715327  0.91444737  0.91356928  0.87728049],0.895612603113
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiktionary_google_bigrams_vw  -->  precision  0.895456761433 recall  0.891988757146 f1  0.892791730372
loaded (38065) terms
extended to (48205) terms
done loading vocabulary
vectorizing done, 48205 terms vocabulary tokenized
vectorizing done, 48205 terms vocabulary tokenized
accuracy scores = [ 0.83340398  0.87295864  0.87813163  0.82606847],0.852640682281
macro precision scores = [ 0.83299231  0.87265768  0.8766014   0.82390862],0.851540002695
macro recall scores = [ 0.82623813  0.87039807  0.87493965  0.82072595],0.848075446911
macro f1 scores = [ 0.82767729  0.87121236  0.87505091  0.82114931],0.84877246757
weighted average precision scores = [ 0.83455608  0.87304778  0.87894489  0.82773428],0.853570756273
weighted average recall scores = [ 0.83455608  0.87304778  0.87894489  0.82773428],0.853570756273
weighted f1 scores = [ 0.83238701  0.8727274   0.87784757  0.82576839],0.852182592491
ng20_raw_lemmas_bigrams_df1_tf1 ^ wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.851540002695 recall  0.848075446911 f1  0.84877246757
loaded (241012) terms
vectorizing done, 241012 terms vocabulary tokenized
vectorizing done, 241012 terms vocabulary tokenized
accuracy scores = [ 0.88066978  0.91346766  0.91740977  0.87773761],0.897321203122
macro precision scores = [ 0.87989665  0.91327689  0.91639892  0.87701685],0.896647325839
macro recall scores = [ 0.87361869  0.91147155  0.91464265  0.87328584],0.893254680896
macro f1 scores = [ 0.87468159  0.91220662  0.914949    0.8742206 ],0.894014450714
weighted average precision scores = [ 0.88096798  0.9137847   0.91787143  0.87930509],0.897982301707
weighted average recall scores = [ 0.88096798  0.9137847   0.91787143  0.87930509],0.897982301707
weighted f1 scores = [ 0.87921614  0.91347269  0.91714921  0.87765889],0.896874232443
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw  -->  precision  0.896647325839 recall  0.893254680896 f1  0.894014450714
loaded (74549) terms
extended to (94709) terms
done loading vocabulary
vectorizing done, 94709 terms vocabulary tokenized
vectorizing done, 94709 terms vocabulary tokenized
accuracy scores = [ 0.8615939   0.89501591  0.90424628  0.85264725],0.878375833335
macro precision scores = [ 0.86137118  0.89461497  0.90227222  0.85097494],0.87730832911
macro recall scores = [ 0.85384125  0.89284685  0.90155334  0.84771907],0.87399012858
macro f1 scores = [ 0.85509411  0.89360031  0.90143707  0.84837406],0.874626388935
weighted average precision scores = [ 0.86226244  0.89550677  0.90437601  0.85421824],0.879090866634
weighted average recall scores = [ 0.86226244  0.89550677  0.90437601  0.85421824],0.879090866634
weighted f1 scores = [ 0.85996898  0.89513798  0.90387676  0.85250512],0.877872209702
ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.87730832911 recall  0.87399012858 f1  0.874626388935
loaded (149349) terms
vectorizing done, 149349 terms vocabulary tokenized
vectorizing done, 149349 terms vocabulary tokenized
accuracy scores = [ 0.87367529  0.90774125  0.91167728  0.87497342],0.892016810266
macro precision scores = [ 0.87298652  0.90765079  0.91039407  0.8748176 ],0.891462244532
macro recall scores = [ 0.86688897  0.90534822  0.90865089  0.87082854],0.88792915887
macro f1 scores = [ 0.86813006  0.906255    0.90886999  0.87199777],0.888813205991
weighted average precision scores = [ 0.87388186  0.90813047  0.91220791  0.87652019],0.892685110522
weighted average recall scores = [ 0.87388186  0.90813047  0.91220791  0.87652019],0.892685110522
weighted f1 scores = [ 0.87241982  0.90772726  0.91136217  0.87498792],0.891624290513
ng20_raw_stems_unigrams_df1_tf1  -->  precision  0.891462244532 recall  0.88792915887 f1  0.888813205991
loaded (1491727) terms
vectorizing done, 1491727 terms vocabulary tokenized
vectorizing done, 1491727 terms vocabulary tokenized
accuracy scores = [ 0.88448495  0.91792153  0.92208068  0.8811397 ],0.901406713941
macro precision scores = [ 0.88511522  0.91907552  0.92242733  0.88195872],0.902144197741
macro recall scores = [ 0.87682189  0.91596255  0.91921724  0.87774668],0.897437091136
macro f1 scores = [ 0.8780785   0.91705036  0.92001401  0.87885571],0.898499645653
weighted average precision scores = [ 0.88536876  0.91904889  0.92307645  0.88267962],0.902543431362
weighted average recall scores = [ 0.88536876  0.91904889  0.92307645  0.88267962],0.902543431362
weighted f1 scores = [ 0.88277973  0.91804398  0.92186898  0.88098977],0.900920613312
ng20_raw_stems_bigrams_df1_tf1  -->  precision  0.902144197741 recall  0.897437091136 f1  0.898499645653
loaded (208752) terms
vectorizing done, 208752 terms vocabulary tokenized
vectorizing done, 208752 terms vocabulary tokenized
accuracy scores = [ 0.87833828  0.91537646  0.91762208  0.87773761],0.897268608003
macro precision scores = [ 0.87876102  0.9152304   0.91662775  0.87811717],0.897184085688
macro recall scores = [ 0.87136966  0.91315029  0.9148879   0.87355239],0.893240059813
macro f1 scores = [ 0.87273547  0.91398163  0.91515437  0.8748169 ],0.894172091397
weighted average precision scores = [ 0.87910238  0.91589747  0.91812868  0.87968081],0.898202333431
weighted average recall scores = [ 0.87910238  0.91589747  0.91812868  0.87968081],0.898202333431
weighted f1 scores = [ 0.87698808  0.9154478   0.9173378   0.87776041],0.896883524644
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiki_bigrams  -->  precision  0.897184085688 recall  0.893240059813 f1  0.894172091397
loaded (59403) terms
extended to (76544) terms
done loading vocabulary
vectorizing done, 76544 terms vocabulary tokenized
vectorizing done, 76544 terms vocabulary tokenized
accuracy scores = [ 0.85650699  0.89544008  0.89936306  0.85264725],0.875989345772
macro precision scores = [ 0.85567747  0.89455038  0.89783458  0.85135414],0.87485414101
macro recall scores = [ 0.84919427  0.89306525  0.89626757  0.84765502],0.871545526885
macro f1 scores = [ 0.85059217  0.89370258  0.89640495  0.84838177],0.872270368439
weighted average precision scores = [ 0.85709858  0.89554702  0.89934128  0.8543022 ],0.876572271422
weighted average recall scores = [ 0.85709858  0.89554702  0.89934128  0.8543022 ],0.876572271422
weighted f1 scores = [ 0.85535557  0.89540173  0.89878589  0.8523985 ],0.875485418897
ng20_raw_stems_bigrams_df1_tf1 ^ wiki_bigrams (extended unigrams) -->  precision  0.87485414101 recall  0.871545526885 f1  0.872270368439
loaded (160445) terms
vectorizing done, 160445 terms vocabulary tokenized
vectorizing done, 160445 terms vocabulary tokenized
accuracy scores = [ 0.8745231   0.91219512  0.91358811  0.87795024],0.894564144972
macro precision scores = [ 0.87471687  0.91210103  0.91249311  0.87804314],0.894338539147
macro recall scores = [ 0.86795055  0.90984243  0.91057539  0.87406814],0.890609129511
macro f1 scores = [ 0.86939075  0.9107439   0.91086665  0.87516984],0.891542786712
weighted average precision scores = [ 0.87519099  0.91260868  0.91420757  0.87950207],0.895377325635
weighted average recall scores = [ 0.87519099  0.91260868  0.91420757  0.87950207],0.895377325635
weighted f1 scores = [ 0.87340029  0.91220998  0.91329481  0.87790446],0.894202387309
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiktionary_bigrams  -->  precision  0.894338539147 recall  0.890609129511 f1  0.891542786712
loaded (11096) terms
extended to (16351) terms
done loading vocabulary
vectorizing done, 16351 terms vocabulary tokenized
vectorizing done, 16351 terms vocabulary tokenized
accuracy scores = [ 0.79037728  0.83541888  0.84309979  0.78822028],0.814279056762
macro precision scores = [ 0.78767473  0.83402471  0.84069678  0.78673127],0.812281874703
macro recall scores = [ 0.78191486  0.83177067  0.83968327  0.78247747],0.808961564265
macro f1 scores = [ 0.7825067   0.83236403  0.83958049  0.782899  ],0.809337554159
weighted average precision scores = [ 0.79026444  0.83609377  0.84232085  0.78995954],0.814659650098
weighted average recall scores = [ 0.79026444  0.83609377  0.84232085  0.78995954],0.814659650098
weighted f1 scores = [ 0.78854497  0.83525963  0.84217122  0.78747066],0.813361620193
ng20_raw_stems_bigrams_df1_tf1 ^ wiktionary_bigrams (extended unigrams) -->  precision  0.812281874703 recall  0.808961564265 f1  0.809337554159
loaded (185015) terms
vectorizing done, 185015 terms vocabulary tokenized
vectorizing done, 185015 terms vocabulary tokenized
accuracy scores = [ 0.87706655  0.91113468  0.91464968  0.87709972],0.894987658824
macro precision scores = [ 0.87700926  0.91101023  0.91329227  0.87726448],0.894644060757
macro recall scores = [ 0.87026295  0.90868815  0.91135514  0.87301792],0.890831040991
macro f1 scores = [ 0.87156857  0.90962318  0.91171902  0.87422467],0.891783859583
weighted average precision scores = [ 0.87752302  0.91172052  0.91531146  0.87888583],0.895860206077
weighted average recall scores = [ 0.87752302  0.91172052  0.91531146  0.87888583],0.895860206077
weighted f1 scores = [ 0.87576459  0.91123049  0.91444072  0.8771383 ],0.89464352426
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ google_bigrams  -->  precision  0.894644060757 recall  0.890831040991 f1  0.891783859583
loaded (35666) terms
extended to (43305) terms
done loading vocabulary
vectorizing done, 43305 terms vocabulary tokenized
vectorizing done, 43305 terms vocabulary tokenized
accuracy scores = [ 0.82344214  0.86765642  0.87367304  0.82798214],0.848188431837
macro precision scores = [ 0.81998485  0.86685934  0.87142459  0.82577114],0.846009979433
macro recall scores = [ 0.81514498  0.86511482  0.87001545  0.82276425],0.843259876766
macro f1 scores = [ 0.81584188  0.86575208  0.86987008  0.82332901],0.843698261704
weighted average precision scores = [ 0.82344668  0.86790755  0.87399998  0.82936855],0.848680690993
weighted average recall scores = [ 0.82344668  0.86790755  0.87399998  0.82936855],0.848680690993
weighted f1 scores = [ 0.82203135  0.86756429  0.87308111  0.8277775 ],0.847613561748
ng20_raw_stems_bigrams_df1_tf1 ^ google_bigrams (extended unigrams) -->  precision  0.846009979433 recall  0.843259876766 f1  0.843698261704
loaded (211394) terms
vectorizing done, 211394 terms vocabulary tokenized
vectorizing done, 211394 terms vocabulary tokenized
accuracy scores = [ 0.8791861   0.91580064  0.91825902  0.87646183],0.897426897074
macro precision scores = [ 0.87923042  0.91563411  0.91717365  0.87651557],0.897138437813
macro recall scores = [ 0.87207286  0.9136625   0.91541234  0.87216654],0.893328557487
macro f1 scores = [ 0.87342108  0.91445352  0.91570952  0.87334465],0.894232189764
weighted average precision scores = [ 0.87980946  0.91629163  0.91863533  0.87825346],0.89824746805
weighted average recall scores = [ 0.87980946  0.91629163  0.91863533  0.87825346],0.89824746805
weighted f1 scores = [ 0.87782362  0.9158665   0.91793738  0.8764327 ],0.897015050951
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiki_wiktionary_bigrams_vw  -->  precision  0.897138437813 recall  0.893328557487 f1  0.894232189764
loaded (62045) terms
extended to (79424) terms
done loading vocabulary
vectorizing done, 79424 terms vocabulary tokenized
vectorizing done, 79424 terms vocabulary tokenized
accuracy scores = [ 0.85968631  0.89607635  0.89957537  0.85222199],0.876890004335
macro precision scores = [ 0.85879335  0.89534827  0.89841688  0.85053517],0.875773418451
macro recall scores = [ 0.85247535  0.89367442  0.89651245  0.84720865],0.872467718883
macro f1 scores = [ 0.85386861  0.89438442  0.8967374   0.84775852],0.873187236272
weighted average precision scores = [ 0.86006999  0.89634726  0.89961134  0.85380263],0.877457804559
weighted average recall scores = [ 0.86006999  0.89634726  0.89961134  0.85380263],0.877457804559
weighted f1 scores = [ 0.85851267  0.89609886  0.8989739   0.85194222],0.876381912389
ng20_raw_stems_bigrams_df1_tf1 ^ wiki_wiktionary_bigrams_vw (extended unigrams) -->  precision  0.875773418451 recall  0.872467718883 f1  0.873187236272
loaded (227804) terms
vectorizing done, 227804 terms vocabulary tokenized
vectorizing done, 227804 terms vocabulary tokenized
accuracy scores = [ 0.87727851  0.91516437  0.91868365  0.87773761],0.897216035743
macro precision scores = [ 0.87780323  0.91506013  0.91779835  0.87843899],0.89727517518
macro recall scores = [ 0.87033799  0.91294694  0.91598284  0.87384091],0.893277170614
macro f1 scores = [ 0.87177293  0.91380278  0.9163247   0.87508443],0.894246209002
weighted average precision scores = [ 0.87815481  0.91560044  0.91915955  0.87978471],0.898174877977
weighted average recall scores = [ 0.87815481  0.91560044  0.91915955  0.87978471],0.898174877977
weighted f1 scores = [ 0.87601357  0.91520564  0.91842403  0.87777722],0.896855115654
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiki_google_bigrams_vw  -->  precision  0.89727517518 recall  0.893277170614 f1  0.894246209002
loaded (78455) terms
extended to (96203) terms
done loading vocabulary
vectorizing done, 96203 terms vocabulary tokenized
vectorizing done, 96203 terms vocabulary tokenized
accuracy scores = [ 0.85989826  0.89607635  0.90127389  0.85604933],0.878324457402
macro precision scores = [ 0.85952953  0.89518401  0.90012004  0.85480133],0.877408728267
macro recall scores = [ 0.85247451  0.89351135  0.89828524  0.85120527],0.873869095448
macro f1 scores = [ 0.85387685  0.89421765  0.89856591  0.85187748],0.874634474219
weighted average precision scores = [ 0.8606263   0.89607542  0.90147962  0.85781305],0.878998597663
weighted average recall scores = [ 0.8606263   0.89607542  0.90147962  0.85781305],0.878998597663
weighted f1 scores = [ 0.85861685  0.89596594  0.90083051  0.85585329],0.877816650193
ng20_raw_stems_bigrams_df1_tf1 ^ wiki_google_bigrams_vw (extended unigrams) -->  precision  0.877408728267 recall  0.873869095448 f1  0.874634474219
loaded (191637) terms
vectorizing done, 191637 terms vocabulary tokenized
vectorizing done, 191637 terms vocabulary tokenized
accuracy scores = [ 0.87749046  0.91219512  0.91592357  0.87752498],0.895783533736
macro precision scores = [ 0.87788479  0.91179992  0.91485832  0.87719603],0.895434765148
macro recall scores = [ 0.87082658  0.90960681  0.91286686  0.87350893],0.891702295251
macro f1 scores = [ 0.87227196  0.91048736  0.91327221  0.87447573],0.892626815519
weighted average precision scores = [ 0.87819408  0.91255401  0.91649023  0.87913923],0.896594388194
weighted average recall scores = [ 0.87819408  0.91255401  0.91649023  0.87913923],0.896594388194
weighted f1 scores = [ 0.8762901   0.91219352  0.91568367  0.87750485],0.895418036326
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiktionary_google_bigrams_vw  -->  precision  0.895434765148 recall  0.891702295251 f1  0.892626815519
loaded (42288) terms
extended to (51312) terms
done loading vocabulary
vectorizing done, 51312 terms vocabulary tokenized
vectorizing done, 51312 terms vocabulary tokenized
accuracy scores = [ 0.83213226  0.87507953  0.87855626  0.83074633],0.854128597059
macro precision scores = [ 0.82912644  0.87461987  0.87734732  0.82948757],0.852645302953
macro recall scores = [ 0.82377476  0.87260123  0.87509626  0.82600037],0.849368155801
macro f1 scores = [ 0.82461894  0.87339291  0.87538843  0.82655216],0.849988109752
weighted average precision scores = [ 0.83227228  0.8751051   0.87910681  0.83253032],0.854753628424
weighted average recall scores = [ 0.83227228  0.8751051   0.87910681  0.83253032],0.854753628424
weighted f1 scores = [ 0.83072631  0.87490816  0.87809114  0.83050002],0.853556408956
ng20_raw_stems_bigrams_df1_tf1 ^ wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.852645302953 recall  0.849368155801 f1  0.849988109752
loaded (230266) terms
vectorizing done, 230266 terms vocabulary tokenized
vectorizing done, 230266 terms vocabulary tokenized
accuracy scores = [ 0.87897414  0.91558855  0.91889597  0.8762492 ],0.89742696436
macro precision scores = [ 0.87923866  0.91536146  0.91794151  0.87655193],0.897273389421
macro recall scores = [ 0.87187131  0.91345669  0.91610066  0.87208015],0.893377200371
macro f1 scores = [ 0.87324951  0.91422332  0.91644292  0.87327342],0.894297290711
weighted average precision scores = [ 0.87973537  0.91605419  0.91941804  0.87815706],0.898341164547
weighted average recall scores = [ 0.87973537  0.91605419  0.91941804  0.87815706],0.898341164547
weighted f1 scores = [ 0.87763466  0.91565081  0.91864578  0.87623186],0.897040776134
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw  -->  precision  0.897273389421 recall  0.893377200371 f1  0.894297290711
loaded (80917) terms
extended to (98882) terms
done loading vocabulary
vectorizing done, 98882 terms vocabulary tokenized
vectorizing done, 98882 terms vocabulary tokenized
accuracy scores = [ 0.86053412  0.89840933  0.90169851  0.85541144],0.879013352464
macro precision scores = [ 0.86001258  0.89753003  0.90055981  0.85420419],0.878076652502
macro recall scores = [ 0.85309571  0.89602143  0.89874276  0.85061523],0.874618784011
macro f1 scores = [ 0.85449328  0.89668078  0.89898561  0.85131762],0.875369323275
weighted average precision scores = [ 0.86108767  0.89851894  0.90194105  0.85736835],0.879729003166
weighted average recall scores = [ 0.86108767  0.89851894  0.90194105  0.85736835],0.879729003166
weighted f1 scores = [ 0.8592173   0.89838341  0.90125552  0.85533918],0.878548851333
ng20_raw_stems_bigrams_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.878076652502 recall  0.874618784011 f1  0.875369323275
loaded (173377) terms
vectorizing done, 173377 terms vocabulary tokenized
vectorizing done, 173377 terms vocabulary tokenized
accuracy scores = [ 0.87876219  0.90901379  0.91464968  0.87561131],0.894509241654
macro precision scores = [ 0.87904181  0.90876526  0.91411927  0.87498159],0.894226979963
macro recall scores = [ 0.87257629  0.90707534  0.91234667  0.87136795],0.890841563669
macro f1 scores = [ 0.8738204   0.90771558  0.91272111  0.87235188],0.891652240927
weighted average precision scores = [ 0.87958028  0.90902545  0.91508791  0.87699234],0.895171495751
weighted average recall scores = [ 0.87958028  0.90902545  0.91508791  0.87699234],0.895171495751
weighted f1 scores = [ 0.87763057  0.90884531  0.91441611  0.87553814],0.894107532624
ng20_raw_unigrams_stopwords  -->  precision  0.894226979963 recall  0.890841563669 f1  0.891652240927
loaded (1726872) terms
vectorizing done, 1726872 terms vocabulary tokenized
vectorizing done, 1726872 terms vocabulary tokenized
accuracy scores = [ 0.88384909  0.91580064  0.9163482   0.87773761],0.89843388362
macro precision scores = [ 0.88422351  0.91635596  0.91578829  0.87833001],0.898674442892
macro recall scores = [ 0.87783407  0.91434246  0.91367248  0.87397197],0.894955246665
macro f1 scores = [ 0.87906054  0.91501077  0.91410156  0.8751622 ],0.895833768495
weighted average precision scores = [ 0.88453749  0.91633613  0.91680002  0.87947629],0.899287484472
weighted average recall scores = [ 0.88453749  0.91633613  0.91680002  0.87947629],0.899287484472
weighted f1 scores = [ 0.88268195  0.915742    0.91604156  0.87768549],0.898037753083
ng20_raw_bigrams_stopwords  -->  precision  0.898674442892 recall  0.894955246665 f1  0.895833768495
loaded (166113) terms
vectorizing done, 166113 terms vocabulary tokenized
vectorizing done, 166113 terms vocabulary tokenized
accuracy scores = [ 0.88024587  0.90752916  0.91231423  0.8762492 ],0.894084614208
macro precision scores = [ 0.88050853  0.90723504  0.911104    0.87533301],0.893545144946
macro recall scores = [ 0.87351551  0.90530798  0.91001359  0.87184365],0.890170181816
macro f1 scores = [ 0.87477512  0.90604712  0.91019475  0.87274587],0.890940715572
weighted average precision scores = [ 0.88079308  0.90764776  0.9127288   0.87763375],0.894700847158
weighted average recall scores = [ 0.88079308  0.90764776  0.9127288   0.87763375],0.894700847158
weighted f1 scores = [ 0.87884118  0.90739085  0.91218629  0.87614982],0.893642032508
ng20_raw_lemmas_unigrams_stopwords_df1_tf1  -->  precision  0.893545144946 recall  0.890170181816 f1  0.890940715572
loaded (1659720) terms
vectorizing done, 1659720 terms vocabulary tokenized
vectorizing done, 1659720 terms vocabulary tokenized
accuracy scores = [ 0.88066978  0.91728526  0.91698514  0.8811397 ],0.899019967802
macro precision scores = [ 0.88040578  0.91790316  0.91614577  0.88212251],0.899144303747
macro recall scores = [ 0.87456852  0.91553018  0.91407729  0.87734312],0.895379777001
macro f1 scores = [ 0.87581382  0.91636503  0.91464714  0.87871888],0.896386216615
weighted average precision scores = [ 0.88085582  0.91799439  0.91728151  0.88296723],0.899774740956
weighted average recall scores = [ 0.88085582  0.91799439  0.91728151  0.88296723],0.899774740956
weighted f1 scores = [ 0.87948419  0.91730141  0.91673844  0.88112134],0.898661344448
ng20_raw_lemmas_bigrams_stopwords_df1_tf1  -->  precision  0.899144303747 recall  0.895379777001 f1  0.896386216615
loaded (217211) terms
done loading vocabulary
vectorizing done, 217211 terms vocabulary tokenized
vectorizing done, 217211 terms vocabulary tokenized
accuracy scores = [ 0.88066978  0.91389183  0.914862    0.87986392],0.897321880575
macro precision scores = [ 0.88049208  0.91382881  0.9137393   0.87907561],0.896783948394
macro recall scores = [ 0.87387867  0.91188794  0.91195186  0.87558138],0.893324961971
macro f1 scores = [ 0.87519544  0.91268545  0.91240327  0.87649365],0.89419445354
weighted average precision scores = [ 0.88129618  0.91417697  0.91538634  0.88138795],0.898061859477
weighted average recall scores = [ 0.88129618  0.91417697  0.91538634  0.88138795],0.898061859477
weighted f1 scores = [ 0.87946842  0.91388091  0.91474652  0.8798327 ],0.896982139745
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_bigrams  -->  precision  0.896783948394 recall  0.893324961971 f1  0.89419445354
loaded (51098) terms
extended to (68835) terms
done loading vocabulary
vectorizing done, 68835 terms vocabulary tokenized
vectorizing done, 68835 terms vocabulary tokenized
accuracy scores = [ 0.85523527  0.89225875  0.89936306  0.85137147],0.874557135051
macro precision scores = [ 0.85428614  0.89137263  0.89687552  0.84951422],0.873012126979
macro recall scores = [ 0.84842125  0.88953215  0.89642036  0.84655819],0.870232988647
macro f1 scores = [ 0.84962313  0.89024187  0.89627671  0.84714726],0.870822242306
weighted average precision scores = [ 0.85545869  0.89221306  0.89920915  0.85246771],0.874837151197
weighted average recall scores = [ 0.85545869  0.89221306  0.89920915  0.85246771],0.874837151197
weighted f1 scores = [ 0.85396661  0.89204939  0.89893882  0.85107336],0.87400704699
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_bigrams (extended unigrams) -->  precision  0.873012126979 recall  0.870232988647 f1  0.870822242306
loaded (174244) terms
done loading vocabulary
vectorizing done, 174244 terms vocabulary tokenized
vectorizing done, 174244 terms vocabulary tokenized
accuracy scores = [ 0.87982196  0.91049841  0.91316348  0.8762492 ],0.894933263095
macro precision scores = [ 0.88005572  0.9099713   0.91180661  0.8754863 ],0.894329982376
macro recall scores = [ 0.87305903  0.90822778  0.91077658  0.87174608],0.890952366552
macro f1 scores = [ 0.87434181  0.90891993  0.91092452  0.87273427],0.891730134946
weighted average precision scores = [ 0.88052497  0.9105285   0.91363132  0.87781121],0.895624003003
weighted average recall scores = [ 0.88052497  0.9105285   0.91363132  0.87781121],0.895624003003
weighted f1 scores = [ 0.87849513  0.91035817  0.91305342  0.87619961],0.894526581085
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiktionary_bigrams  -->  precision  0.894329982376 recall  0.890952366552 f1  0.891730134946
loaded (8131) terms
extended to (13305) terms
done loading vocabulary
vectorizing done, 13305 terms vocabulary tokenized
vectorizing done, 13305 terms vocabulary tokenized
accuracy scores = [ 0.77660025  0.82248144  0.82993631  0.76461833],0.798409082752
macro precision scores = [ 0.77295377  0.82143345  0.82570106  0.76354242],0.795907674188
macro recall scores = [ 0.76947164  0.81944113  0.82633579  0.75830883],0.793389347259
macro f1 scores = [ 0.76996717  0.81993588  0.82559462  0.75913265],0.793657581687
weighted average precision scores = [ 0.77575701  0.82325168  0.82880866  0.76672875],0.798636528441
weighted average recall scores = [ 0.77575701  0.82325168  0.82880866  0.76672875],0.798636528441
weighted f1 scores = [ 0.77520275  0.82236614  0.82898128  0.76397753],0.797631925114
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiktionary_bigrams (extended unigrams) -->  precision  0.795907674188 recall  0.793389347259 f1  0.793657581687
loaded (206782) terms
vectorizing done, 206782 terms vocabulary tokenized
vectorizing done, 206782 terms vocabulary tokenized
accuracy scores = [ 0.8819415   0.90965005  0.91380042  0.87709972],0.895622925467
macro precision scores = [ 0.88193019  0.90942814  0.91239783  0.87663521],0.895097840514
macro recall scores = [ 0.87541514  0.90742652  0.91105306  0.87290427],0.891699747194
macro f1 scores = [ 0.87665118  0.90822673  0.91137209  0.87388573],0.892533934301
weighted average precision scores = [ 0.88249107  0.90984803  0.914224    0.87903642],0.896399877583
weighted average recall scores = [ 0.88249107  0.90984803  0.914224    0.87903642],0.896399877583
weighted f1 scores = [ 0.88065941  0.90957259  0.91369193  0.87721913],0.895285765931
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ google_bigrams  -->  precision  0.895097840514 recall  0.891699747194 f1  0.892533934301
loaded (40669) terms
extended to (49080) terms
done loading vocabulary
vectorizing done, 49080 terms vocabulary tokenized
vectorizing done, 49080 terms vocabulary tokenized
accuracy scores = [ 0.82301823  0.86702015  0.87006369  0.82011482],0.84505422278
macro precision scores = [ 0.82091988  0.86555273  0.86805181  0.81905138],0.843393948802
macro recall scores = [ 0.81582775  0.86412426  0.86672625  0.81406627],0.840186130436
macro f1 scores = [ 0.81681293  0.86460106  0.8667396   0.81516771],0.840830324079
weighted average precision scores = [ 0.82326851  0.86661395  0.87015441  0.82256922],0.845651521964
weighted average recall scores = [ 0.82326851  0.86661395  0.87015441  0.82256922],0.845651521964
weighted f1 scores = [ 0.82184357  0.8666011   0.86951897  0.82002916],0.84449819951
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ google_bigrams (extended unigrams) -->  precision  0.843393948802 recall  0.840186130436 f1  0.840830324079
loaded (218249) terms
vectorizing done, 218249 terms vocabulary tokenized
vectorizing done, 218249 terms vocabulary tokenized
accuracy scores = [ 0.88066978  0.91410392  0.91443737  0.87965129],0.897215588173
macro precision scores = [ 0.88046724  0.9140411   0.91333333  0.87899202],0.896708424256
macro recall scores = [ 0.87387537  0.91209119  0.91154278  0.87537895],0.893222070963
macro f1 scores = [ 0.87517238  0.91289229  0.91198747  0.87633007],0.894095550894
weighted average precision scores = [ 0.88129112  0.91439457  0.9149586   0.88119899],0.897960822251
weighted average recall scores = [ 0.88129112  0.91439457  0.9149586   0.88119899],0.897960822251
weighted f1 scores = [ 0.87945668  0.91409475  0.91431174  0.87961461],0.896869444188
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_bigrams_vw  -->  precision  0.896708424256 recall  0.893222070963 f1  0.894095550894
loaded (52136) terms
extended to (70081) terms
done loading vocabulary
vectorizing done, 70081 terms vocabulary tokenized
vectorizing done, 70081 terms vocabulary tokenized
accuracy scores = [ 0.85650699  0.89247084  0.89957537  0.84988305],0.87460906429
macro precision scores = [ 0.85594986  0.8915693   0.89714916  0.84799632],0.873166160028
macro recall scores = [ 0.84970464  0.88973624  0.89661756  0.84519063],0.870312270005
macro f1 scores = [ 0.85098437  0.89044918  0.89651916  0.845697  ],0.870912430862
weighted average precision scores = [ 0.85696711  0.89240764  0.89955514  0.85109377],0.87500591564
weighted average recall scores = [ 0.85696711  0.89240764  0.89955514  0.85109377],0.87500591564
weighted f1 scores = [ 0.85526061  0.89226008  0.89922523  0.84962714],0.87409326645
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_bigrams_vw (extended unigrams) -->  precision  0.873166160028 recall  0.870312270005 f1  0.870912430862
loaded (241441) terms
vectorizing done, 241441 terms vocabulary tokenized
vectorizing done, 241441 terms vocabulary tokenized
accuracy scores = [ 0.88236541  0.91516437  0.91592357  0.87943866],0.898223000291
macro precision scores = [ 0.88212051  0.91513006  0.91459089  0.87875877],0.897650059486
macro recall scores = [ 0.87566374  0.91303484  0.91290332  0.87501828],0.894155044176
macro f1 scores = [ 0.87698224  0.91387756  0.9132867   0.87597341],0.895029978999
weighted average precision scores = [ 0.88291255  0.91542002  0.91636327  0.88106445],0.898940071613
weighted average recall scores = [ 0.88291255  0.91542002  0.91636327  0.88106445],0.898940071613
weighted f1 scores = [ 0.88118196  0.9151127   0.91575505  0.87938711],0.897859206117
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_google_bigrams_vw  -->  precision  0.897650059486 recall  0.894155044176 f1  0.895029978999
loaded (75328) terms
extended to (94090) terms
done loading vocabulary
vectorizing done, 94090 terms vocabulary tokenized
vectorizing done, 94090 terms vocabulary tokenized
accuracy scores = [ 0.85544722  0.895228    0.90127389  0.85392303],0.876468033091
macro precision scores = [ 0.85484705  0.89471282  0.89892885  0.85265097],0.875284921806
macro recall scores = [ 0.84868325  0.89254956  0.89862448  0.84902624],0.872220883256
macro f1 scores = [ 0.85002553  0.89342287  0.89841943  0.8498245 ],0.872923083994
weighted average precision scores = [ 0.85592078  0.89538967  0.9013135   0.85531215],0.87698402276
weighted average recall scores = [ 0.85592078  0.89538967  0.9013135   0.85531215],0.87698402276
weighted f1 scores = [ 0.85429367  0.89512909  0.90095819  0.85366779],0.876012188249
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_google_bigrams_vw (extended unigrams) -->  precision  0.875284921806 recall  0.872220883256 f1  0.872923083994
loaded (210546) terms
vectorizing done, 210546 terms vocabulary tokenized
vectorizing done, 210546 terms vocabulary tokenized
accuracy scores = [ 0.88236541  0.91049841  0.91401274  0.87773761],0.896153542886
macro precision scores = [ 0.88207364  0.91009473  0.91282752  0.87712528],0.89553029378
macro recall scores = [ 0.87560726  0.90819541  0.911141    0.87341258],0.89208906243
macro f1 scores = [ 0.87678889  0.90895032  0.91156417  0.87441957],0.892930738027
weighted average precision scores = [ 0.88277396  0.91065355  0.91450641  0.87943425],0.896842041679
weighted average recall scores = [ 0.88277396  0.91065355  0.91450641  0.87943425],0.896842041679
weighted f1 scores = [ 0.88101257  0.91040196  0.91388894  0.87777763],0.895770276418
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiktionary_google_bigrams_vw  -->  precision  0.89553029378 recall  0.89208906243 f1  0.892930738027
loaded (44433) terms
extended to (54188) terms
done loading vocabulary
vectorizing done, 54188 terms vocabulary tokenized
vectorizing done, 54188 terms vocabulary tokenized
accuracy scores = [ 0.82958881  0.87104984  0.87388535  0.82394216],0.849616541161
macro precision scores = [ 0.82808122  0.87046879  0.87153223  0.82230097],0.84809580349
macro recall scores = [ 0.82234761  0.86855816  0.87061503  0.81767649],0.844799322455
macro f1 scores = [ 0.82349378  0.86920315  0.87054629  0.81849263],0.845433963721
weighted average precision scores = [ 0.82993397  0.87120085  0.87409196  0.82567701],0.850225950674
weighted average recall scores = [ 0.82993397  0.87120085  0.87409196  0.82567701],0.850225950674
weighted f1 scores = [ 0.82837492  0.87083764  0.87350662  0.82340034],0.849029879423
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.84809580349 recall  0.844799322455 f1  0.845433963721
loaded (242282) terms
vectorizing done, 242282 terms vocabulary tokenized
vectorizing done, 242282 terms vocabulary tokenized
accuracy scores = [ 0.88215345  0.91495228  0.91571125  0.87965129],0.89811706847
macro precision scores = [ 0.88189119  0.91491183  0.9144042   0.87893467],0.897535473437
macro recall scores = [ 0.87546131  0.91283404  0.91269585  0.87522071],0.894052976153
macro f1 scores = [ 0.87677575  0.91367303  0.91308544  0.87616763],0.89492546334
weighted average precision scores = [ 0.882675    0.91519038  0.91616935  0.88129296],0.898831921999
weighted average recall scores = [ 0.882675    0.91519038  0.91616935  0.88129296],0.898831921999
weighted f1 scores = [ 0.88096714  0.9148971   0.91554767  0.87961262],0.897756130618
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw  -->  precision  0.897535473437 recall  0.894052976153 f1  0.89492546334
loaded (76169) terms
extended to (95111) terms
done loading vocabulary
vectorizing done, 95111 terms vocabulary tokenized
vectorizing done, 95111 terms vocabulary tokenized
accuracy scores = [ 0.85629504  0.89501591  0.90127389  0.85328514],0.876467492362
macro precision scores = [ 0.85585407  0.89446782  0.89885619  0.85187165],0.875262432042
macro recall scores = [ 0.84927089  0.89222286  0.89862775  0.84840619],0.87213192298
macro f1 scores = [ 0.85068094  0.89313207  0.89840254  0.84916728],0.87284570467
weighted average precision scores = [ 0.85686719  0.89511867  0.90133355  0.85474411],0.87701588194
weighted average recall scores = [ 0.85686719  0.89511867  0.90133355  0.85474411],0.87701588194
weighted f1 scores = [ 0.85508226  0.89488711  0.90098219  0.85309848],0.876012509884
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.875262432042 recall  0.87213192298 f1  0.87284570467
loaded (149113) terms
vectorizing done, 149113 terms vocabulary tokenized
vectorizing done, 149113 terms vocabulary tokenized
accuracy scores = [ 0.87727851  0.90625663  0.91358811  0.87667446],0.893449427285
macro precision scores = [ 0.87777723  0.90663275  0.9123212   0.87672327],0.893363611589
macro recall scores = [ 0.87046475  0.90416188  0.9103817   0.87263112],0.889409860607
macro f1 scores = [ 0.87209594  0.90510257  0.91075639  0.87384613],0.890450257654
weighted average precision scores = [ 0.87817464  0.90684008  0.91400464  0.87841902],0.894359593116
weighted average recall scores = [ 0.87817464  0.90684008  0.91400464  0.87841902],0.894359593116
weighted f1 scores = [ 0.87622805  0.90628384  0.91327125  0.87677455],0.893139420942
ng20_raw_stems_unigrams_stopwords_df1_tf1  -->  precision  0.893363611589 recall  0.889409860607 f1  0.890450257654
loaded (1587711) terms
vectorizing done, 1587711 terms vocabulary tokenized
vectorizing done, 1587711 terms vocabulary tokenized
accuracy scores = [ 0.88130564  0.91728526  0.92038217  0.87943866],0.899602929893
macro precision scores = [ 0.88165461  0.91777244  0.92021701  0.88058861],0.900058167452
macro recall scores = [ 0.87456169  0.9155965   0.91736857  0.87576579],0.895823140229
macro f1 scores = [ 0.87593295  0.91638112  0.91790074  0.87717662],0.896847858403
weighted average precision scores = [ 0.88194699  0.91807415  0.92118711  0.8812484 ],0.900614163784
weighted average recall scores = [ 0.88194699  0.91807415  0.92118711  0.8812484 ],0.900614163784
weighted f1 scores = [ 0.88001435  0.91738615  0.92002379  0.87942603],0.899212577792
ng20_raw_stems_bigrams_stopwords_df1_tf1  -->  precision  0.900058167452 recall  0.895823140229 f1  0.896847858403
loaded (206785) terms
vectorizing done, 206785 terms vocabulary tokenized
vectorizing done, 206785 terms vocabulary tokenized
accuracy scores = [ 0.87961     0.91325557  0.91847134  0.8790134 ],0.897587576215
macro precision scores = [ 0.87971503  0.91313897  0.91753011  0.87925964],0.897410936683
macro recall scores = [ 0.8727607   0.91107671  0.91542974  0.87491038],0.893544383832
macro f1 scores = [ 0.87423843  0.9118569   0.91589354  0.87620822],0.894549273692
weighted average precision scores = [ 0.88037238  0.91387045  0.91884064  0.88073542],0.898454723336
weighted average recall scores = [ 0.88037238  0.91387045  0.91884064  0.88073542],0.898454723336
weighted f1 scores = [ 0.87850737  0.913325    0.9181582   0.87906471],0.897263819756
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_bigrams  -->  precision  0.897410936683 recall  0.893544383832 f1  0.894549273692
loaded (57672) terms
extended to (73779) terms
done loading vocabulary
vectorizing done, 73779 terms vocabulary tokenized
vectorizing done, 73779 terms vocabulary tokenized
accuracy scores = [ 0.85396354  0.89416755  0.89617834  0.84945779],0.873441807773
macro precision scores = [ 0.85257286  0.89292825  0.89423446  0.84833225],0.872016955938
macro recall scores = [ 0.84661177  0.8914783   0.89320803  0.84498028],0.869069592992
macro f1 scores = [ 0.84795687  0.89205268  0.89335826  0.84564525],0.869753266171
weighted average precision scores = [ 0.85454025  0.89408674  0.89587408  0.85115203],0.8739132738
weighted average recall scores = [ 0.85454025  0.89408674  0.89587408  0.85115203],0.8739132738
weighted f1 scores = [ 0.85296395  0.89399906  0.89570712  0.84933012],0.873000062519
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_bigrams (extended unigrams) -->  precision  0.872016955938 recall  0.869069592992 f1  0.869753266171
loaded (157884) terms
vectorizing done, 157884 terms vocabulary tokenized
vectorizing done, 157884 terms vocabulary tokenized
accuracy scores = [ 0.8768546   0.90965005  0.91507431  0.87773761],0.894829144174
macro precision scores = [ 0.87695123  0.90995602  0.91367316  0.87774365],0.894581015291
macro recall scores = [ 0.86989966  0.90740911  0.91173095  0.87379508],0.890708702146
macro f1 scores = [ 0.8713597   0.90839525  0.91209985  0.87493358],0.89169709605
weighted average precision scores = [ 0.8776028   0.91018871  0.9154711   0.87952293],0.895696383206
weighted average recall scores = [ 0.8776028   0.91018871  0.9154711   0.87952293],0.895696383206
weighted f1 scores = [ 0.87570629  0.90966378  0.91474922  0.8778442 ],0.894490871441
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiktionary_bigrams  -->  precision  0.894581015291 recall  0.890708702146 f1  0.89169709605
loaded (8771) terms
extended to (13555) terms
done loading vocabulary
vectorizing done, 13555 terms vocabulary tokenized
vectorizing done, 13555 terms vocabulary tokenized
accuracy scores = [ 0.78762187  0.82417815  0.83524416  0.77439932],0.805360877361
macro precision scores = [ 0.78444506  0.8234632   0.83237822  0.77307704],0.803340880263
macro recall scores = [ 0.77954704  0.8209955   0.83248984  0.76866066],0.800423259961
macro f1 scores = [ 0.78035486  0.82176103  0.83203781  0.76897226],0.800781487057
weighted average precision scores = [ 0.78747979  0.82524524  0.83483643  0.77586566],0.805856780705
weighted average recall scores = [ 0.78747979  0.82524524  0.83483643  0.77586566],0.805856780705
weighted f1 scores = [ 0.78627534  0.824246    0.83467216  0.77332071],0.804628553232
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiktionary_bigrams (extended unigrams) -->  precision  0.803340880263 recall  0.800423259961 f1  0.800781487057
loaded (196881) terms
vectorizing done, 196881 terms vocabulary tokenized
vectorizing done, 196881 terms vocabulary tokenized
accuracy scores = [ 0.88109368  0.91134677  0.91783439  0.87795024],0.897056272209
macro precision scores = [ 0.8813113   0.91091062  0.91635628  0.8781332 ],0.896677851438
macro recall scores = [ 0.8742356   0.90895677  0.91485529  0.8738294 ],0.892969263896
macro f1 scores = [ 0.87572372  0.90973801  0.9151438   0.87510216],0.893926921827
weighted average precision scores = [ 0.88184307  0.91169741  0.91804774  0.87976795],0.8978390437
weighted average recall scores = [ 0.88184307  0.91169741  0.91804774  0.87976795],0.8978390437
weighted f1 scores = [ 0.87995173  0.91134693  0.91753971  0.87804653],0.89672122536
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ google_bigrams  -->  precision  0.896677851438 recall  0.892969263896 f1  0.893926921827
loaded (47768) terms
extended to (55661) terms
done loading vocabulary
vectorizing done, 55661 terms vocabulary tokenized
vectorizing done, 55661 terms vocabulary tokenized
accuracy scores = [ 0.82301823  0.86702015  0.87048832  0.82458006],0.846276688632
macro precision scores = [ 0.81999572  0.86575811  0.86838371  0.8227877 ],0.844231310793
macro recall scores = [ 0.81425207  0.86436028  0.86707799  0.81874603],0.841109094161
macro f1 scores = [ 0.81507533  0.86483582  0.86698825  0.81950873],0.84160203571
weighted average precision scores = [ 0.82363523  0.86684061  0.87071057  0.82620883],0.846848807909
weighted average recall scores = [ 0.82363523  0.86684061  0.87071057  0.82620883],0.846848807909
weighted f1 scores = [ 0.82167107  0.86672995  0.86992566  0.82419801],0.845631173466
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ google_bigrams (extended unigrams) -->  precision  0.844231310793 recall  0.841109094161 f1  0.84160203571
loaded (207882) terms
vectorizing done, 207882 terms vocabulary tokenized
vectorizing done, 207882 terms vocabulary tokenized
accuracy scores = [ 0.87982196  0.91367975  0.91804671  0.87858814],0.897534137078
macro precision scores = [ 0.87996559  0.9135901   0.9170498   0.87891949],0.897381244489
macro recall scores = [ 0.87307918  0.91165291  0.91495174  0.87437749],0.893515329154
macro f1 scores = [ 0.8745845   0.91238419  0.91540809  0.87573046],0.894526812178
weighted average precision scores = [ 0.88060936  0.91431486  0.91845522  0.88038007],0.898439877918
weighted average recall scores = [ 0.88060936  0.91431486  0.91845522  0.88038007],0.898439877918
weighted f1 scores = [ 0.87877418  0.91376909  0.91774712  0.87864185],0.897233059481
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_bigrams_vw  -->  precision  0.897381244489 recall  0.893515329154 f1  0.894526812178
loaded (58769) terms
extended to (75015) terms
done loading vocabulary
vectorizing done, 75015 terms vocabulary tokenized
vectorizing done, 75015 terms vocabulary tokenized
accuracy scores = [ 0.85438745  0.89437964  0.89639066  0.84924516],0.873600728149
macro precision scores = [ 0.85247949  0.89321576  0.89439969  0.84825749],0.87208810604
macro recall scores = [ 0.84689963  0.89155447  0.89328684  0.84477689],0.869129458734
macro f1 scores = [ 0.84808294  0.89219574  0.89345639  0.84552317],0.869814558244
weighted average precision scores = [ 0.85467585  0.89426796  0.89608152  0.85096425],0.87399739555
weighted average recall scores = [ 0.85467585  0.89426796  0.89608152  0.85096425],0.87399739555
weighted f1 scores = [ 0.85327921  0.89416707  0.89589723  0.84915039],0.873123473655
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_bigrams_vw (extended unigrams) -->  precision  0.87208810604 recall  0.869129458734 f1  0.869814558244
loaded (235766) terms
vectorizing done, 235766 terms vocabulary tokenized
vectorizing done, 235766 terms vocabulary tokenized
accuracy scores = [ 0.87982196  0.91325557  0.91847134  0.87943866],0.897746879888
macro precision scores = [ 0.88011559  0.91298841  0.9174818   0.87957363],0.897539859839
macro recall scores = [ 0.87308075  0.91113448  0.91541109  0.87519309],0.893704850287
macro f1 scores = [ 0.87463011  0.91184682  0.91588091  0.87652756],0.894721349777
weighted average precision scores = [ 0.88074361  0.9137475   0.91883712  0.8811221 ],0.898612584247
weighted average recall scores = [ 0.88074361  0.9137475   0.91883712  0.8811221 ],0.898612584247
weighted f1 scores = [ 0.87880966  0.91329894  0.91817781  0.87950201],0.897447103325
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_google_bigrams_vw  -->  precision  0.897539859839 recall  0.893704850287 f1  0.894721349777
loaded (86653) terms
extended to (103595) terms
done loading vocabulary
vectorizing done, 103595 terms vocabulary tokenized
vectorizing done, 103595 terms vocabulary tokenized
accuracy scores = [ 0.85565918  0.89671262  0.89766454  0.85328514],0.875830369397
macro precision scores = [ 0.85454885  0.89549747  0.89611192  0.85218569],0.874585983759
macro recall scores = [ 0.84802284  0.89381514  0.89485149  0.84877836],0.871366958995
macro f1 scores = [ 0.84931901  0.89451195  0.8950008   0.84941667],0.872062105749
weighted average precision scores = [ 0.85629864  0.89666372  0.89797475  0.85492217],0.876464821159
weighted average recall scores = [ 0.85629864  0.89666372  0.89797475  0.85492217],0.876464821159
weighted f1 scores = [ 0.85446637  0.89656996  0.89737452  0.8530742 ],0.875371262004
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_google_bigrams_vw (extended unigrams) -->  precision  0.874585983759 recall  0.871366958995 f1  0.872062105749
loaded (200920) terms
vectorizing done, 200920 terms vocabulary tokenized
vectorizing done, 200920 terms vocabulary tokenized
accuracy scores = [ 0.87982196  0.91177094  0.91613588  0.87922603],0.896738702325
macro precision scores = [ 0.88014122  0.91136831  0.91472823  0.87914212],0.896344968558
macro recall scores = [ 0.8730169   0.90948271  0.91294843  0.87516873],0.892654193359
macro f1 scores = [ 0.87453392  0.91022556  0.91331044  0.87635153],0.893605365141
weighted average precision scores = [ 0.88055127  0.91216525  0.91640941  0.88085998],0.897496475851
weighted average recall scores = [ 0.88055127  0.91216525  0.91640941  0.88085998],0.897496475851
weighted f1 scores = [ 0.87867302  0.91178467  0.91581877  0.87929924],0.896393924654
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiktionary_google_bigrams_vw  -->  precision  0.896344968558 recall  0.892654193359 f1  0.893605365141
loaded (51807) terms
extended to (60709) terms
done loading vocabulary
vectorizing done, 60709 terms vocabulary tokenized
vectorizing done, 60709 terms vocabulary tokenized
accuracy scores = [ 0.82662145  0.87592789  0.87664544  0.82606847],0.851315810415
macro precision scores = [ 0.82429284  0.87465156  0.87469674  0.82468846],0.849582397849
macro recall scores = [ 0.81792935  0.87297324  0.87296157  0.82072337],0.846146882048
macro f1 scores = [ 0.81874618  0.87359482  0.87311923  0.82141195],0.846718048084
weighted average precision scores = [ 0.82738559  0.87583654  0.87692396  0.82795566],0.852025436991
weighted average recall scores = [ 0.82738559  0.87583654  0.87692396  0.82795566],0.852025436991
weighted f1 scores = [ 0.82513155  0.87568949  0.87614413  0.82578058],0.850686437342
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.849582397849 recall  0.846146882048 f1  0.846718048084
loaded (236636) terms
vectorizing done, 236636 terms vocabulary tokenized
vectorizing done, 236636 terms vocabulary tokenized
accuracy scores = [ 0.88003391  0.91325557  0.91847134  0.87943866],0.897799868442
macro precision scores = [ 0.88029282  0.91307008  0.91745465  0.87960609],0.897605912388
macro recall scores = [ 0.87328822  0.91113448  0.91541109  0.87518883],0.893755654582
macro f1 scores = [ 0.87482194  0.9118717   0.91587183  0.87653041],0.894773971056
weighted average precision scores = [ 0.88092776  0.91379013  0.91880355  0.88116533],0.898671692419
weighted average recall scores = [ 0.88092776  0.91379013  0.91880355  0.88116533],0.898671692419
weighted f1 scores = [ 0.87900723  0.91330488  0.91816564  0.87951254],0.897497572661
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw  -->  precision  0.897605912388 recall  0.893755654582 f1  0.894773971056
loaded (87523) terms
extended to (104579) terms
done loading vocabulary
vectorizing done, 104579 terms vocabulary tokenized
vectorizing done, 104579 terms vocabulary tokenized
accuracy scores = [ 0.85565918  0.89692471  0.89745223  0.85243462],0.875617682874
macro precision scores = [ 0.85427308  0.89590241  0.89569163  0.85113637],0.874250871196
macro recall scores = [ 0.84788121  0.89417988  0.89463828  0.84787418],0.871143388981
macro f1 scores = [ 0.84906535  0.89487757  0.89472193  0.84847722],0.871785519234
weighted average precision scores = [ 0.85619157  0.89686451  0.89753716  0.85390057],0.876123451918
weighted average recall scores = [ 0.85619157  0.89686451  0.89753716  0.85390057],0.876123451918
weighted f1 scores = [ 0.85438179  0.89676362  0.89708815  0.85218001],0.875103393736
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.874250871196 recall  0.871143388981 f1  0.871785519234
done!

