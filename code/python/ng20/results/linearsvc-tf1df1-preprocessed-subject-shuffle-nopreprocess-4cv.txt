IPython Notebookng20_classifier-4cv Last Checkpoint: Dec 31 19:24 (autosaved)
File
Edit
View
Insert
Cell
Kernel
Help
 Cell Toolbar:
Classify 20ng docs
Classifiy 20ng docs using unigrams and bigrams features with different classifiers and report classification results
In [31]:

def vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size):
    # tokenize text
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer
    from ng20_globals import *
    
    # generate corpus vectors
    vectorizer = CountVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),vocabulary=vocabulary,stop_words={})
    corpus_vectors = vectorizer.fit_transform(corpus)
    
    print 'vectorizing done, {0} terms vocabulary tokenized'.format(len(vectorizer.vocabulary_))
    
    # generate tfidf vectors
    transformer = TfidfTransformer()
    corpus_tfidf_vectors = transformer.fit_transform(corpus_vectors)
 
    return corpus_tfidf_vectors
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-31-743655218ee8>:1: SyntaxWarning: import * only allowed at module level
  def vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size):
In [32]:

def vectorize_corpus_new(corpus,tokenizer,vocabulary,max_ngram_size):
    # tokenize text
    from sklearn.feature_extraction.text import TfidfVectorizer
    from ng20_globals import *
    
    # generate corpus vectors
    vectorizer = TfidfVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),vocabulary=vocabulary,stop_words={})
    corpus_tfidf_vectors = vectorizer.fit_transform(corpus)
    
    print 'vectorizing done, {0} terms vocabulary tokenized'.format(len(vectorizer.vocabulary_))
    
    return corpus_tfidf_vectors
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-32-f88374f887f0>:1: SyntaxWarning: import * only allowed at module level
  def vectorize_corpus_new(corpus,tokenizer,vocabulary,max_ngram_size):
In [33]:

# given classifier predictions probabilities, return predictions with top n probabilities > 0.5 for each instance or greatest one if all are <=0.5
def get_max_n_pred(pred_proba, n_pred, threshold):
    import heapq
    import numpy
    max_n_pred = numpy.ndarray(shape=pred_proba.shape)
    for i in range(len(pred_proba)):
        largest_n_proba = heapq.nlargest(n_pred,pred_proba[i])
        max_n_pred[i] = numpy.array(((pred_proba[i]>threshold) & (pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1))
        if max_n_pred[i].sum(axis=0)==0: # at least one label should be returned
            max_n_pred[i] = numpy.array(((pred_proba[i]>=max(pred_proba[i])) & 1))
    return max_n_pred
In [34]:

# reference: http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification
def classify(x_train,y_train,x_test,y_test,max_labels):
    from sklearn.preprocessing import MultiLabelBinarizer
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.svm import SVC
    from sklearn.svm import LinearSVC
    from sklearn.linear_model import LogisticRegression
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.naive_bayes import BernoulliNB
    from sklearn.preprocessing import binarize
    from sklearn.cross_validation import train_test_split
    from sklearn import metrics
    import numpy
    from numpy import mean
    import scipy
    from sklearn.cross_validation import cross_val_score
    from sklearn.metrics import make_scorer
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import f1_score
    
    # combine train and test vectors
    #print 'merging training and testing samples x({0}),x({1})'.format(x_train.shape,x_test.shape)
    #print 'merging training and testing samples y({0}),y({1})'.format(len(y_train),len(y_test))
    if x_test.shape[0]>0:
        x = scipy.sparse.vstack((x_train,x_test))
        y = y_train + y_test
    else:
        x = x_train
        y = y_train
    #print 'merged into x({0})'.format(x.shape)
    #print 'merged into y({0})'.format(len(y))
    # binarize the labels
    #mlb = MultiLabelBinarizer()
    #y_train_binarized = mlb.fit_transform(y_train)
    
    # train/test split
    #corpus_tfidf_vectors, labels_binarized = shuffle(corpus_tfidf_vectors, labels_binarized)
    #x_train, x_test, y_train, y_test = train_test_split(corpus_tfidf_vectors, labels_binarized, test_size=test_size, random_state=1)
    
    # classify
    #cls = OneVsRestClassifier(LogisticRegression(class_weight='auto'))
    #cls = OneVsRestClassifier(LogisticRegression())
    #cls = MultinomialNB(alpha=0.01)
    #cls = OneVsRestClassifier(BernoulliNB()need binarize(x_train and x_test))
    #cls = OneVsRestClassifier(SVC(kernel='linear',probability=True,max_iter=1000))
    cls = LinearSVC()
    acc_scores = cross_val_score(cls,x,y,scoring='accuracy',cv=4,n_jobs=-1)
    print 'accuracy scores = {0},{1}'.format(acc_scores,mean(acc_scores))
    macro_p_scores = cross_val_score(cls,x,y,scoring=make_scorer(precision_score,average='macro'),cv=4,n_jobs=-1)
    print 'macro precision scores = {0},{1}'.format(macro_p_scores,mean(macro_p_scores))
    macro_r_scores = cross_val_score(cls,x,y,scoring=make_scorer(recall_score,average='macro'),cv=4,n_jobs=-1)
    print 'macro recall scores = {0},{1}'.format(macro_r_scores,mean(macro_r_scores))
    macro_f1_scores = cross_val_score(cls,x,y,scoring=make_scorer(f1_score,average='macro'),cv=4,n_jobs=-1)
    print 'macro f1 scores = {0},{1}'.format(macro_f1_scores,mean(macro_f1_scores))
    p_scores = cross_val_score(cls,x,y,scoring=make_scorer(precision_score,average='weighted'),cv=4,n_jobs=-1)
    print 'weighted average precision scores = {0},{1}'.format(p_scores,mean(p_scores))
    r_scores = cross_val_score(cls,x,y,scoring=make_scorer(precision_score,average='weighted'),cv=4,n_jobs=-1)
    print 'weighted average recall scores = {0},{1}'.format(r_scores,mean(r_scores))
    f1_scores = cross_val_score(cls,x,y,scoring=make_scorer(f1_score,average='weighted'),cv=4,n_jobs=-1)
    print 'weighted f1 scores = {0},{1}'.format(f1_scores,mean(f1_scores))
   
    return {'precision':mean(macro_p_scores),
            'recall':mean(macro_r_scores),
            'f1':mean(macro_f1_scores)}
In [35]:

import sklearn
from sklearn.svm import LinearSVC
c = LinearSVC()
print sklearn.__version__
0.15.2
In [36]:

def test_all_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import RawTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    
    max_ngram_size = 1
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = None
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}{1}_unigrams{2}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern)
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-36-5bcc0f6a848c>:1: SyntaxWarning: import * only allowed at module level
  def test_all_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [37]:

def test_lemmatized_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    
    max_ngram_size = 1
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-37-094e349cf724>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [38]:

def test_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    
    max_ngram_size = 2
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = None
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}{1}_bigrams{2}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern)
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-38-c28207c92f80>:1: SyntaxWarning: import * only allowed at module level
  def test_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [39]:

def test_lemmatized_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    
    max_ngram_size = 2
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-39-781cd4f52dc5>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [40]:

def test_lemmatized_bigrams_with_LSA(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens,num_components):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    from sklearn.decomposition import TruncatedSVD
    from scipy import sparse
    import numpy
    
    max_ngram_size = 2
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # apply LSA
    #print numpy.max(corpus_train_tfidf_vectors)
    #print numpy.min(corpus_train_tfidf_vectors)
    lsa = TruncatedSVD(n_components=num_components)
    lsa.fit(corpus_train_tfidf_vectors)
    #corpus_train_tfidf_vectors = numpy.dot(corpus_train_tfidf_vectors,pca.components_.transpose())
    corpus_train_tfidf_vectors = lsa.transform(corpus_train_tfidf_vectors)
    corpus_test_tfidf_vectors = lsa.transform(corpus_test_tfidf_vectors)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print 'LSA ^' , vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-40-0ae0e12f66cc>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_bigrams_with_LSA(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens,num_components):
In [41]:

def test_stemmed_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer    
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    
    max_ngram_size = 1
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()    
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()    
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)    
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-41-d27651528d70>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_unigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [42]:

def test_stemmed_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer    
    from ng20_globals import *
    from ng20_vocabulary_loader import load_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary = load_vocabulary(vocabulary_tbl_name)
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-42-8129eac551f8>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [43]:

def test_lemmatized_wiki_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_bigrams'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')
    print 'done loading vocabulary'
    
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-43-180bdc332cbe>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_wiki_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [44]:

def test_lemmatized_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary_extend_unigrams
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    if len(bigrams_src)==1:
        vocabulary_tbl_intersect = '{0}_bigrams'.format(bigrams_src[0])
    else:
        vocabulary_tbl_intersect = '{0}_'.format(bigrams_src[0])
        for i in range(len(bigrams_src)-1):
            vocabulary_tbl_intersect = '{0}{1}_'.format(vocabulary_tbl_intersect,bigrams_src[i+1])
        vocabulary_tbl_intersect = '{0}bigrams_vw'.format(vocabulary_tbl_intersect)
        
    vocabulary = load_common_vocabulary_extend_unigrams(vocabulary_tbl_name,vocabulary_tbl_intersect,'lemma')
    print 'done loading vocabulary'
    
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,'^',vocabulary_tbl_intersect,'(extended unigrams) --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-44-cd1ea993b277>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [45]:

def test_lemmatized_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()    
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()    
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiktionary_bigrams'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')
    print 'done loading vocabulary'
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-45-98c5bdcd6b4f>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [46]:

def test_lemmatized_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'google_bigrams'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')    
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-46-1c5602c28486>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [47]:

def test_lemmatized_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_wiktionary_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-47-c5ecc72d6e7c>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [48]:

def test_lemmatized_wiki_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_google_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-48-60ce875247ce>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_wiki_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [49]:

def test_lemmatized_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiktionary_google_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-49-2010f39a430e>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [50]:

def test_stemmed_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_wiktionary_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-50-a08ffa055f62>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [51]:

def test_stemmed_wiki_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_google_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-51-781d3020ecf2>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_wiki_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [52]:

def test_stemmed_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiktionary_google_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-52-69e08236a4c4>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [53]:

def test_lemmatized_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from lemmatizing_tokenizer import LemmaTokenizer
    from lemmatizing_tokenizer import RawLemmaTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = LemmaTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawLemmaTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_wiktionary_google_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-53-bdd713c1f2ac>:1: SyntaxWarning: import * only allowed at module level
  def test_lemmatized_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [54]:

def test_stemmed_wiki_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()    
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()    
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_bigrams'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')    
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-54-6b79430f2923>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_wiki_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [55]:

def test_stemmed_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary_extend_unigrams
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    if len(bigrams_src)==1:
        vocabulary_tbl_intersect = '{0}_bigrams'.format(bigrams_src[0])
    else:
        vocabulary_tbl_intersect = '{0}_'.format(bigrams_src[0])
        for i in range(len(bigrams_src)-1):
            vocabulary_tbl_intersect = '{0}{1}_'.format(vocabulary_tbl_intersect,bigrams_src[i+1])
        vocabulary_tbl_intersect = '{0}bigrams_vw'.format(vocabulary_tbl_intersect)
        
    vocabulary = load_common_vocabulary_extend_unigrams(vocabulary_tbl_name,vocabulary_tbl_intersect,'stem')
    print 'done loading vocabulary'
    
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name,'^',vocabulary_tbl_intersect,'(extended unigrams) --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-55-a650503cc609>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [56]:

def test_stemmed_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiktionary_bigrams'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-56-3444b80fb7e1>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_wiktionary_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [57]:

def test_stemmed_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
        
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'google_bigrams'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-57-dd4a635e1842>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_google_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [58]:

def test_stemmed_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
    from stemming_tokenizer import StemmingTokenizer
    from stemming_tokenizer import RawStemmingTokenizer
    from ng20_globals import *
    from ng20_vocabulary_loader import load_common_vocabulary
    
    max_ngram_size = 2
    
    if with_stopwords_removal==False:
        stopwords_pattern = ''
    else:
        stopwords_pattern = '_stopwords'
    if use_chi_features==False:
        chi_features_pattern = ''
    else:
        chi_features_pattern = '_chi'
    if use_raw_tokens==False:
        raw_tokens_pattern = ''
        tokenizer = StemmingTokenizer()
    else:
        raw_tokens_pattern = '_raw'
        tokenizer = RawStemmingTokenizer()
    
    # load vocabulary
    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)
    
    vocabulary_tbl_intersect = 'wiki_wiktionary_google_bigrams_vw'
    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')
 
    # generate tfidf vectors
    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)
    
    # classify & evaluate    
    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],
                       corpus_test_tfidf_vectors,corpus_test_data['labels'],
                       max_labels)
    
    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<input>:1: SyntaxWarning: import * only allowed at module level
<ipython-input-58-ce43e0732809>:1: SyntaxWarning: import * only allowed at module level
  def test_stemmed_all_bigrams(corpus_train_data,corpus_test_data,with_stopwords_removal,use_chi_features,use_raw_tokens):
In [59]:

def test():
    from ng20_corpus_loader import load_corpus_and_labels
    from ng20_corpus_loader import load_corpus_with_labels_mappings
    
    # load 20ng docs with class lables from DB
    corpus_train_data = load_corpus_and_labels('train')
    print 'done loading {0} train records and {1} labels.'.format(len(corpus_train_data['corpus']),len(corpus_train_data['labels_dic']))
 
    corpus_test_data = load_corpus_with_labels_mappings('test',corpus_train_data['labels_dic'])
    print 'done loading {0} test records.'.format(len(corpus_test_data['corpus']))
    
    stopwords_removal_mask = 1
    chi_features_mask = 2
    raw_tokens_mask = 4
    for i in range(4,6): # test w/o stopword removal and w/o chi square features and w/o raw tokens
        stopwords_removal = i&stopwords_removal_mask==stopwords_removal_mask
        use_chi_features = i&chi_features_mask==chi_features_mask
        use_raw_tokens = i&raw_tokens_mask==raw_tokens_mask
        
        test_all_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_all_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        #test_lemmatized_bigrams_with_LSA({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
        #                         {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
        #                         stopwords_removal,use_chi_features,use_raw_tokens,113240)
        
        test_lemmatized_wiki_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['wiki'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_wiki_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['wiki','wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_wiki_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['wiki','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_wiktionary_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_all_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_lemmatized_bigrams_unigrams(['wiki','wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_wiki_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['wiki'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_wiki_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['wiki','wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_wiki_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['wiki','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_wiktionary_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_all_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
        
        test_stemmed_bigrams_unigrams(['wiki','wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},
                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},
                                 stopwords_removal,use_chi_features,use_raw_tokens)
In [60]:

# test using all vocabulary
test()
 
print 'done!'
loaded 18828 records.
done loading 18828 train records and 20 labels.
loaded 0 records.
done loading 0 test records.
loaded (167750) terms
vectorizing done, 167750 terms vocabulary tokenized
vectorizing done, 167750 terms vocabulary tokenized
accuracy scores = [ 0.87253446  0.90997877  0.9019983   0.87954884],0.891015093138
macro precision scores = [ 0.87195789  0.90939983  0.90200963  0.87893088],0.890574559218
macro recall scores = [ 0.86515053  0.90786054  0.89597204  0.87656757],0.886387672514
macro f1 scores = [ 0.86633596  0.90843382  0.89728504  0.87693697],0.887247945924
weighted average precision scores = [ 0.87323734  0.91009057  0.9039059   0.88146636],0.89217504307
weighted average recall scores = [ 0.87323734  0.91009057  0.9039059   0.88146636],0.89217504307
weighted f1 scores = [ 0.87119441  0.90985032  0.90147776  0.87972682],0.890562326584
ng20_raw_unigrams  -->  precision  0.890574559218 recall  0.886387672514 f1  0.887247945924
loaded (1605094) terms
vectorizing done, 1605094 terms vocabulary tokenized
vectorizing done, 1605094 terms vocabulary tokenized
accuracy scores = [ 0.88080594  0.9163482   0.91156463  0.88699723],0.898928998282
macro precision scores = [ 0.88419687  0.91771791  0.91421344  0.88841389],0.901135529949
macro recall scores = [ 0.87290331  0.91449996  0.90555585  0.88411317],0.89426807207
macro f1 scores = [ 0.87482531  0.91570256  0.90745673  0.88488874],0.895718335033
weighted average precision scores = [ 0.88334767  0.91712781  0.91496623  0.88976747],0.901302291132
weighted average recall scores = [ 0.88334767  0.91712781  0.91496623  0.88976747],0.901302291132
weighted f1 scores = [ 0.87928042  0.91637392  0.91117671  0.88706916],0.898475053208
ng20_raw_bigrams  -->  precision  0.901135529949 recall  0.89426807207 f1  0.895718335033
loaded (160532) terms
vectorizing done, 160532 terms vocabulary tokenized
vectorizing done, 160532 terms vocabulary tokenized
accuracy scores = [ 0.86617179  0.90530786  0.89795918  0.87337731],0.885704036444
macro precision scores = [ 0.86470933  0.90510251  0.89629328  0.87232134],0.884606613255
macro recall scores = [ 0.85834756  0.90308201  0.89184225  0.87013408],0.88085147302
macro f1 scores = [ 0.85957206  0.90390451  0.89271634  0.87029574],0.881622163863
weighted average precision scores = [ 0.866104    0.90556796  0.89861505  0.8753682 ],0.886413803162
weighted average recall scores = [ 0.866104    0.90556796  0.89861505  0.8753682 ],0.886413803162
weighted f1 scores = [ 0.86464568  0.90527164  0.89712497  0.8734708 ],0.8851282735
ng20_raw_lemmas_unigrams_df1_tf1  -->  precision  0.884606613255 recall  0.88085147302 f1  0.881622163863
loaded (1535281) terms
vectorizing done, 1535281 terms vocabulary tokenized
vectorizing done, 1535281 terms vocabulary tokenized
accuracy scores = [ 0.87847296  0.91571125  0.91028912  0.88721004],0.897920842908
macro precision scores = [ 0.88150031  0.91677479  0.9111702   0.88852499],0.89949257366
macro recall scores = [ 0.87021964  0.9136957   0.90408833  0.88453408],0.893134438472
macro f1 scores = [ 0.87206531  0.9148706   0.90554634  0.88526743],0.894437419451
weighted average precision scores = [ 0.88083674  0.91632076  0.91262998  0.88985251],0.899909997158
weighted average recall scores = [ 0.88083674  0.91632076  0.91262998  0.88985251],0.899909997158
weighted f1 scores = [ 0.87686658  0.91569403  0.90966716  0.88733614],0.897390976842
ng20_raw_lemmas_bigrams_df1_tf1  -->  precision  0.89949257366 recall  0.893134438472 f1  0.894437419451
loaded (214720) terms
done loading vocabulary
vectorizing done, 214720 terms vocabulary tokenized
vectorizing done, 214720 terms vocabulary tokenized
accuracy scores = [ 0.87126193  0.90828025  0.90242347  0.87827197],0.890059406734
macro precision scores = [ 0.87112255  0.90829407  0.90126996  0.87772578],0.889603089293
macro recall scores = [ 0.86374278  0.90605994  0.89624652  0.87547135],0.885380147172
macro f1 scores = [ 0.86523985  0.90696723  0.89723524  0.87579433],0.886309163339
weighted average precision scores = [ 0.87189543  0.90865117  0.90367586  0.88017205],0.89109862829
weighted average recall scores = [ 0.87189543  0.90865117  0.90367586  0.88017205],0.89109862829
weighted f1 scores = [ 0.86990673  0.90827904  0.90174325  0.87844291],0.889592986578
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_bigrams  -->  precision  0.889603089293 recall  0.885380147172 f1  0.886309163339
loaded (54188) terms
extended to (72897) terms
done loading vocabulary
vectorizing done, 72897 terms vocabulary tokenized
vectorizing done, 72897 terms vocabulary tokenized
accuracy scores = [ 0.85217391  0.88832272  0.88201531  0.85677804],0.869822493667
macro precision scores = [ 0.85155049  0.88855423  0.87997518  0.85588804],0.86899198526
macro recall scores = [ 0.84402233  0.88549361  0.87530794  0.8534538 ],0.86456942144
macro f1 scores = [ 0.8454096   0.88667191  0.87603195  0.85378434],0.865474449421
weighted average precision scores = [ 0.85314001  0.88881301  0.88285677  0.85879415],0.870900984693
weighted average recall scores = [ 0.85314001  0.88881301  0.88285677  0.85879415],0.870900984693
weighted f1 scores = [ 0.85077499  0.88826485  0.88107252  0.85691804],0.869257597917
ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_bigrams (extended unigrams) -->  precision  0.86899198526 recall  0.86456942144 f1  0.865474449421
loaded (171574) terms
done loading vocabulary
vectorizing done, 171574 terms vocabulary tokenized
vectorizing done, 171574 terms vocabulary tokenized
accuracy scores = [ 0.86850477  0.90721868  0.89987245  0.87465418],0.887562521594
macro precision scores = [ 0.86755703  0.90701448  0.8985225   0.87372814],0.886705534717
macro recall scores = [ 0.86059056  0.90476689  0.89369136  0.87134758],0.882599097273
macro f1 scores = [ 0.86179723  0.9056532   0.8946256   0.87174626],0.883455572591
weighted average precision scores = [ 0.86873628  0.90749175  0.9008098   0.87654981],0.888396913291
weighted average recall scores = [ 0.86873628  0.90749175  0.9008098   0.87654981],0.888396913291
weighted f1 scores = [ 0.86689376  0.90714371  0.8990739   0.87483133],0.886985675289
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiktionary_bigrams  -->  precision  0.886705534717 recall  0.882599097273 f1  0.883455572591
loaded (11042) terms
extended to (16946) terms
done loading vocabulary
vectorizing done, 16946 terms vocabulary tokenized
vectorizing done, 16946 terms vocabulary tokenized
accuracy scores = [ 0.77454931  0.82675159  0.82546769  0.77739945],0.801042009208
macro precision scores = [ 0.77057748  0.82476317  0.82202012  0.77490011],0.798065220045
macro recall scores = [ 0.76607417  0.82248433  0.81821811  0.77277386],0.794887615338
macro f1 scores = [ 0.76646958  0.82311639  0.81825384  0.77251253],0.795088085202
weighted average precision scores = [ 0.77425086  0.82706151  0.82434773  0.77862966],0.801072441111
weighted average recall scores = [ 0.77425086  0.82706151  0.82434773  0.77862966],0.801072441111
weighted f1 scores = [ 0.77285108  0.82645359  0.82337641  0.7766911 ],0.799843044584
ng20_raw_lemmas_bigrams_df1_tf1 ^ wiktionary_bigrams (extended unigrams) -->  precision  0.798065220045 recall  0.794887615338 f1  0.795088085202
loaded (190903) terms
vectorizing done, 190903 terms vocabulary tokenized
vectorizing done, 190903 terms vocabulary tokenized
accuracy scores = [ 0.8699894   0.91019108  0.90136054  0.87614386],0.889421220741
macro precision scores = [ 0.86859013  0.90995384  0.90031269  0.87493902],0.888448918143
macro recall scores = [ 0.86213975  0.9078314   0.8951192   0.87292643],0.884504196547
macro f1 scores = [ 0.86338164  0.90868628  0.89614419  0.87307785],0.885322488865
weighted average precision scores = [ 0.87002712  0.91036704  0.90251763  0.87802892],0.890235177396
weighted average recall scores = [ 0.87002712  0.91036704  0.90251763  0.87802892],0.890235177396
weighted f1 scores = [ 0.8684981   0.91009983  0.90060229  0.87625716],0.888864345408
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ google_bigrams  -->  precision  0.888448918143 recall  0.884504196547 f1  0.885322488865
loaded (30371) terms
extended to (38414) terms
done loading vocabulary
vectorizing done, 38414 terms vocabulary tokenized
vectorizing done, 38414 terms vocabulary tokenized
accuracy scores = [ 0.81378579  0.86242038  0.85416667  0.81613109],0.836625982646
macro precision scores = [ 0.80989035  0.86073864  0.85115336  0.81514829],0.834232661243
macro recall scores = [ 0.80567419  0.85950695  0.8470399   0.81192161],0.831035664009
macro f1 scores = [ 0.806216    0.85991392  0.84760234  0.81237983],0.831528021047
weighted average precision scores = [ 0.81287963  0.86250991  0.85456868  0.81898781],0.837236508096
weighted average recall scores = [ 0.81287963  0.86250991  0.85456868  0.81898781],0.837236508096
weighted f1 scores = [ 0.81202039  0.86226136  0.85309215  0.81644036],0.835953564857
ng20_raw_lemmas_bigrams_df1_tf1 ^ google_bigrams (extended unigrams) -->  precision  0.834232661243 recall  0.831035664009 f1  0.831528021047
loaded (217713) terms
vectorizing done, 217713 terms vocabulary tokenized
vectorizing done, 217713 terms vocabulary tokenized
accuracy scores = [ 0.87232238  0.91061571  0.90306122  0.8786976 ],0.891174226593
macro precision scores = [ 0.87248885  0.91051523  0.90203835  0.87810755],0.890787493774
macro recall scores = [ 0.86482431  0.90834058  0.89687075  0.87585382],0.886472363471
macro f1 scores = [ 0.86631373  0.90922106  0.89787652  0.87617978],0.887397770714
weighted average precision scores = [ 0.87316582  0.91099973  0.90432774  0.88059625],0.892272388924
weighted average recall scores = [ 0.87316582  0.91099973  0.90432774  0.88059625],0.892272388924
weighted f1 scores = [ 0.87097396  0.91062152  0.90234393  0.87886725],0.890701667734
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_wiktionary_bigrams_vw  -->  precision  0.890787493774 recall  0.886472363471 f1  0.887397770714
loaded (57181) terms
extended to (76318) terms
done loading vocabulary
vectorizing done, 76318 terms vocabulary tokenized
vectorizing done, 76318 terms vocabulary tokenized
accuracy scores = [ 0.85090138  0.88980892  0.88201531  0.85486274],0.869397084663
macro precision scores = [ 0.85044807  0.88998521  0.88023075  0.85438476],0.868762197143
macro recall scores = [ 0.84259611  0.88706689  0.87555154  0.85181488],0.864257355618
macro f1 scores = [ 0.84394177  0.88815781  0.87626033  0.85216243],0.865130587555
weighted average precision scores = [ 0.85212663  0.89030909  0.88274701  0.85719829],0.870595257482
weighted average recall scores = [ 0.85212663  0.89030909  0.88274701  0.85719829],0.870595257482
weighted f1 scores = [ 0.84948534  0.88973484  0.88100477  0.85510985],0.868833701228
ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_wiktionary_bigrams_vw (extended unigrams) -->  precision  0.868762197143 recall  0.864257355618 f1  0.865130587555
loaded (230421) terms
vectorizing done, 230421 terms vocabulary tokenized
vectorizing done, 230421 terms vocabulary tokenized
accuracy scores = [ 0.87104984  0.9089172   0.90284864  0.87912322],0.890484723887
macro precision scores = [ 0.87123424  0.9090872   0.90171698  0.87860368],0.890160523327
macro recall scores = [ 0.86342918  0.90666975  0.89676581  0.87640968],0.885818606604
macro f1 scores = [ 0.86498517  0.90765307  0.89770623  0.87665386],0.886749583166
weighted average precision scores = [ 0.87188125  0.90923214  0.90410602  0.8810048 ],0.891556050577
weighted average recall scores = [ 0.87188125  0.90923214  0.90410602  0.8810048 ],0.891556050577
weighted f1 scores = [ 0.86968504  0.90888021  0.90214786  0.87923972],0.889988207704
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_google_bigrams_vw  -->  precision  0.890160523327 recall  0.885818606604 f1  0.886749583166
loaded (69889) terms
extended to (89352) terms
done loading vocabulary
vectorizing done, 89352 terms vocabulary tokenized
vectorizing done, 89352 terms vocabulary tokenized
accuracy scores = [ 0.85344645  0.88874735  0.88307823  0.85826772],0.870884935352
macro precision scores = [ 0.85352066  0.88871363  0.88113136  0.85722637],0.870148002966
macro recall scores = [ 0.84507308  0.88607301  0.87629503  0.85477944],0.865555141292
macro f1 scores = [ 0.84657486  0.88715485  0.8769802   0.85511661],0.86645663139
weighted average precision scores = [ 0.85467246  0.88917583  0.88387464  0.86022027],0.871985799357
weighted average recall scores = [ 0.85467246  0.88917583  0.88387464  0.86022027],0.871985799357
weighted f1 scores = [ 0.85193224  0.88876282  0.88201423  0.85837979],0.87027226754
ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_google_bigrams_vw (extended unigrams) -->  precision  0.870148002966 recall  0.865555141292 f1  0.86645663139
loaded (197794) terms
vectorizing done, 197794 terms vocabulary tokenized
vectorizing done, 197794 terms vocabulary tokenized
accuracy scores = [ 0.87211029  0.90976645  0.90242347  0.87742073],0.890430234469
macro precision scores = [ 0.87122881  0.90979357  0.90139889  0.87639379],0.889703766916
macro recall scores = [ 0.8640593   0.90753383  0.89623767  0.87436477],0.885548892172
macro f1 scores = [ 0.86536245  0.90845345  0.89729774  0.87458444],0.88642451933
weighted average precision scores = [ 0.87256542  0.91004287  0.90355732  0.87928834],0.891363489823
weighted average recall scores = [ 0.87256542  0.91004287  0.90355732  0.87928834],0.891363489823
weighted f1 scores = [ 0.87060601  0.90972255  0.90169769  0.8775907 ],0.889904237202
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiktionary_google_bigrams_vw  -->  precision  0.889703766916 recall  0.885548892172 f1  0.88642451933
loaded (37262) terms
extended to (47296) terms
done loading vocabulary
vectorizing done, 47296 terms vocabulary tokenized
vectorizing done, 47296 terms vocabulary tokenized
accuracy scores = [ 0.82841994  0.8670913   0.86267007  0.82847414],0.846663860738
macro precision scores = [ 0.82575823  0.86606254  0.86004875  0.82740928],0.844819699926
macro recall scores = [ 0.82037784  0.86405917  0.85572868  0.82430249],0.841117044436
macro f1 scores = [ 0.82102682  0.86479918  0.8563649   0.8246678 ],0.841714675581
weighted average precision scores = [ 0.82800758  0.86721596  0.8630848   0.83084953],0.847289465828
weighted average recall scores = [ 0.82800758  0.86721596  0.8630848   0.83084953],0.847289465828
weighted f1 scores = [ 0.82658029  0.86693221  0.86158596  0.82850109],0.845899889149
ng20_raw_lemmas_bigrams_df1_tf1 ^ wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.844819699926 recall  0.841117044436 f1  0.841714675581
loaded (233262) terms
vectorizing done, 233262 terms vocabulary tokenized
vectorizing done, 233262 terms vocabulary tokenized
accuracy scores = [ 0.87295864  0.9104034   0.90369898  0.87848478],0.891386450811
macro precision scores = [ 0.87332917  0.91038581  0.90250606  0.87778778],0.891002203729
macro recall scores = [ 0.86535877  0.90811707  0.89764444  0.87561264],0.886683228534
macro f1 scores = [ 0.86686417  0.90903987  0.89859508  0.87586791],0.887591759328
weighted average precision scores = [ 0.87395752  0.91073739  0.90478677  0.8802767 ],0.892439596876
weighted average recall scores = [ 0.87395752  0.91073739  0.90478677  0.8802767 ],0.892439596876
weighted f1 scores = [ 0.87159286  0.91038304  0.9029759   0.87857309],0.890881221671
ng20_raw_lemmas_unigrams_df1_tf1 ^ ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw  -->  precision  0.891002203729 recall  0.886683228534 f1  0.887591759328
loaded (72730) terms
extended to (92594) terms
done loading vocabulary
vectorizing done, 92594 terms vocabulary tokenized
vectorizing done, 92594 terms vocabulary tokenized
accuracy scores = [ 0.8542948   0.89171975  0.88456633  0.85656523],0.871786525554
macro precision scores = [ 0.85394107  0.89184927  0.88313168  0.85595262],0.871218659434
macro recall scores = [ 0.84609893  0.8891226   0.87791512  0.85337383],0.866627620086
macro f1 scores = [ 0.84744116  0.89019457  0.87868723  0.85379335],0.867529075347
weighted average precision scores = [ 0.85534647  0.89226178  0.88547351  0.85876306],0.872961203047
weighted average recall scores = [ 0.85534647  0.89226178  0.88547351  0.85876306],0.872961203047
weighted f1 scores = [ 0.85280785  0.89173508  0.88347315  0.85680868],0.871206188353
ng20_raw_lemmas_bigrams_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.871218659434 recall  0.866627620086 f1  0.867529075347
loaded (143566) terms
vectorizing done, 143566 terms vocabulary tokenized
vectorizing done, 143566 terms vocabulary tokenized
accuracy scores = [ 0.86638388  0.90615711  0.89562075  0.87188764],0.885012344431
macro precision scores = [ 0.86422292  0.90584637  0.89576624  0.87058649],0.884105503914
macro recall scores = [ 0.85841699  0.90360174  0.88948586  0.86852577],0.880007590245
macro f1 scores = [ 0.85946223  0.90451218  0.8908436   0.86879218],0.880902545416
weighted average precision scores = [ 0.86612712  0.90632727  0.89758028  0.87360197],0.885909161609
weighted average recall scores = [ 0.86612712  0.90632727  0.89758028  0.87360197],0.885909161609
weighted f1 scores = [ 0.86486763  0.90606084  0.89505724  0.87200778],0.884498372387
ng20_raw_stems_unigrams_df1_tf1  -->  precision  0.884105503914 recall  0.880007590245 f1  0.880902545416
loaded (1434241) terms
vectorizing done, 1434241 terms vocabulary tokenized
vectorizing done, 1434241 terms vocabulary tokenized
accuracy scores = [ 0.87995758  0.91762208  0.91071429  0.88572037],0.898503578653
macro precision scores = [ 0.88143974  0.91848402  0.91252296  0.88690872],0.899838858566
macro recall scores = [ 0.87173403  0.91543498  0.90439414  0.88297202],0.89363379509
macro f1 scores = [ 0.87339725  0.91661519  0.90615763  0.88370139],0.89496786333
weighted average precision scores = [ 0.88153368  0.91835647  0.91351592  0.88825571],0.900415446482
weighted average recall scores = [ 0.88153368  0.91835647  0.91351592  0.88825571],0.900415446482
weighted f1 scores = [ 0.8783757   0.91768665  0.91015413  0.88580271],0.898004799717
ng20_raw_stems_bigrams_df1_tf1  -->  precision  0.899838858566 recall  0.89363379509 f1  0.89496786333
loaded (201544) terms
vectorizing done, 201544 terms vocabulary tokenized
vectorizing done, 201544 terms vocabulary tokenized
accuracy scores = [ 0.87062566  0.91061571  0.90178571  0.8750798 ],0.889526723133
macro precision scores = [ 0.86979066  0.91062601  0.90178168  0.87460635],0.889201173904
macro recall scores = [ 0.86293815  0.90824962  0.89569737  0.87207592],0.884740265339
macro f1 scores = [ 0.86434302  0.90920708  0.89702944  0.87250795],0.885771873011
weighted average precision scores = [ 0.87111258  0.91090651  0.90389407  0.87692905],0.890710554861
weighted average recall scores = [ 0.87111258  0.91090651  0.90389407  0.87692905],0.890710554861
weighted f1 scores = [ 0.86935786  0.91056215  0.90135015  0.87519406],0.889116055031
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiki_bigrams  -->  precision  0.889201173904 recall  0.884740265339 f1  0.885771873011
loaded (57978) terms
extended to (74840) terms
done loading vocabulary
vectorizing done, 74840 terms vocabulary tokenized
vectorizing done, 74840 terms vocabulary tokenized
accuracy scores = [ 0.84835631  0.8910828   0.88222789  0.8561396 ],0.869451651881
macro precision scores = [ 0.84580585  0.89033289  0.88052087  0.85540914],0.868017188807
macro recall scores = [ 0.83984027  0.88864427  0.87529752  0.85255798],0.864085009729
macro f1 scores = [ 0.84093197  0.88930558  0.87613523  0.85305071],0.864855872417
weighted average precision scores = [ 0.84829661  0.89132619  0.88342319  0.85804036],0.870271589754
weighted average recall scores = [ 0.84829661  0.89132619  0.88342319  0.85804036],0.870271589754
weighted f1 scores = [ 0.84685247  0.89103766  0.88130245  0.85616807],0.868840162861
ng20_raw_stems_bigrams_df1_tf1 ^ wiki_bigrams (extended unigrams) -->  precision  0.868017188807 recall  0.864085009729 f1  0.864855872417
loaded (154593) terms
vectorizing done, 154593 terms vocabulary tokenized
vectorizing done, 154593 terms vocabulary tokenized
accuracy scores = [ 0.86702015  0.90806794  0.89965986  0.87422856],0.887244128057
macro precision scores = [ 0.86558978  0.90814728  0.90018445  0.87350115],0.886855664576
macro recall scores = [ 0.85918616  0.90566572  0.89335008  0.87109976],0.882325432828
macro f1 scores = [ 0.86028756  0.90666645  0.89480743  0.87158777],0.883337303335
weighted average precision scores = [ 0.86710866  0.90844482  0.90161844  0.87587537],0.88826182214
weighted average recall scores = [ 0.86710866  0.90844482  0.90161844  0.87587537],0.88826182214
weighted f1 scores = [ 0.86550593  0.90805086  0.89900307  0.87437322],0.886733270292
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiktionary_bigrams  -->  precision  0.886855664576 recall  0.882325432828 f1  0.883337303335
loaded (11027) terms
extended to (16256) terms
done loading vocabulary
vectorizing done, 16256 terms vocabulary tokenized
vectorizing done, 16256 terms vocabulary tokenized
accuracy scores = [ 0.78006363  0.82717622  0.83205782  0.79208342],0.807845273166
macro precision scores = [ 0.77524003  0.82546567  0.82952355  0.79083962],0.805267216895
macro recall scores = [ 0.770894    0.82345893  0.8247966   0.7883949 ],0.801886106968
macro f1 scores = [ 0.77124827  0.82395031  0.82521746  0.78852196],0.802234498725
weighted average precision scores = [ 0.77977391  0.82783979  0.83164536  0.79367255],0.80823290337
weighted average recall scores = [ 0.77977391  0.82783979  0.83164536  0.79367255],0.80823290337
weighted f1 scores = [ 0.77841573  0.8270285   0.83026783  0.79180912],0.806880297685
ng20_raw_stems_bigrams_df1_tf1 ^ wiktionary_bigrams (extended unigrams) -->  precision  0.805267216895 recall  0.801886106968 f1  0.802234498725
loaded (178372) terms
vectorizing done, 178372 terms vocabulary tokenized
vectorizing done, 178372 terms vocabulary tokenized
accuracy scores = [ 0.86723224  0.90955414  0.89902211  0.87699511],0.888200897963
macro precision scores = [ 0.86501815  0.90926887  0.89906118  0.875937  ],0.887321301359
macro recall scores = [ 0.85943456  0.90702283  0.89292566  0.87404915],0.883358050379
macro f1 scores = [ 0.86038069  0.90791658  0.89433106  0.87432191],0.884237559767
weighted average precision scores = [ 0.86699049  0.90983441  0.90110787  0.87838979],0.88908063789
weighted average recall scores = [ 0.86699049  0.90983441  0.90110787  0.87838979],0.88908063789
weighted f1 scores = [ 0.86573887  0.90948941  0.89862647  0.87704776],0.887725626708
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ google_bigrams  -->  precision  0.887321301359 recall  0.883358050379 f1  0.884237559767
loaded (34806) terms
extended to (42343) terms
done loading vocabulary
vectorizing done, 42343 terms vocabulary tokenized
vectorizing done, 42343 terms vocabulary tokenized
accuracy scores = [ 0.81845175  0.85944798  0.85863095  0.82336667],0.839974339723
macro precision scores = [ 0.81429296  0.85804307  0.85533577  0.8227615 ],0.837608323913
macro recall scores = [ 0.80970464  0.85638917  0.85152852  0.81945134],0.834268417103
macro f1 scores = [ 0.81025348  0.85693803  0.85201598  0.81992995],0.834784358928
weighted average precision scores = [ 0.81792912  0.8594792   0.85882188  0.82639675],0.840656736975
weighted average recall scores = [ 0.81792912  0.8594792   0.85882188  0.82639675],0.840656736975
weighted f1 scores = [ 0.81675567  0.85921321  0.85748442  0.82371945],0.839293187019
ng20_raw_stems_bigrams_df1_tf1 ^ google_bigrams (extended unigrams) -->  precision  0.837608323913 recall  0.834268417103 f1  0.834784358928
loaded (204179) terms
vectorizing done, 204179 terms vocabulary tokenized
vectorizing done, 204179 terms vocabulary tokenized
accuracy scores = [ 0.8699894   0.90955414  0.90136054  0.87720792],0.889527999117
macro precision scores = [ 0.86976037  0.90945148  0.90129213  0.87663261],0.88928414642
macro recall scores = [ 0.86216065  0.90701269  0.89527792  0.87416636],0.884654403989
macro f1 scores = [ 0.86361514  0.9079778   0.89656945  0.87453913],0.885675381504
weighted average precision scores = [ 0.87085277  0.90992346  0.90342484  0.87896589],0.890791740217
weighted average recall scores = [ 0.87085277  0.90992346  0.90342484  0.87896589],0.890791740217
weighted f1 scores = [ 0.8686761   0.90951599  0.90089755  0.8772539 ],0.889085884608
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiki_wiktionary_bigrams_vw  -->  precision  0.88928414642 recall  0.884654403989 f1  0.885675381504
loaded (60613) terms
extended to (77715) terms
done loading vocabulary
vectorizing done, 77715 terms vocabulary tokenized
vectorizing done, 77715 terms vocabulary tokenized
accuracy scores = [ 0.84962884  0.89235669  0.8807398   0.85592679],0.869663030216
macro precision scores = [ 0.84711877  0.89142031  0.87907598  0.85493897],0.868138507888
macro recall scores = [ 0.84103387  0.88985331  0.87399795  0.85248357],0.864342176723
macro f1 scores = [ 0.84204436  0.89047176  0.87476443  0.85287209],0.865038158632
weighted average precision scores = [ 0.84958058  0.89253702  0.88187707  0.8577414 ],0.870434014572
weighted average recall scores = [ 0.84958058  0.89253702  0.88187707  0.8577414 ],0.870434014572
weighted f1 scores = [ 0.84803231  0.89229442  0.8797793   0.85600478],0.869027703872
ng20_raw_stems_bigrams_df1_tf1 ^ wiki_wiktionary_bigrams_vw (extended unigrams) -->  precision  0.868138507888 recall  0.864342176723 f1  0.865038158632
loaded (220033) terms
vectorizing done, 220033 terms vocabulary tokenized
vectorizing done, 220033 terms vocabulary tokenized
accuracy scores = [ 0.87147402  0.9118896   0.90306122  0.87486699],0.89032295829
macro precision scores = [ 0.87061747  0.91198588  0.90341788  0.87424867],0.890067476139
macro recall scores = [ 0.86353585  0.90937771  0.89673645  0.8717762 ],0.885356551786
macro f1 scores = [ 0.86486407  0.91041795  0.8981867   0.87217543],0.886411038161
weighted average precision scores = [ 0.87191316  0.91219998  0.90520174  0.87665005],0.891491230711
weighted average recall scores = [ 0.87191316  0.91219998  0.90520174  0.87665005],0.891491230711
weighted f1 scores = [ 0.87005153  0.91181865  0.90251164  0.8749428 ],0.889831156993
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiki_google_bigrams_vw  -->  precision  0.890067476139 recall  0.885356551786 f1  0.886411038161
loaded (76467) terms
extended to (93926) terms
done loading vocabulary
vectorizing done, 93926 terms vocabulary tokenized
vectorizing done, 93926 terms vocabulary tokenized
accuracy scores = [ 0.84962884  0.8910828   0.88095238  0.85762928],0.86982332761
macro precision scores = [ 0.84774421  0.89027879  0.87952739  0.85657894],0.868532329786
macro recall scores = [ 0.84104411  0.88851907  0.87415535  0.8543608 ],0.864519835736
macro f1 scores = [ 0.842213    0.88920841  0.87503272  0.8546036 ],0.865264432738
weighted average precision scores = [ 0.85001356  0.89123015  0.88229414  0.85931281],0.870712664752
weighted average recall scores = [ 0.85001356  0.89123015  0.88229414  0.85931281],0.870712664752
weighted f1 scores = [ 0.84811378  0.89098747  0.88005508  0.85760922],0.869191388397
ng20_raw_stems_bigrams_df1_tf1 ^ wiki_google_bigrams_vw (extended unigrams) -->  precision  0.868532329786 recall  0.864519835736 f1  0.865264432738
loaded (184965) terms
vectorizing done, 184965 terms vocabulary tokenized
vectorizing done, 184965 terms vocabulary tokenized
accuracy scores = [ 0.86808059  0.9104034   0.90157313  0.87614386],0.889050245131
macro precision scores = [ 0.86642404  0.91032503  0.90198208  0.87544846],0.888544900992
macro recall scores = [ 0.86015068  0.90781176  0.89551467  0.87324906],0.884181542946
macro f1 scores = [ 0.86118454  0.90881855  0.89697175  0.87360965],0.885146120736
weighted average precision scores = [ 0.8681152   0.91076089  0.90348848  0.87776883],0.890033349392
weighted average recall scores = [ 0.8681152   0.91076089  0.90348848  0.87776883],0.890033349392
weighted f1 scores = [ 0.86651322  0.9103641   0.90103067  0.87625365],0.888540411304
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiktionary_google_bigrams_vw  -->  precision  0.888544900992 recall  0.884181542946 f1  0.885146120736
loaded (41399) terms
extended to (50316) terms
done loading vocabulary
vectorizing done, 50316 terms vocabulary tokenized
vectorizing done, 50316 terms vocabulary tokenized
accuracy scores = [ 0.82481442  0.86836518  0.86564626  0.83379442],0.848155071343
macro precision scores = [ 0.82141324  0.86709908  0.86471585  0.8335378 ],0.846691492399
macro recall scores = [ 0.81611772  0.86532904  0.85902858  0.83061161],0.842771737277
macro f1 scores = [ 0.81676299  0.86589027  0.86006877  0.83101053],0.843433138837
weighted average precision scores = [ 0.82456937  0.86836019  0.86693615  0.83649555],0.849090315357
weighted average recall scores = [ 0.82456937  0.86836019  0.86693615  0.83649555],0.849090315357
weighted f1 scores = [ 0.82311226  0.86808346  0.86475303  0.834116  ],0.847516188318
ng20_raw_stems_bigrams_df1_tf1 ^ wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.846691492399 recall  0.842771737277 f1  0.843433138837
loaded (222491) terms
vectorizing done, 222491 terms vocabulary tokenized
vectorizing done, 222491 terms vocabulary tokenized
accuracy scores = [ 0.87020148  0.91061571  0.9019983   0.87656948],0.889846244516
macro precision scores = [ 0.8701052   0.91052411  0.9022032   0.87591673],0.889687311486
macro recall scores = [ 0.86241984  0.90817456  0.89576676  0.87350219],0.884965836834
macro f1 scores = [ 0.86387465  0.90911623  0.89713745  0.87389005],0.886004594513
weighted average precision scores = [ 0.87096698  0.91101344  0.90406867  0.87826109],0.89107754398
weighted average recall scores = [ 0.87096698  0.91101344  0.90406867  0.87826109],0.89107754398
weighted f1 scores = [ 0.86880627  0.91060732  0.90144579  0.87662301],0.889370599913
ng20_raw_stems_unigrams_df1_tf1 ^ ng20_raw_stems_bigrams_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw  -->  precision  0.889687311486 recall  0.884965836834 f1  0.886004594513
loaded (78925) terms
extended to (96603) terms
done loading vocabulary
vectorizing done, 96603 terms vocabulary tokenized
vectorizing done, 96603 terms vocabulary tokenized
accuracy scores = [ 0.85111347  0.89235669  0.87988946  0.85550117],0.86971519545
macro precision scores = [ 0.84924018  0.89144001  0.87860443  0.85486683],0.868537863345
macro recall scores = [ 0.84263001  0.8899028   0.87323529  0.85241947],0.864546891796
macro f1 scores = [ 0.84373739  0.89050057  0.87405551  0.8526537 ],0.865236793483
weighted average precision scores = [ 0.85141713  0.89246672  0.88127308  0.85745517],0.870653023739
weighted average recall scores = [ 0.85141713  0.89246672  0.88127308  0.85745517],0.870653023739
weighted f1 scores = [ 0.84956861  0.89225542  0.87897116  0.85549658],0.86907294265
ng20_raw_stems_bigrams_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.868537863345 recall  0.864546891796 f1  0.865236793483
loaded (167391) terms
vectorizing done, 167391 terms vocabulary tokenized
vectorizing done, 167391 terms vocabulary tokenized
accuracy scores = [ 0.87253446  0.9104034   0.90072279  0.8806129 ],0.891068386745
macro precision scores = [ 0.87134142  0.91005086  0.90059502  0.87995379],0.890485270636
macro recall scores = [ 0.86518823  0.90811338  0.89453041  0.87720456],0.886259147668
macro f1 scores = [ 0.86639124  0.90885636  0.89581566  0.87774885],0.887203027462
weighted average precision scores = [ 0.87307141  0.91052411  0.90276276  0.88246449],0.892205690703
weighted average recall scores = [ 0.87307141  0.91052411  0.90276276  0.88246449],0.892205690703
weighted f1 scores = [ 0.87131342  0.91026728  0.90021268  0.8807311 ],0.890631117312
ng20_raw_unigrams_stopwords  -->  precision  0.890485270636 recall  0.886259147668 f1  0.887203027462
loaded (1674190) terms
vectorizing done, 1674190 terms vocabulary tokenized
vectorizing done, 1674190 terms vocabulary tokenized
accuracy scores = [ 0.88165429  0.91507431  0.90986395  0.88699723],0.898397445954
macro precision scores = [ 0.88199566  0.91547215  0.91095493  0.88679228],0.898803754999
macro recall scores = [ 0.87417644  0.91317635  0.90360049  0.88370693],0.893665051861
macro f1 scores = [ 0.87561518  0.91406144  0.90531794  0.88423877],0.894808331684
weighted average precision scores = [ 0.88276574  0.91546348  0.91217946  0.88877758],0.899796561918
weighted average recall scores = [ 0.88276574  0.91546348  0.91217946  0.88877758],0.899796561918
weighted f1 scores = [ 0.88030918  0.9150288   0.90933917  0.88690311],0.897895065709
ng20_raw_bigrams_stopwords  -->  precision  0.898803754999 recall  0.893665051861 f1  0.894808331684
loaded (160181) terms
vectorizing done, 160181 terms vocabulary tokenized
vectorizing done, 160181 terms vocabulary tokenized
accuracy scores = [ 0.87020148  0.90488323  0.89902211  0.87401575],0.887030642169
macro precision scores = [ 0.86919707  0.90498701  0.89703522  0.8727097 ],0.885982249084
macro recall scores = [ 0.86298292  0.90222246  0.89282265  0.87042392],0.882112990967
macro f1 scores = [ 0.86437102  0.90328543  0.8936478   0.87075726],0.883015377886
weighted average precision scores = [ 0.87045401  0.90526007  0.8999167   0.87582978],0.887865140289
weighted average recall scores = [ 0.87045401  0.90526007  0.8999167   0.87582978],0.887865140289
weighted f1 scores = [ 0.86900874  0.90479761  0.89834131  0.8741289 ],0.886569140441
ng20_raw_lemmas_unigrams_stopwords_df1_tf1  -->  precision  0.885982249084 recall  0.882112990967 f1  0.883015377886
loaded (1608107) terms
vectorizing done, 1608107 terms vocabulary tokenized
vectorizing done, 1608107 terms vocabulary tokenized
accuracy scores = [ 0.88059385  0.91656051  0.90837585  0.88572037],0.897812643837
macro precision scores = [ 0.88141392  0.91742493  0.90817367  0.88521156],0.898056017272
macro recall scores = [ 0.8731754   0.91452021  0.9020865   0.88228412],0.893016559358
macro f1 scores = [ 0.87490083  0.91565245  0.90345813  0.88282622],0.894209407596
weighted average precision scores = [ 0.88172655  0.91705313  0.90997521  0.88750742],0.899065578032
weighted average recall scores = [ 0.88172655  0.91705313  0.90997521  0.88750742],0.899065578032
weighted f1 scores = [ 0.87935192  0.91653173  0.90775471  0.88573051],0.897342217766
ng20_raw_lemmas_bigrams_stopwords_df1_tf1  -->  precision  0.898056017272 recall  0.893016559358 f1  0.894209407596
loaded (210034) terms
done loading vocabulary
vectorizing done, 210034 terms vocabulary tokenized
vectorizing done, 210034 terms vocabulary tokenized
accuracy scores = [ 0.87168611  0.91167728  0.90242347  0.87827197],0.891014708173
macro precision scores = [ 0.87044048  0.9116163   0.9007199   0.87727202],0.890012176577
macro recall scores = [ 0.86404061  0.90905552  0.89607969  0.87496479],0.886035154355
macro f1 scores = [ 0.86537232  0.91005755  0.89703595  0.87534419],0.8869525025
weighted average precision scores = [ 0.87195066  0.91184131  0.90360171  0.88016117],0.891888714002
weighted average recall scores = [ 0.87195066  0.91184131  0.90360171  0.88016117],0.891888714002
weighted f1 scores = [ 0.87040921  0.91152821  0.901814    0.87846392],0.890553835666
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_bigrams  -->  precision  0.890012176577 recall  0.886035154355 f1  0.8869525025
loaded (49853) terms
extended to (67310) terms
done loading vocabulary
vectorizing done, 67310 terms vocabulary tokenized
vectorizing done, 67310 terms vocabulary tokenized
accuracy scores = [ 0.84814422  0.89087049  0.87861395  0.85145776],0.867271602861
macro precision scores = [ 0.84612193  0.88993026  0.87602222  0.85171407],0.865947120766
macro recall scores = [ 0.8401555   0.88845263  0.87199547  0.84802374],0.862156833064
macro f1 scores = [ 0.84142129  0.88896946  0.87264612  0.84858961],0.862906619471
weighted average precision scores = [ 0.84859451  0.89113947  0.87914524  0.85429707],0.868294072461
weighted average recall scores = [ 0.84859451  0.89113947  0.87914524  0.85429707],0.868294072461
weighted f1 scores = [ 0.8469763   0.89079018  0.87769821  0.85164818],0.866778218068
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_bigrams (extended unigrams) -->  precision  0.865947120766 recall  0.862156833064 f1  0.862906619471
loaded (168259) terms
done loading vocabulary
vectorizing done, 168259 terms vocabulary tokenized
vectorizing done, 168259 terms vocabulary tokenized
accuracy scores = [ 0.87020148  0.90764331  0.89965986  0.87593105],0.888358927458
macro precision scores = [ 0.86873474  0.90762692  0.89782237  0.87475536],0.8872348482
macro recall scores = [ 0.86293325  0.9049357   0.89334621  0.87235985],0.883393753239
macro f1 scores = [ 0.8641988   0.9059439   0.89422     0.87276229],0.884281245915
weighted average precision scores = [ 0.87044805  0.90795591  0.90067699  0.87785043],0.889232847095
weighted average recall scores = [ 0.87044805  0.90795591  0.90067699  0.87785043],0.889232847095
weighted f1 scores = [ 0.86905577  0.90750732  0.89897276  0.87611103],0.887911718423
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiktionary_bigrams  -->  precision  0.8872348482 recall  0.883393753239 f1  0.884281245915
loaded (8078) terms
extended to (13231) terms
done loading vocabulary
vectorizing done, 13231 terms vocabulary tokenized
vectorizing done, 13231 terms vocabulary tokenized
accuracy scores = [ 0.76776246  0.81507431  0.81441327  0.77037668],0.791906677852
macro precision scores = [ 0.76332719  0.81337342  0.81033687  0.76824495],0.788820605349
macro recall scores = [ 0.75940264  0.81071876  0.80691787  0.76497961],0.785504720037
macro f1 scores = [ 0.75999344  0.81153537  0.80688702  0.7649149 ],0.785832684053
weighted average precision scores = [ 0.7673274   0.81689464  0.81389046  0.77278438],0.79272421822
weighted average recall scores = [ 0.7673274   0.81689464  0.81389046  0.77278438],0.79272421822
weighted f1 scores = [ 0.76642248  0.81546215  0.81275113  0.76988543],0.79113029693
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiktionary_bigrams (extended unigrams) -->  precision  0.788820605349 recall  0.785504720037 f1  0.785832684053
loaded (200143) terms
vectorizing done, 200143 terms vocabulary tokenized
vectorizing done, 200143 terms vocabulary tokenized
accuracy scores = [ 0.87253446  0.90785563  0.9005102   0.87614386],0.88926103882
macro precision scores = [ 0.87151477  0.90757071  0.8993223   0.87504065],0.888362108363
macro recall scores = [ 0.86501051  0.90515573  0.89432308  0.87287224],0.884340389116
macro f1 scores = [ 0.8663503   0.9060936   0.89535573  0.87320712],0.8852516872
weighted average precision scores = [ 0.87286557  0.90796424  0.90180885  0.87800116],0.890159954812
weighted average recall scores = [ 0.87286557  0.90796424  0.90180885  0.87800116],0.890159954812
weighted f1 scores = [ 0.87124467  0.90768219  0.89988159  0.87633983],0.888787072161
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ google_bigrams  -->  precision  0.888362108363 recall  0.884340389116 f1  0.8852516872
loaded (39962) terms
extended to (48275) terms
done loading vocabulary
vectorizing done, 48275 terms vocabulary tokenized
vectorizing done, 48275 terms vocabulary tokenized
accuracy scores = [ 0.81187699  0.86050955  0.85586735  0.81570547],0.835989839666
macro precision scores = [ 0.80821858  0.85845768  0.85278698  0.81514853],0.833652942716
macro recall scores = [ 0.80363053  0.85787081  0.84928013  0.81125829],0.830509938627
macro f1 scores = [ 0.80417256  0.85799692  0.84977459  0.81187093],0.830953751728
weighted average precision scores = [ 0.81139746  0.86002222  0.8562385   0.8186815 ],0.836584918475
weighted average recall scores = [ 0.81139746  0.86002222  0.8562385   0.8186815 ],0.836584918475
weighted f1 scores = [ 0.81010891  0.86010022  0.85493356  0.81589701],0.835259924539
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ google_bigrams (extended unigrams) -->  precision  0.833652942716 recall  0.830509938627 f1  0.830953751728
loaded (211067) terms
vectorizing done, 211067 terms vocabulary tokenized
vectorizing done, 211067 terms vocabulary tokenized
accuracy scores = [ 0.87126193  0.91125265  0.90263605  0.87848478],0.890908855589
macro precision scores = [ 0.87015952  0.91122273  0.90095315  0.87734902],0.889921104849
macro recall scores = [ 0.86352051  0.90864896  0.89639816  0.87510083],0.885917115982
macro f1 scores = [ 0.86487667  0.90965683  0.89734068  0.87547745],0.886837909031
weighted average precision scores = [ 0.87163595  0.91143743  0.9038215   0.88031982],0.891803677622
weighted average recall scores = [ 0.87163595  0.91143743  0.9038215   0.88031982],0.891803677622
weighted f1 scores = [ 0.86996502  0.91111333  0.90205155  0.87867391],0.890450952121
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_bigrams_vw  -->  precision  0.889921104849 recall  0.885917115982 f1  0.886837909031
loaded (50886) terms
extended to (68550) terms
done loading vocabulary
vectorizing done, 68550 terms vocabulary tokenized
vectorizing done, 68550 terms vocabulary tokenized
accuracy scores = [ 0.84729586  0.89065817  0.87882653  0.85060651],0.866846770249
macro precision scores = [ 0.84536863  0.88970707  0.87608586  0.85098458],0.865536534531
macro recall scores = [ 0.83918703  0.88829362  0.87200949  0.84733628],0.861706606393
macro f1 scores = [ 0.84051399  0.88879204  0.87264517  0.84781422],0.862441353869
weighted average precision scores = [ 0.84776719  0.89086249  0.87937973  0.85352564],0.867883764523
weighted average recall scores = [ 0.84776719  0.89086249  0.87937973  0.85352564],0.867883764523
weighted f1 scores = [ 0.84609732  0.89055923  0.87787702  0.85077272],0.86632656963
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_bigrams_vw (extended unigrams) -->  precision  0.865536534531 recall  0.861706606393 f1  0.862441353869
loaded (233813) terms
vectorizing done, 233813 terms vocabulary tokenized
vectorizing done, 233813 terms vocabulary tokenized
accuracy scores = [ 0.87211029  0.91125265  0.90221088  0.87699511],0.890642232486
macro precision scores = [ 0.87077927  0.91140608  0.90056115  0.87608347],0.889707490212
macro recall scores = [ 0.86433525  0.90869441  0.89598328  0.87372941],0.885685584307
macro f1 scores = [ 0.86561642  0.90975682  0.896958    0.87412963],0.886615219355
weighted average precision scores = [ 0.87234327  0.91143263  0.90334968  0.8788831 ],0.891502172797
weighted average recall scores = [ 0.87234327  0.91143263  0.90334968  0.8788831 ],0.891502172797
weighted f1 scores = [ 0.87076142  0.91110332  0.90162792  0.87718343],0.890169022013
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_google_bigrams_vw  -->  precision  0.889707490212 recall  0.885685584307 f1  0.886615219355
loaded (73632) terms
extended to (92101) terms
done loading vocabulary
vectorizing done, 92101 terms vocabulary tokenized
vectorizing done, 92101 terms vocabulary tokenized
accuracy scores = [ 0.84687169  0.88980892  0.88031463  0.85167057],0.86716644934
macro precision scores = [ 0.84623587  0.8888877   0.87741     0.85209129],0.866156215327
macro recall scores = [ 0.838472    0.88750115  0.87394736  0.84813968],0.862015048084
macro f1 scores = [ 0.84003226  0.88803717  0.87445642  0.84877807],0.862825980863
weighted average precision scores = [ 0.84802352  0.88993914  0.88079961  0.85467583],0.868359526801
weighted average recall scores = [ 0.84802352  0.88993914  0.88079961  0.85467583],0.868359526801
weighted f1 scores = [ 0.84557787  0.88972661  0.87946014  0.85187462],0.86665980983
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_google_bigrams_vw (extended unigrams) -->  precision  0.866156215327 recall  0.862015048084 f1  0.862825980863
loaded (203890) terms
vectorizing done, 203890 terms vocabulary tokenized
vectorizing done, 203890 terms vocabulary tokenized
accuracy scores = [ 0.87211029  0.90870488  0.90072279  0.87571824],0.889314049147
macro precision scores = [ 0.87039079  0.90851403  0.89950289  0.87485443],0.888315535383
macro recall scores = [ 0.8645613   0.90598057  0.89448883  0.87245457],0.884371316422
macro f1 scores = [ 0.86582598  0.9069607   0.89553767  0.87280577],0.885282529211
weighted average precision scores = [ 0.87225466  0.90882882  0.90198482  0.87782011],0.890222102193
weighted average recall scores = [ 0.87225466  0.90882882  0.90198482  0.87782011],0.890222102193
weighted f1 scores = [ 0.87090906  0.90852755  0.90008363  0.87593388],0.888863530551
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiktionary_google_bigrams_vw  -->  precision  0.888315535383 recall  0.884371316422 f1  0.885282529211
loaded (43709) terms
extended to (53360) terms
done loading vocabulary
vectorizing done, 53360 terms vocabulary tokenized
vectorizing done, 53360 terms vocabulary tokenized
accuracy scores = [ 0.81696713  0.86666667  0.8607568   0.82549479],0.842471345426
macro precision scores = [ 0.81451828  0.8649278   0.85778386  0.82445346],0.840420847813
macro recall scores = [ 0.8094301   0.86427085  0.85373763  0.82082678],0.837066339623
macro f1 scores = [ 0.81028978  0.8644115   0.85436185  0.82141092],0.837618514383
weighted average precision scores = [ 0.81694084  0.86637535  0.86107208  0.82795633],0.843086150345
weighted average recall scores = [ 0.81694084  0.86637535  0.86107208  0.82795633],0.843086150345
weighted f1 scores = [ 0.81553211  0.86633602  0.85971914  0.82551587],0.841775785307
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.840420847813 recall  0.837066339623 f1  0.837618514383
loaded (234652) terms
vectorizing done, 234652 terms vocabulary tokenized
vectorizing done, 234652 terms vocabulary tokenized
accuracy scores = [ 0.87211029  0.91125265  0.9019983   0.87656948],0.890482680609
macro precision scores = [ 0.87078215  0.91141065  0.9003944   0.87555059],0.889534448633
macro recall scores = [ 0.86433025  0.90869441  0.89578253  0.87320517],0.885503088861
macro f1 scores = [ 0.86559785  0.90975816  0.89675277  0.87361367],0.886430612296
weighted average precision scores = [ 0.8723368   0.91143668  0.90317693  0.87841488],0.891341322416
weighted average recall scores = [ 0.8723368   0.91143668  0.90317693  0.87841488],0.891341322416
weighted f1 scores = [ 0.87073992  0.91110435  0.90141297  0.8767458 ],0.89000075866
ng20_raw_lemmas_unigrams_stopwords_df1_tf1 ^ ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw  -->  precision  0.889534448633 recall  0.885503088861 f1  0.886430612296
loaded (74471) terms
extended to (93119) terms
done loading vocabulary
vectorizing done, 93119 terms vocabulary tokenized
vectorizing done, 93119 terms vocabulary tokenized
accuracy scores = [ 0.84835631  0.89044586  0.87988946  0.85167057],0.867590548378
macro precision scores = [ 0.84719857  0.8897414   0.877019    0.85224102],0.866549996545
macro recall scores = [ 0.83997186  0.88818667  0.87319752  0.84820366],0.86238992792
macro f1 scores = [ 0.84138047  0.88879172  0.87376044  0.84885665],0.863197319817
weighted average precision scores = [ 0.84917725  0.89068005  0.88048912  0.8547741 ],0.868780130037
weighted average recall scores = [ 0.84917725  0.89068005  0.88048912  0.8547741 ],0.868780130037
weighted f1 scores = [ 0.84699353  0.89040176  0.87898603  0.85189657],0.867069471256
ng20_raw_lemmas_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.866549996545 recall  0.86238992792 f1  0.863197319817
loaded (143329) terms
vectorizing done, 143329 terms vocabulary tokenized
vectorizing done, 143329 terms vocabulary tokenized
accuracy scores = [ 0.86574761  0.90700637  0.89795918  0.87018515],0.885224578218
macro precision scores = [ 0.86296666  0.90746793  0.89750772  0.86973739],0.884419922319
macro recall scores = [ 0.85783087  0.90445808  0.89186221  0.8670913 ],0.880310615608
macro f1 scores = [ 0.85886655  0.9056563   0.89312227  0.86757473],0.881304963467
weighted average precision scores = [ 0.86536715  0.90741236  0.89969825  0.87231423],0.88619799811
weighted average recall scores = [ 0.86536715  0.90741236  0.89969825  0.87231423],0.88619799811
weighted f1 scores = [ 0.86438741  0.9069613   0.89746796  0.87045698],0.884818411812
ng20_raw_stems_unigrams_stopwords_df1_tf1  -->  precision  0.884419922319 recall  0.880310615608 f1  0.881304963467
loaded (1537054) terms
vectorizing done, 1537054 terms vocabulary tokenized
vectorizing done, 1537054 terms vocabulary tokenized
accuracy scores = [ 0.87656416  0.91762208  0.90731293  0.8844435 ],0.896485665353
macro precision scores = [ 0.87592515  0.91824055  0.90814746  0.88471804],0.896757801301
macro recall scores = [ 0.86864806  0.91536918  0.90080987  0.88115015],0.891494315774
macro f1 scores = [ 0.8699507   0.91651342  0.90247237  0.88196256],0.892724761404
weighted average precision scores = [ 0.8771715   0.91811042  0.90971029  0.88614118],0.897783345454
weighted average recall scores = [ 0.8771715   0.91811042  0.90971029  0.88614118],0.897783345454
weighted f1 scores = [ 0.87510939  0.91761472  0.90682601  0.88437671],0.895981708942
ng20_raw_stems_bigrams_stopwords_df1_tf1  -->  precision  0.896757801301 recall  0.891494315774 f1  0.892724761404
loaded (199661) terms
vectorizing done, 199661 terms vocabulary tokenized
vectorizing done, 199661 terms vocabulary tokenized
accuracy scores = [ 0.87041357  0.91167728  0.90242347  0.87486699],0.889845329611
macro precision scores = [ 0.86884813  0.91157073  0.90238147  0.87464301],0.889360835951
macro recall scores = [ 0.86312902  0.90936354  0.89638955  0.87174865],0.885157687625
macro f1 scores = [ 0.8645425   0.91024562  0.8977773   0.87236774],0.886233288747
weighted average precision scores = [ 0.87073479  0.91185518  0.90437808  0.87666276],0.890907702208
weighted average recall scores = [ 0.87073479  0.91185518  0.90437808  0.87666276],0.890907702208
weighted f1 scores = [ 0.86947371  0.91157321  0.90201614  0.87497194],0.889508750014
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_bigrams  -->  precision  0.889360835951 recall  0.885157687625 f1  0.886233288747
loaded (56332) terms
extended to (72162) terms
done loading vocabulary
vectorizing done, 72162 terms vocabulary tokenized
vectorizing done, 72162 terms vocabulary tokenized
accuracy scores = [ 0.84390244  0.89575372  0.88095238  0.85252181],0.868282587157
macro precision scores = [ 0.84129692  0.89546684  0.87829868  0.85239583],0.866864569708
macro recall scores = [ 0.83518516  0.89353527  0.87430268  0.84918263],0.863051436218
macro f1 scores = [ 0.83653007  0.89431919  0.87503685  0.84966452],0.863887657012
weighted average precision scores = [ 0.84446421  0.89614268  0.8810998   0.85510135],0.869202010126
weighted average recall scores = [ 0.84446421  0.89614268  0.8810998   0.85510135],0.869202010126
weighted f1 scores = [ 0.84280311  0.89578055  0.87995476  0.85271763],0.867814011618
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_bigrams (extended unigrams) -->  precision  0.866864569708 recall  0.863051436218 f1  0.863887657012
loaded (152039) terms
vectorizing done, 152039 terms vocabulary tokenized
vectorizing done, 152039 terms vocabulary tokenized
accuracy scores = [ 0.86829268  0.90721868  0.89817177  0.86912109],0.88570105622
macro precision scores = [ 0.86553185  0.90758103  0.8974766   0.86839179],0.884745317084
macro recall scores = [ 0.86047359  0.9048299   0.89227219  0.86584694],0.880855655089
macro f1 scores = [ 0.86151127  0.90593353  0.89342823  0.86635449],0.881806880326
weighted average precision scores = [ 0.86795258  0.9075091   0.89980096  0.87102592],0.886572138783
weighted average recall scores = [ 0.86795258  0.9075091   0.89980096  0.87102592],0.886572138783
weighted f1 scores = [ 0.86699499  0.90713629  0.89770789  0.86934627],0.885296361674
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiktionary_bigrams  -->  precision  0.884745317084 recall  0.880855655089 f1  0.881806880326
loaded (8710) terms
extended to (13472) terms
done loading vocabulary
vectorizing done, 13472 terms vocabulary tokenized
vectorizing done, 13472 terms vocabulary tokenized
accuracy scores = [ 0.77242842  0.81995754  0.81887755  0.7765482 ],0.796952927464
macro precision scores = [ 0.76767891  0.81808295  0.81321655  0.77532999],0.793577101682
macro recall scores = [ 0.76344988  0.81607317  0.81162621  0.77235286],0.790875531316
macro f1 scores = [ 0.76379043  0.81668462  0.81149196  0.77227976],0.791061694166
weighted average precision scores = [ 0.771005    0.82096455  0.81800262  0.77879681],0.7971922434
weighted average recall scores = [ 0.771005    0.82096455  0.81800262  0.77879681],0.7971922434
weighted f1 scores = [ 0.77033753  0.82006596  0.81761006  0.77611456],0.796032028553
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiktionary_bigrams (extended unigrams) -->  precision  0.793577101682 recall  0.790875531316 f1  0.791061694166
loaded (190345) terms
vectorizing done, 190345 terms vocabulary tokenized
vectorizing done, 190345 terms vocabulary tokenized
accuracy scores = [ 0.86829268  0.90997877  0.89944728  0.87146201],0.887295185903
macro precision scores = [ 0.86551418  0.90983428  0.89917946  0.87114836],0.886419070667
macro recall scores = [ 0.8603927   0.90740401  0.89348079  0.86839919],0.882419171451
macro f1 scores = [ 0.86143798  0.9083673   0.89478777  0.86893049],0.883380885921
weighted average precision scores = [ 0.86801613  0.91017359  0.90132946  0.87339755],0.888229182942
weighted average recall scores = [ 0.86801613  0.91017359  0.90132946  0.87339755],0.888229182942
weighted f1 scores = [ 0.86699678  0.90986065  0.89903269  0.87162408],0.886878548221
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ google_bigrams  -->  precision  0.886419070667 recall  0.882419171451 f1  0.883380885921
loaded (47016) terms
extended to (54812) terms
done loading vocabulary
vectorizing done, 54812 terms vocabulary tokenized
vectorizing done, 54812 terms vocabulary tokenized
accuracy scores = [ 0.81823966  0.86348195  0.85693027  0.82081294],0.839866206245
macro precision scores = [ 0.81427428  0.86113719  0.85499201  0.81966167],0.837516287237
macro recall scores = [ 0.80875925  0.86013404  0.85027429  0.81671584],0.833970857704
macro f1 scores = [ 0.80952346  0.86044408  0.85102939  0.81706821],0.834516282557
weighted average precision scores = [ 0.81806893  0.86321312  0.85789238  0.82347104],0.840661367113
weighted average recall scores = [ 0.81806893  0.86321312  0.85789238  0.82347104],0.840661367113
weighted f1 scores = [ 0.81653158  0.86317316  0.85597146  0.82103835],0.8391786347
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ google_bigrams (extended unigrams) -->  precision  0.837516287237 recall  0.833970857704 f1  0.834516282557
loaded (200752) terms
vectorizing done, 200752 terms vocabulary tokenized
vectorizing done, 200752 terms vocabulary tokenized
accuracy scores = [ 0.87041357  0.91210191  0.90221088  0.8750798 ],0.889951543274
macro precision scores = [ 0.86883682  0.91196795  0.90222054  0.87479462],0.889454982896
macro recall scores = [ 0.86312902  0.90976842  0.89613347  0.87200775],0.885259664287
macro f1 scores = [ 0.86454712  0.91065108  0.89753303  0.87257932],0.886327639142
weighted average precision scores = [ 0.87073918  0.91227113  0.9042284   0.87672669],0.890991347139
weighted average recall scores = [ 0.87073918  0.91227113  0.9042284   0.87672669],0.890991347139
weighted f1 scores = [ 0.86948693  0.91199813  0.90180274  0.87511446],0.889600565404
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_bigrams_vw  -->  precision  0.889454982896 recall  0.885259664287 f1  0.886327639142
loaded (57423) terms
extended to (73391) terms
done loading vocabulary
vectorizing done, 73391 terms vocabulary tokenized
vectorizing done, 73391 terms vocabulary tokenized
accuracy scores = [ 0.84559915  0.89490446  0.88031463  0.85145776],0.868068998266
macro precision scores = [ 0.84272354  0.89443553  0.87763877  0.85156341],0.866590310367
macro recall scores = [ 0.83686943  0.8926011   0.87345951  0.84826449],0.862798633321
macro f1 scores = [ 0.83817841  0.89334171  0.87418108  0.84872808],0.863607320962
weighted average precision scores = [ 0.84607395  0.89521582  0.88054547  0.85404581],0.86897026124
weighted average recall scores = [ 0.84607395  0.89521582  0.88054547  0.85404581],0.86897026124
weighted f1 scores = [ 0.84452972  0.89489845  0.87927008  0.85159995],0.867574551503
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_bigrams_vw (extended unigrams) -->  precision  0.866590310367 recall  0.862798633321 f1  0.863607320962
loaded (228162) terms
vectorizing done, 228162 terms vocabulary tokenized
vectorizing done, 228162 terms vocabulary tokenized
accuracy scores = [ 0.86977731  0.9118896   0.90263605  0.87593105],0.890058501663
macro precision scores = [ 0.86860837  0.91172103  0.90274994  0.87555814],0.889659370081
macro recall scores = [ 0.86235655  0.90934489  0.89652722  0.87281737],0.885261508
macro f1 scores = [ 0.8637876   0.91029322  0.89797428  0.87338518],0.886360071083
weighted average precision scores = [ 0.87021534  0.91205637  0.90465332  0.87768455],0.891152392318
weighted average recall scores = [ 0.87021534  0.91205637  0.90465332  0.87768455],0.891152392318
weighted f1 scores = [ 0.86871483  0.91177091  0.90221969  0.87604032],0.889686439397
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_google_bigrams_vw  -->  precision  0.889659370081 recall  0.885261508 f1  0.886360071083
loaded (84833) terms
extended to (101486) terms
done loading vocabulary
vectorizing done, 101486 terms vocabulary tokenized
vectorizing done, 101486 terms vocabulary tokenized
accuracy scores = [ 0.84687169  0.89660297  0.88116497  0.85592679],0.870141604357
macro precision scores = [ 0.84437385  0.8960874   0.87898456  0.85610228],0.868887020853
macro recall scores = [ 0.8380005   0.89407547  0.87470877  0.8529919 ],0.864944159592
macro f1 scores = [ 0.83928454  0.89488899  0.87549589  0.85348682],0.865789059611
weighted average precision scores = [ 0.84724895  0.89676478  0.88188461  0.85833417],0.871058126665
weighted average recall scores = [ 0.84724895  0.89676478  0.88188461  0.85833417],0.871058126665
weighted f1 scores = [ 0.84554196  0.89651696  0.88035318  0.85609274],0.869626211124
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_google_bigrams_vw (extended unigrams) -->  precision  0.868887020853 recall  0.864944159592 f1  0.865789059611
loaded (194359) terms
vectorizing done, 194359 terms vocabulary tokenized
vectorizing done, 194359 terms vocabulary tokenized
accuracy scores = [ 0.86850477  0.91019108  0.89987245  0.87188764],0.887613984863
macro precision scores = [ 0.86576371  0.90996376  0.89957491  0.87153203],0.886708600281
macro recall scores = [ 0.86047832  0.90754835  0.89388474  0.86884992],0.882690331885
macro f1 scores = [ 0.861482    0.90850924  0.89518217  0.86938678],0.883640048028
weighted average precision scores = [ 0.86824524  0.91040322  0.90173634  0.87380203],0.888546707088
weighted average recall scores = [ 0.86824524  0.91040322  0.90173634  0.87380203],0.888546707088
weighted f1 scores = [ 0.86712642  0.91008265  0.89944221  0.87208693],0.887184554196
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiktionary_google_bigrams_vw  -->  precision  0.886708600281 recall  0.882690331885 f1  0.883640048028
loaded (51030) terms
extended to (59828) terms
done loading vocabulary
vectorizing done, 59828 terms vocabulary tokenized
vectorizing done, 59828 terms vocabulary tokenized
accuracy scores = [ 0.82099682  0.86857749  0.86118197  0.82464354],0.843849956831
macro precision scores = [ 0.81763376  0.86674938  0.85914196  0.82396881],0.841873477002
macro recall scores = [ 0.81213264  0.86590166  0.85465832  0.82079493],0.838371886302
macro f1 scores = [ 0.81304212  0.86610304  0.8554077   0.82121458],0.838941861729
weighted average precision scores = [ 0.82103949  0.86849985  0.86212722  0.82752545],0.844798000873
weighted average recall scores = [ 0.82103949  0.86849985  0.86212722  0.82752545],0.844798000873
weighted f1 scores = [ 0.81950469  0.86832361  0.86031989  0.82494218],0.843272589889
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.841873477002 recall  0.838371886302 f1  0.838941861729
loaded (229029) terms
vectorizing done, 229029 terms vocabulary tokenized
vectorizing done, 229029 terms vocabulary tokenized
accuracy scores = [ 0.8699894   0.91210191  0.90284864  0.87593105],0.890217748747
macro precision scores = [ 0.86857471  0.91193401  0.90297449  0.87562533],0.88977713288
macro recall scores = [ 0.86255736  0.90954319  0.8967313   0.87283355],0.885416348365
macro f1 scores = [ 0.86396127  0.91049744  0.89818732  0.87340739],0.886513355639
weighted average precision scores = [ 0.87034644  0.91227618  0.90488964  0.87771129],0.89130588671
weighted average recall scores = [ 0.87034644  0.91227618  0.90488964  0.87771129],0.89130588671
weighted f1 scores = [ 0.86894393  0.91198552  0.90244282  0.87603429],0.889851638452
ng20_raw_stems_unigrams_stopwords_df1_tf1 ^ ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw  -->  precision  0.88977713288 recall  0.885416348365 f1  0.886513355639
loaded (85700) terms
extended to (102466) terms
done loading vocabulary
vectorizing done, 102466 terms vocabulary tokenized
vectorizing done, 102466 terms vocabulary tokenized
accuracy scores = [ 0.84793213  0.89639066  0.8807398   0.85507555],0.870034533394
macro precision scores = [ 0.84499258  0.89556144  0.87855955  0.85519341],0.868576745784
macro recall scores = [ 0.83901363  0.89387182  0.87428689  0.8521001 ],0.864818110161
macro f1 scores = [ 0.8402424   0.89455267  0.875099    0.85250254],0.865599152119
weighted average precision scores = [ 0.84819208  0.89654307  0.8815584   0.85755532],0.870962218588
weighted average recall scores = [ 0.84819208  0.89654307  0.8815584   0.85755532],0.870962218588
weighted f1 scores = [ 0.84664283  0.89631813  0.87999327  0.85519697],0.86953779933
ng20_raw_stems_bigrams_stopwords_df1_tf1 ^ wiki_wiktionary_google_bigrams_vw (extended unigrams) -->  precision  0.868576745784 recall  0.864818110161 f1  0.865599152119
done!

