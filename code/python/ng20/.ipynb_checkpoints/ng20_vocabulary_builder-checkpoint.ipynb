{
 "metadata": {
  "name": "",
  "signature": "sha256:7428391d57096712b520bb5bf9d297d4643f4e02c5f1b1d94517bc9f85fadadd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "20ng Vocabulary Builder"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Build stemmed and lemmatized vocabulary (unigrams + bigrams) from 20ng corpus and store into DB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf):\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    # tokenize text\n",
      "    #vectorizer = CountVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),stop_words=stop_words)\n",
      "    vectorizer = CountVectorizer(max_df=max_df,min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),stop_words=stop_words)\n",
      "    corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "\n",
      "    # apply minimum term frequency threshold\n",
      "    term_freq = corpus_vectors.sum(axis=0) # sum on culomns to obtain term frequencies\n",
      "    terms_to_remove = []\n",
      "    for k,v in vectorizer.vocabulary_.iteritems():\n",
      "        if(term_freq[0,vectorizer.vocabulary_[k]]<min_tf):\n",
      "            terms_to_remove.append(k)\n",
      "\n",
      "    print 'removing ({0}) terms under tf threshold'.format(len(terms_to_remove))\n",
      "    for k in terms_to_remove:\n",
      "        del vectorizer.vocabulary_[k]\n",
      "\n",
      "    return vectorizer.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_vocabulary_new(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf):\n",
      "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    # tokenize text\n",
      "    #vectorizer = CountVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),stop_words=stop_words)\n",
      "    vectorizer = TfidfVectorizer(max_df=max_df,min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),stop_words='english')\n",
      "    corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "\n",
      "    return vectorizer.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_vocabulary(vocabulary,tbl_name):\n",
      "    # save vocabulary in DB for future use\n",
      "    import sqlite3 as sqlitedb\n",
      "    from ng20_globals import *\n",
      "\n",
      "    tbl_name = tbl_name.replace('.','_')\n",
      "    l = []\n",
      "    l.extend([i] for i in vocabulary)\n",
      "    con = sqlitedb.connect(db_path)    \n",
      "    with con:\n",
      "        con.execute('drop table if exists {0}'.format(tbl_name))\n",
      "        con.execute('create table {0}(term text)'.format(tbl_name))\n",
      "        con.executemany('insert into {0}(term) values(?)'.format(tbl_name),l)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build raw unigrams vocabulary\n",
      "def build_raw_unigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,1,1)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_unigrams'.format()\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build raw unigrams vocabulary\n",
      "def build_raw_unigrams_vocabulary_new(corpus):\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,1,1)\n",
      "    vocabulary_new = build_vocabulary_new(corpus,tokenizer,stop_words,max_ngram_size,1,1)\n",
      "    print len(vocabulary),',',len(vocabulary_new)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_unigrams'.format()\n",
      "    #save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build unigrams vocabulary\n",
      "def build_unigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_unigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_unigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_lemmas_unigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized test unigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_test_unigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_lemmas_test_unigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed test unigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_test_unigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_stems_test_unigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized all unigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_all_unigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_lemmas_all_unigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed all unigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_all_unigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_stems_all_unigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_lemmatized_unigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_lemmas_unigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build unigrams stopwords vocabulary\n",
      "def build_unigrams_stopwords_vocabulary(corpus,stop_words,):\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_unigrams_stopwords_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build bigrams stopwords vocabulary\n",
      "def build_bigrams_stopwords_vocabulary(corpus,stop_words,):\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_bigrams_stopwords_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build raw unigrams stopwords vocabulary\n",
      "def build_raw_unigrams_stopwords_vocabulary(corpus,stop_words,):\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,1,1)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_unigrams_stopwords'.format()\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build raw bigrams stopwords vocabulary\n",
      "def build_raw_bigrams_stopwords_vocabulary(corpus,stop_words,):\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,1,1)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_bigrams_stopwords'.format()\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams stopwords vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_unigrams_stopwords_vocabulary(corpus,stop_words,):\n",
      "    from ng20_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_lemmas_unigrams_stopwords_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams stopwords vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_lemmatized_unigrams_stopwords_vocabulary(corpus,stop_words,):\n",
      "    from ng20_globals import *\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_lemmas_unigrams_stopwords_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build raw bigrams vocabulary\n",
      "def build_raw_bigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,1,1)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_bigrams'.format()\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized bigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_bigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_lemmas_bigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized all bigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_all_bigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_lemmas_all_bigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed all bigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_all_bigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_stems_all_bigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized test bigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_test_bigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_lemmas_test_bigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed test bigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_test_bigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_stems_test_bigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build bigrams vocabulary\n",
      "def build_bigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_bigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized bigrams vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_lemmatized_bigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_lemmas_bigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized bigrams stopwords vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_bigrams_stopwords_vocabulary(corpus,stop_words,):\n",
      "    from ng20_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_lemmas_bigrams_stopwords_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized bigrams stopwords vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_lemmatized_bigrams_stopwords_vocabulary(corpus,stop_words,):\n",
      "    from ng20_globals import *\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_lemmas_bigrams_stopwords_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed unigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_unigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer()    \n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_stems_unigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed unigrams vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_stemmed_unigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer()    \n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_stems_unigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed unigrams stopwords vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_unigrams_stopwords_vocabulary(corpus,stop_words,):\n",
      "    from ng20_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer()    \n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_stems_unigrams_stopwords_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed unigrams stopwords vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_stemmed_unigrams_stopwords_vocabulary(corpus,stop_words,):\n",
      "    from ng20_globals import *\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer()    \n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_stems_unigrams_stopwords_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed bigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_bigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer() \n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2    \n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_stems_bigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed bigrams vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_stemmed_bigrams_vocabulary(corpus):\n",
      "    from ng20_globals import *\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer() \n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2    \n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_stems_bigrams_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed bigrams stopwords vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_bigrams_stopwords_vocabulary(corpus,stop_words,):\n",
      "    from ng20_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer() \n",
      "    max_ngram_size = 2    \n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_raw_stems_bigrams_stopwords_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed bigrams stopwords vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_stemmed_bigrams_stopwords_vocabulary(corpus,stop_words,):\n",
      "    from ng20_globals import *\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer() \n",
      "    max_ngram_size = 2    \n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'ng20_stems_bigrams_stopwords_df{0}_tf{1}'.format(min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build():\n",
      "    from ng20_corpus_loader import load_corpus\n",
      "    from stopwords_loader import load_inquiry_stopwords\n",
      "    \n",
      "    # load 20ng docs from DB\n",
      "    corpus_all = load_corpus('both')\n",
      "    corpus = corpus_all['corpus']\n",
      "    \n",
      "    # build vocabulary without stopwords removal\n",
      "    build_raw_unigrams_vocabulary(corpus)\n",
      "    #build_unigrams_vocabulary(corpus)\n",
      "    #build_lemmatized_unigrams_vocabulary(corpus)\n",
      "    build_raw_lemmatized_unigrams_vocabulary(corpus)    \n",
      "    #build_lemmatized_bigrams_vocabulary(corpus)\n",
      "    build_raw_lemmatized_bigrams_vocabulary(corpus)\n",
      "    build_raw_bigrams_vocabulary(corpus)\n",
      "    #build_bigrams_vocabulary(corpus)\n",
      "    #build_stemmed_unigrams_vocabulary(corpus)\n",
      "    build_raw_stemmed_unigrams_vocabulary(corpus)\n",
      "    #build_stemmed_bigrams_vocabulary(corpus)\n",
      "    build_raw_stemmed_bigrams_vocabulary(corpus)\n",
      "\n",
      "    # load inquiry stopwords list\n",
      "    stop_words = load_inquiry_stopwords()\n",
      "\n",
      "    # build vocabulary with stopwords removal\n",
      "    #build_unigrams_stopwords_vocabulary(corpus,stop_words)\n",
      "    build_raw_unigrams_stopwords_vocabulary(corpus,stop_words)\n",
      "    #build_bigrams_stopwords_vocabulary(corpus,stop_words)\n",
      "    build_raw_bigrams_stopwords_vocabulary(corpus,stop_words)\n",
      "    #build_lemmatized_unigrams_stopwords_vocabulary(corpus,stop_words)\n",
      "    build_raw_lemmatized_unigrams_stopwords_vocabulary(corpus,stop_words)\n",
      "    #build_lemmatized_bigrams_stopwords_vocabulary(corpus,stop_words)\n",
      "    build_raw_lemmatized_bigrams_stopwords_vocabulary(corpus,stop_words)\n",
      "    #build_stemmed_unigrams_stopwords_vocabulary(corpus,stop_words)\n",
      "    build_raw_stemmed_unigrams_stopwords_vocabulary(corpus,stop_words)\n",
      "    #build_stemmed_bigrams_stopwords_vocabulary(corpus,stop_words)\n",
      "    build_raw_stemmed_bigrams_stopwords_vocabulary(corpus,stop_words)\n",
      "        \n",
      "    #corpus_test = load_corpus(,'test')\n",
      "    #build_raw_lemmatized_test_unigrams_vocabulary(corpus_test['corpus'],)    \n",
      "    #build_raw_stemmed_test_unigrams_vocabulary(corpus_test['corpus'],)    \n",
      "    #build_raw_lemmatized_test_bigrams_vocabulary(corpus_test['corpus'],)            \n",
      "    #build_raw_stemmed_test_bigrams_vocabulary(corpus_test['corpus'],)            \n",
      "    \n",
      "    #corpus = load_corpus(,'both')\n",
      "    #build_raw_lemmatized_all_unigrams_vocabulary(corpus['corpus'],)    \n",
      "    #build_raw_stemmed_all_unigrams_vocabulary(corpus['corpus'],)    \n",
      "    #build_raw_lemmatized_all_bigrams_vocabulary(corpus['corpus'],)\n",
      "    #build_raw_stemmed_all_bigrams_vocabulary(corpus['corpus'],)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build 20ng docs vocabulary\n",
      "build()\n",
      "print 'done!'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}