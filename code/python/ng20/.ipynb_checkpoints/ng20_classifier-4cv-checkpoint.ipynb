{
 "metadata": {
  "name": "",
  "signature": "sha256:ab411e5bf1fcbeac3241752adda3a52bcb943ab5ed4a9d2758cd0d3f86ade63d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classify 20ng docs"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Classifiy 20ng docs using unigrams and bigrams features with different classifiers and report classification results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size):\n",
      "    # tokenize text\n",
      "    import numpy as np\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "    from sklearn.feature_extraction.text import TfidfTransformer\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    # generate corpus vectors\n",
      "    vectorizer = CountVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),vocabulary=vocabulary,stop_words={})\n",
      "    corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "    \n",
      "    print 'vectorizing done, {0} terms vocabulary tokenized'.format(len(vectorizer.vocabulary_))\n",
      "    \n",
      "    # generate tfidf vectors\n",
      "    transformer = TfidfTransformer()\n",
      "    corpus_tfidf_vectors = transformer.fit_transform(corpus_vectors)\n",
      "\n",
      "    return vectorizer, corpus_tfidf_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize_corpus_new(corpus,tokenizer,vocabulary,max_ngram_size):\n",
      "    # tokenize text\n",
      "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    from ng20_globals import *\n",
      "    \n",
      "    # generate corpus vectors\n",
      "    vectorizer = TfidfVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),vocabulary=vocabulary,stop_words={})\n",
      "    corpus_tfidf_vectors = vectorizer.fit_transform(corpus)\n",
      "    \n",
      "    print 'vectorizing done, {0} terms vocabulary tokenized'.format(len(vectorizer.vocabulary_))\n",
      "    \n",
      "    return corpus_tfidf_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# given classifier predictions probabilities, return predictions with top n probabilities > 0.5 for each instance or greatest one if all are <=0.5\n",
      "def get_max_n_pred(pred_proba, n_pred, threshold):\n",
      "    import heapq\n",
      "    import numpy\n",
      "    max_n_pred = numpy.ndarray(shape=pred_proba.shape)\n",
      "    for i in range(len(pred_proba)):\n",
      "        largest_n_proba = heapq.nlargest(n_pred,pred_proba[i])\n",
      "        max_n_pred[i] = numpy.array(((pred_proba[i]>threshold) & (pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1))\n",
      "        if max_n_pred[i].sum(axis=0)==0: # at least one label should be returned\n",
      "            max_n_pred[i] = numpy.array(((pred_proba[i]>=max(pred_proba[i])) & 1))\n",
      "    return max_n_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_top_feature_names(classifier, feature_names, label_names, max_features=20):\n",
      "    import numpy as np\n",
      "    from sklearn.utils.extmath import density\n",
      "    if hasattr(classifier,'coef_'):\n",
      "        print 'dimensionality: {0}'.format(classifier.coef_.shape[1])\n",
      "        print 'density: {0}'.format(density(classifier.coef_))\n",
      "        for i, category in enumerate(label_names):\n",
      "            top = np.argsort(classifier.coef_[i])[max_features*-1:]\n",
      "            print category,\",\".join(feature_names[top])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reference: http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification\n",
      "def classify(x_train,y_train,x_test,y_test,max_labels):\n",
      "    from sklearn.preprocessing import MultiLabelBinarizer\n",
      "    from sklearn.multiclass import OneVsRestClassifier\n",
      "    from sklearn.svm import SVC\n",
      "    from sklearn.svm import LinearSVC\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "    from sklearn.naive_bayes import MultinomialNB\n",
      "    from sklearn.naive_bayes import BernoulliNB\n",
      "    from sklearn.preprocessing import binarize\n",
      "    from sklearn.cross_validation import train_test_split\n",
      "    from sklearn import metrics\n",
      "    import numpy as np\n",
      "    from numpy import mean\n",
      "    import scipy\n",
      "    from sklearn.cross_validation import cross_val_score\n",
      "    from sklearn.metrics import make_scorer\n",
      "    from sklearn.metrics import precision_score\n",
      "    from sklearn.metrics import recall_score\n",
      "    from sklearn.metrics import f1_score\n",
      "    \n",
      "    # combine train and test vectors\n",
      "    #print 'merging training and testing samples x({0}),x({1})'.format(x_train.shape,x_test.shape)\n",
      "    #print 'merging training and testing samples y({0}),y({1})'.format(len(y_train),len(y_test))\n",
      "    if x_test.shape[0]>0:\n",
      "        x = scipy.sparse.vstack((x_train,x_test))\n",
      "        y = y_train + y_test\n",
      "    else:\n",
      "        x = x_train\n",
      "        y = y_train\n",
      "    #print 'merged into x({0})'.format(x.shape)\n",
      "    #print 'merged into y({0})'.format(len(y))\n",
      "    # binarize the labels\n",
      "    #mlb = MultiLabelBinarizer()\n",
      "    #y_train_binarized = mlb.fit_transform(y_train)\n",
      "    \n",
      "    # train/test split\n",
      "    #corpus_tfidf_vectors, labels_binarized = shuffle(corpus_tfidf_vectors, labels_binarized)\n",
      "    #x_train, x_test, y_train, y_test = train_test_split(corpus_tfidf_vectors, labels_binarized, test_size=test_size, random_state=1)\n",
      "    \n",
      "    # classify\n",
      "    #cls = OneVsRestClassifier(LogisticRegression(class_weight='auto'))\n",
      "    #cls = OneVsRestClassifier(LogisticRegression())\n",
      "    #cls = MultinomialNB(alpha=0.01)\n",
      "    #cls = OneVsRestClassifier(BernoulliNB()need binarize(x_train and x_test))\n",
      "    #cls = OneVsRestClassifier(SVC(kernel='linear',probability=True,max_iter=1000))\n",
      "    #cls = LinearSVC(dual=False,penalty='l1')\n",
      "    cls = LinearSVC()\n",
      "    acc_scores = cross_val_score(cls,x,y,scoring='accuracy',cv=4,n_jobs=-1)\n",
      "    print 'accuracy scores = {0},{1}'.format(acc_scores,mean(acc_scores))\n",
      "    macro_p_scores = cross_val_score(cls,x,y,scoring=make_scorer(precision_score,average='macro'),cv=4,n_jobs=-1)\n",
      "    print 'macro precision scores = {0},{1}'.format(macro_p_scores,mean(macro_p_scores))\n",
      "    macro_r_scores = cross_val_score(cls,x,y,scoring=make_scorer(recall_score,average='macro'),cv=4,n_jobs=-1)\n",
      "    print 'macro recall scores = {0},{1}'.format(macro_r_scores,mean(macro_r_scores))\n",
      "    macro_f1_scores = cross_val_score(cls,x,y,scoring=make_scorer(f1_score,average='macro'),cv=4,n_jobs=-1)\n",
      "    print 'macro f1 scores = {0},{1}'.format(macro_f1_scores,mean(macro_f1_scores))\n",
      "    p_scores = cross_val_score(cls,x,y,scoring=make_scorer(precision_score,average='weighted'),cv=4,n_jobs=-1)\n",
      "    print 'weighted average precision scores = {0},{1}'.format(p_scores,mean(p_scores))\n",
      "    r_scores = cross_val_score(cls,x,y,scoring=make_scorer(precision_score,average='weighted'),cv=4,n_jobs=-1)\n",
      "    print 'weighted average recall scores = {0},{1}'.format(r_scores,mean(r_scores))\n",
      "    f1_scores = cross_val_score(cls,x,y,scoring=make_scorer(f1_score,average='weighted'),cv=4,n_jobs=-1)\n",
      "    print 'weighted f1 scores = {0},{1}'.format(f1_scores,mean(f1_scores))\n",
      "   \n",
      "    cls.fit(x,y)\n",
      "    \n",
      "    return {'classifier':cls,\n",
      "            'precision':mean(macro_p_scores),\n",
      "            'recall':mean(macro_r_scores),\n",
      "            'f1':mean(macro_f1_scores)}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn\n",
      "from sklearn.svm import LinearSVC\n",
      "c = LinearSVC()\n",
      "print sklearn.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_all_unigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import RawTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 1\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = None\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}{1}_unigrams{2}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_unigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 1\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_all_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = None\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}{1}_bigrams{2}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "\n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "\n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_bigrams_with_LSA(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens,num_components):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    from sklearn.decomposition import TruncatedSVD\n",
      "    from scipy import sparse\n",
      "    import numpy\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # apply LSA\n",
      "    #print numpy.max(corpus_train_tfidf_vectors)\n",
      "    #print numpy.min(corpus_train_tfidf_vectors)\n",
      "    lsa = TruncatedSVD(n_components=num_components)\n",
      "    lsa.fit(corpus_train_tfidf_vectors)\n",
      "    #corpus_train_tfidf_vectors = numpy.dot(corpus_train_tfidf_vectors,pca.components_.transpose())\n",
      "    corpus_train_tfidf_vectors = lsa.transform(corpus_train_tfidf_vectors)\n",
      "    corpus_test_tfidf_vectors = lsa.transform(corpus_test_tfidf_vectors)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "\n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print 'LSA ^' , vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_unigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer    \n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 1\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()    \n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()    \n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)    \n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "\n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer    \n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "\n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "        \n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_wiki_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')\n",
      "    print 'done loading vocabulary'\n",
      "    \n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "\n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary_extend_unigrams\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    if len(bigrams_src)==1:\n",
      "        vocabulary_tbl_intersect = '{0}_bigrams'.format(bigrams_src[0])\n",
      "    else:\n",
      "        vocabulary_tbl_intersect = '{0}_'.format(bigrams_src[0])\n",
      "        for i in range(len(bigrams_src)-1):\n",
      "            vocabulary_tbl_intersect = '{0}{1}_'.format(vocabulary_tbl_intersect,bigrams_src[i+1])\n",
      "        vocabulary_tbl_intersect = '{0}bigrams_vw'.format(vocabulary_tbl_intersect)\n",
      "        \n",
      "    vocabulary = load_common_vocabulary_extend_unigrams(vocabulary_tbl_name,vocabulary_tbl_intersect,'lemma')\n",
      "    print 'done loading vocabulary'\n",
      "    \n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name,'^',vocabulary_tbl_intersect,'(extended unigrams) --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_wiktionary_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()    \n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()    \n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiktionary_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')\n",
      "    print 'done loading vocabulary'\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_google_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'google_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')    \n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_wiktionary_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_wiki_google_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_google_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiktionary_google_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_wiki_wiktionary_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_wiktionary_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_wiki_google_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_google_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_wiktionary_google_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiktionary_google_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_all_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_lemmas{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_lemmas{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_wiktionary_google_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'lemma')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_wiki_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()    \n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()    \n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')    \n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_bigrams_unigrams(bigrams_src,corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary_extend_unigrams\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    if len(bigrams_src)==1:\n",
      "        vocabulary_tbl_intersect = '{0}_bigrams'.format(bigrams_src[0])\n",
      "    else:\n",
      "        vocabulary_tbl_intersect = '{0}_'.format(bigrams_src[0])\n",
      "        for i in range(len(bigrams_src)-1):\n",
      "            vocabulary_tbl_intersect = '{0}{1}_'.format(vocabulary_tbl_intersect,bigrams_src[i+1])\n",
      "        vocabulary_tbl_intersect = '{0}bigrams_vw'.format(vocabulary_tbl_intersect)\n",
      "        \n",
      "    vocabulary = load_common_vocabulary_extend_unigrams(vocabulary_tbl_name,vocabulary_tbl_intersect,'stem')\n",
      "    print 'done loading vocabulary'\n",
      "    \n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name,'^',vocabulary_tbl_intersect,'(extended unigrams) --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_wiktionary_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiktionary_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_google_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'google_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_all_bigrams(corpus_train_data,corpus_test_data,label_names,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    import numpy as np\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    from ng20_globals import *\n",
      "    from ng20_vocabulary_loader import load_common_vocabulary\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = StemmingTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawStemmingTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name1 = 'ng20{0}_stems{1}_unigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary_tbl_name2 = 'ng20{0}_stems{1}_bigrams{2}_df{3}_tf{4}'.format(raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    \n",
      "    vocabulary_tbl_intersect = 'wiki_wiktionary_google_bigrams_vw'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_name1,vocabulary_tbl_name2,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    vectorizer, corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    _, corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       max_labels)\n",
      "    \n",
      "    print_top_feature_names(results['classifier'], np.asarray(vectorizer.get_feature_names()), label_names)\n",
      "    \n",
      "    print vocabulary_tbl_name1,'^',vocabulary_tbl_name2,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test():\n",
      "    from ng20_corpus_loader import load_corpus_and_labels\n",
      "    from ng20_corpus_loader import load_corpus_with_labels_mappings\n",
      "    from ng20_corpus_loader import load_label_names\n",
      "    \n",
      "    # load 20ng docs with class lables from DB\n",
      "    corpus_train_data = load_corpus_and_labels('train')\n",
      "    print 'done loading {0} train records and {1} labels.'.format(len(corpus_train_data['corpus']),len(corpus_train_data['labels_dic']))\n",
      "\n",
      "    corpus_test_data = load_corpus_with_labels_mappings('test',corpus_train_data['labels_dic'])\n",
      "    print 'done loading {0} test records.'.format(len(corpus_test_data['corpus']))\n",
      "    \n",
      "    label_names = load_label_names()\n",
      "    \n",
      "    stopwords_removal_mask = 1\n",
      "    chi_features_mask = 2\n",
      "    raw_tokens_mask = 4\n",
      "    for i in range(4,6): # test w/o stopword removal and w/o chi square features and w/o raw tokens\n",
      "        stopwords_removal = i&stopwords_removal_mask==stopwords_removal_mask\n",
      "        use_chi_features = i&chi_features_mask==chi_features_mask\n",
      "        use_raw_tokens = i&raw_tokens_mask==raw_tokens_mask\n",
      "        \n",
      "        test_all_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_all_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        #test_lemmatized_bigrams_with_LSA({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "        #                         {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "        #                         label_names,stopwords_removal,use_chi_features,use_raw_tokens,113240)\n",
      "        \n",
      "        test_lemmatized_wiki_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['wiki'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_wiki_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['wiki','wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_wiki_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['wiki','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_wiktionary_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_all_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_lemmatized_bigrams_unigrams(['wiki','wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_wiki_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['wiki'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_wiki_wiktionary_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['wiki','wiktionary'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_wiki_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['wiki','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_wiktionary_google_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_all_bigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)\n",
      "        \n",
      "        test_stemmed_bigrams_unigrams(['wiki','wiktionary','google'],{'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                                 {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                                 label_names,stopwords_removal,use_chi_features,use_raw_tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test using all vocabulary\n",
      "test()\n",
      "\n",
      "print 'done!'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}