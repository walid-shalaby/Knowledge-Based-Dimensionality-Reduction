{
 "metadata": {
  "name": "",
  "signature": "sha256:fd2e2b2d3c6dee162694ead36b5c09cf7b91d3607b109121152caaca41b2391b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "CLEF-IP2010 Chi Square Vocabulary Builder"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Build stemmed and lemmatized vocabulary (unigrams + bigrams) from clef-ip2010 corpus and selec top top n chi square features then store into DB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_chi_vocabulary(corpus,labels,initial_vocabulary,tokenizer,stop_words,max_ngram_size,min_df,min_tf):    \n",
      "    from clef_globals import *\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "    from sklearn.feature_selection import chi2    \n",
      "    from sklearn.feature_selection import SelectKBest\n",
      "    import heapq\n",
      "    import numpy\n",
      "    \n",
      "    # tokenize text using initial vocabulary (achieving min_df and min_tf)\n",
      "    vectorizer = CountVectorizer(tokenizer=tokenizer,ngram_range=(1,max_ngram_size),stop_words=stop_words,vocabulary=initial_vocabulary)\n",
      "    corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "\n",
      "    vocabulary = dict();\n",
      "    \n",
      "    # apply maximum chi square threshold for each class\n",
      "    print 'applying chi2 threshold on {0} labels'.format(labels.shape[0])\n",
      "    for i in range(labels.shape[0]): # each row i in labels 2d-array is [0-1] class membership for class i\n",
      "        # select top n features for this class label\n",
      "        ch2 = SelectKBest(chi2, k=max_chi_square_terms)\n",
      "        ch2.fit(corpus_vectors, labels[i])\n",
      "        top_n_terms = ch2.get_support()\n",
      "        #print 'found {0} top terms'.format(sum((top_n_terms&1)))\n",
      "        #chi_scores,_ = chi2(corpus_vectors, labels[i])\n",
      "        #nans = sum(numpy.isnan(chi_scores))\n",
      "        #if nans>0:\n",
      "        #    print 'chi2 returned {} nans'.format(nans)\n",
      "        \n",
      "        #top_n_scores = heapq.nlargest(max_chi_square_terms,chi_scores)\n",
      "        #top_n_terms = numpy.array((chi_scores>=min(top_n_scores)) & 1)\n",
      "        #print 'found {0} top terms with minimum chi2={1}'.format(sum(top_n_terms),min(top_n_scores))\n",
      "        \n",
      "        # loop on initial vocabulary and choose only top ones\n",
      "        for k,v in vectorizer.vocabulary_.iteritems():\n",
      "            if top_n_terms[v]==True:\n",
      "                if vocabulary.has_key(k)==False: # this is a term whose chi2 is at least min chi2 and not already in vocabulary\n",
      "                    vocabulary[k] = len(vocabulary)\n",
      "    print 'found {0} terms'.format(len(vocabulary))\n",
      "    return vocabulary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_vocabulary(vocabulary,tbl_name):\n",
      "    # save vocabulary in DB for future use\n",
      "    import sqlite3 as sqlitedb\n",
      "    from clef_globals import *\n",
      "\n",
      "    l = []\n",
      "    l.extend([i] for i in vocabulary)\n",
      "    con = sqlitedb.connect(db_path)\n",
      "    with con:\n",
      "        con.execute('drop table if exists {0}'.format(tbl_name))\n",
      "        con.execute('create table {0}(term text)'.format(tbl_name))\n",
      "        con.executemany('insert into {0}(term) values(?)'.format(tbl_name),l)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized top n chi square raw unigrams vocabulary\n",
      "def build_raw_lemmatized_chi_unigrams_vocabulary(corpus,labels,vocabulary_src):\n",
      "    from clef_globals import *\n",
      "    from clef_vocabulary_loader import load_vocabulary\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    \n",
      "    # load initial vocabulary\n",
      "    initial_vocabulary_tbl_name = 'clef_2010_{0}_raw_lemmas_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    initial_vocabulary = load_vocabulary(initial_vocabulary_tbl_name)\n",
      "    \n",
      "    vocabulary = build_chi_vocabulary(corpus,labels,initial_vocabulary,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    \n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_raw_lemmas_chi_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized top n chi square unigrams vocabulary\n",
      "def build_lemmatized_chi_unigrams_vocabulary(corpus,labels,vocabulary_src):\n",
      "    from clef_globals import *\n",
      "    from clef_vocabulary_loader import load_vocabulary\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    \n",
      "    # load initial vocabulary\n",
      "    initial_vocabulary_tbl_name = 'clef_2010_{0}_lemmas_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    initial_vocabulary = load_vocabulary(initial_vocabulary_tbl_name)\n",
      "    \n",
      "    vocabulary = build_chi_vocabulary(corpus,labels,initial_vocabulary,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    \n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_lemmas_chi_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams stopwords vocabulary\n",
      "def build_lemmatized_unigrams_stopwords_vocabulary(corpus,labels,stop_words,vocabulary_src):\n",
      "    from clef_globals import *\n",
      "    from clef_vocabulary_loader import load_vocabulary\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    max_ngram_size = 1\n",
      "    # load initial vocabulary\n",
      "    initial_vocabulary_tbl_name = 'clef_2010_{0}_lemmas_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    initial_vocabulary = load_vocabulary(initial_vocabulary_tbl_name)\n",
      "    \n",
      "    vocabulary = build_chi_vocabulary(corpus,labels,initial_vocabulary,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    \n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_lemmas_chi_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized bigrams chi vocabulary\n",
      "def build_raw_lemmatized_chi_bigrams_vocabulary(corpus,labels,vocabulary_src):\n",
      "    from clef_globals import *\n",
      "    from clef_vocabulary_loader import load_vocabulary\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    # load initial vocabulary\n",
      "    initial_vocabulary_tbl_name = 'clef_2010_{0}_raw_lemmas_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    initial_vocabulary = load_vocabulary(initial_vocabulary_tbl_name)\n",
      "    \n",
      "    vocabulary = build_chi_vocabulary(corpus,labels,initial_vocabulary,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "\n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_raw_lemmas_chi_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams vocabulary\n",
      "def build_lemmatized_bigrams_vocabulary(corpus,labels,vocabulary_src):\n",
      "    from clef_globals import *\n",
      "    from clef_vocabulary_loader import load_vocabulary\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "        \n",
      "    # load initial vocabulary\n",
      "    initial_vocabulary_tbl_name = 'clef_2010_{0}_lemmas_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    initial_vocabulary = load_vocabulary(initial_vocabulary_tbl_name)\n",
      "    \n",
      "    vocabulary = build_chi_vocabulary(corpus,labels,initial_vocabulary,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "\n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_lemmas_chi_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams stopwords vocabulary\n",
      "def build_lemmatized_bigrams_stopwords_vocabulary(corpus,labels,stop_words,vocabulary_src):\n",
      "    from clef_globals import *\n",
      "    from clef_vocabulary_loader import load_vocabulary\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    max_ngram_size = 2\n",
      "    # load initial vocabulary\n",
      "    initial_vocabulary_tbl_name = 'clef_2010_{0}_lemmas_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    initial_vocabulary = load_vocabulary(initial_vocabulary_tbl_name)\n",
      "    \n",
      "    vocabulary = build_chi_vocabulary(corpus,labels,initial_vocabulary,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    \n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_lemmas_chi_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed unigrams vocabulary\n",
      "def build_stemmed_unigrams_vocabulary(corpus,labels,vocabulary_src):\n",
      "    from clef_globals import *\n",
      "    from clef_vocabulary_loader import load_vocabulary\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer()    \n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    # load initial vocabulary\n",
      "    initial_vocabulary_tbl_name = 'clef_2010_{0}_stems_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    initial_vocabulary = load_vocabulary(initial_vocabulary_tbl_name)\n",
      "    \n",
      "    vocabulary = build_chi_vocabulary(corpus,labels,initial_vocabulary,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    \n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_stems_chi_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed unigrams stopwords vocabulary\n",
      "def build_stemmed_unigrams_stopwords_vocabulary(corpus,labels,stop_words,vocabulary_src):\n",
      "    from clef_globals import *\n",
      "    from clef_vocabulary_loader import load_vocabulary\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer()    \n",
      "    max_ngram_size = 1\n",
      "    # load initial vocabulary\n",
      "    initial_vocabulary_tbl_name = 'clef_2010_{0}_stems_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    initial_vocabulary = load_vocabulary(initial_vocabulary_tbl_name)\n",
      "    \n",
      "    vocabulary = build_chi_vocabulary(corpus,labels,initial_vocabulary,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    \n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_stems_chi_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed bigrams vocabulary\n",
      "def build_stemmed_bigrams_vocabulary(corpus,labels,vocabulary_src):\n",
      "    from clef_globals import *\n",
      "    from clef_vocabulary_loader import load_vocabulary\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer() \n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2    \n",
      "    \n",
      "    # load initial vocabulary\n",
      "    initial_vocabulary_tbl_name = 'clef_2010_{0}_stems_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    initial_vocabulary = load_vocabulary(initial_vocabulary_tbl_name)\n",
      "    \n",
      "    vocabulary = build_chi_vocabulary(corpus,labels,initial_vocabulary,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    \n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_stems_chi_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed bigrams stopwords vocabulary\n",
      "def build_stemmed_bigrams_stopwords_vocabulary(corpus,labels,stop_words,vocabulary_src):\n",
      "    from clef_globals import *\n",
      "    from clef_vocabulary_loader import load_vocabulary\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer() \n",
      "    max_ngram_size = 2    \n",
      "\n",
      "    # load initial vocabulary\n",
      "    initial_vocabulary_tbl_name = 'clef_2010_{0}_stems_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    initial_vocabulary = load_vocabulary(initial_vocabulary_tbl_name)\n",
      "    \n",
      "    vocabulary = build_chi_vocabulary(corpus,labels,initial_vocabulary,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "\n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_stems_chi_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build(vocabulary_src):\n",
      "    from clef_corpus_loader import load_corpus_and_labels\n",
      "    from stopwords_loader import load_inquiry_stopwords\n",
      "    from sklearn.preprocessing import MultiLabelBinarizer\n",
      "    \n",
      "    # load clef patents with class lables from DB using only vocabulary_src as main field for vocabulary (e.g., abstract, description, claims...)\n",
      "    corpus_data = load_corpus_and_labels(vocabulary_src,'train')\n",
      "    corpus = corpus_data['corpus']\n",
      "    labels = corpus_data['labels']\n",
      "    labels_dic = corpus_data['labels_dic']\n",
      "    labels_arr = corpus_data['labels_arr']\n",
      "    print 'done loading {0} records and {1} labels.'.format(len(corpus),len(labels_dic))\n",
      "    \n",
      "    # binarize the labels\n",
      "    binarizer = MultiLabelBinarizer()\n",
      "    binarized_labels = binarizer.fit_transform(labels)\n",
      "    \n",
      "    # build vocabulary without stopwords removal\n",
      "    #build_lemmatized_chi_unigrams_vocabulary(corpus,binarized_labels.transpose(),vocabulary_src)\n",
      "    build_raw_lemmatized_chi_unigrams_vocabulary(corpus,binarized_labels.transpose(),vocabulary_src)\n",
      "    build_raw_lemmatized_chi_bigrams_vocabulary(corpus,binarized_labels.transpose(),vocabulary_src)\n",
      "    #build_lemmatized_bigrams_vocabulary(corpus,binarized_labels.transpose(),vocabulary_src)\n",
      "    #build_stemmed_unigrams_vocabulary(corpus,binarized_labels.transpose(),vocabulary_src)\n",
      "    #build_stemmed_bigrams_vocabulary(corpus,binarized_labels.transpose(),vocabulary_src)\n",
      "\n",
      "    # load inquiry stopwords list\n",
      "    #stop_words = load_inquiry_stopwords()\n",
      "\n",
      "    # build vocabulary with stopwords removal\n",
      "    #build_lemmatized_unigrams_stopwords_vocabulary(corpus,binarized_labels.transpose(),stop_words,vocabulary_src)\n",
      "    #build_lemmatized_bigrams_stopwords_vocabulary(corpus,binarized_labels.transpose(),stop_words,vocabulary_src)\n",
      "    #build_stemmed_unigrams_stopwords_vocabulary(corpus,binarized_labels.transpose(),stop_words,vocabulary_src)\n",
      "    #build_stemmed_bigrams_stopwords_vocabulary(corpus,binarized_labels.transpose(),stop_words,vocabulary_src)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    # build clef abstracts vocabulary\n",
      "    vocabulary_src = 'abstract'\n",
      "    build(vocabulary_src)\n",
      "\n",
      "    # build clef claims vocabulary\n",
      "    vocabulary_src = 'claims'\n",
      "    #build(vocabulary_src)\n",
      "\n",
      "    # build clef description vocabulary\n",
      "    vocabulary_src = 'description'\n",
      "    #build(vocabulary_src)\n",
      "\n",
      "    print 'done!'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}