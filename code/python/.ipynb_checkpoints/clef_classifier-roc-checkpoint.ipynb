{
 "metadata": {
  "name": "",
  "signature": "sha256:efc21037794d8d674ab5e1223f517c20d91a97de47d1469dbda466fe2740552a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "ROC for CLEF-IP2010 patents"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "ROC for clef-ip2010 patents"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size):\n",
      "    # tokenize text\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "    from sklearn.feature_extraction.text import TfidfTransformer\n",
      "    from clef_globals import *\n",
      "    \n",
      "    # generate corpus vectors\n",
      "    vectorizer = CountVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),vocabulary=vocabulary,stop_words={})\n",
      "    corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "    \n",
      "    # generate tfidf vectors\n",
      "    transformer = TfidfTransformer()\n",
      "    corpus_tfidf_vectors = transformer.fit_transform(corpus_vectors)\n",
      "\n",
      "    return corpus_tfidf_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# given classifier predictions probabilities, return predictions with top n probabilities > 0.5 for each instance or greatest one if all are <=0.5\n",
      "def get_max_n_pred(pred_proba, n_pred, threshold):\n",
      "    import heapq\n",
      "    import numpy\n",
      "    max_n_pred = numpy.ndarray(shape=pred_proba.shape)\n",
      "    for i in range(len(pred_proba)):\n",
      "        largest_n_proba = heapq.nlargest(n_pred,pred_proba[i])\n",
      "        max_n_pred[i] = numpy.array(((pred_proba[i]>threshold) & (pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1))\n",
      "        if max_n_pred[i].sum(axis=0)==0: # at least one label should be returned\n",
      "            max_n_pred[i] = numpy.array(((pred_proba[i]>=max(pred_proba[i])) & 1))\n",
      "    return max_n_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reference: http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification\n",
      "def classify(x_train,y_train,x_test,y_test,test_size,max_labels,threshold):\n",
      "    from sklearn.preprocessing import MultiLabelBinarizer\n",
      "    from sklearn.multiclass import OneVsRestClassifier\n",
      "    from sklearn.svm import SVC\n",
      "    from sklearn.svm import LinearSVC\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "    from sklearn.naive_bayes import MultinomialNB\n",
      "    from sklearn.cross_validation import train_test_split\n",
      "    from sklearn import metrics\n",
      "    import numpy\n",
      "        \n",
      "    # binarize the labels\n",
      "    mlb = MultiLabelBinarizer()\n",
      "    y_train_binarized = mlb.fit_transform(y_train)\n",
      "    \n",
      "    # train/test split\n",
      "    #corpus_tfidf_vectors, labels_binarized = shuffle(corpus_tfidf_vectors, labels_binarized)\n",
      "    #x_train, x_test, y_train, y_test = train_test_split(corpus_tfidf_vectors, labels_binarized, test_size=test_size, random_state=1)\n",
      "    \n",
      "    # classify\n",
      "    #cls = OneVsRestClassifier(LogisticRegression(class_weight='auto'))\n",
      "    #cls = OneVsRestClassifier(LogisticRegression())\n",
      "    #cls = OneVsRestClassifier(MultinomialNB(alpha=0.01))\n",
      "    #cls = OneVsRestClassifier(SVC(kernel='linear',probability=True,max_iter=1000))\n",
      "    cls = OneVsRestClassifier(LinearSVC())\n",
      "    cls.fit(x_train, y_train_binarized)\n",
      "    pred_proba = 1/(1+numpy.exp(-1*cls.decision_function(x_train)))\n",
      "    # evaluate\n",
      "    y_pred = mlb.inverse_transform(get_max_n_pred(pred_proba, max_labels,threshold))\n",
      "    result = 'threshold: {0}, precision: {1}, recall: {2}, f1: {3}'.format(threshold,metrics.precision_score(y_train, y_pred, average='micro'),metrics.recall_score(y_train, y_pred, average='micro'),metrics.f1_score(y_train, y_pred, average='micro'))    \n",
      "    print result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_unigrams(corpus_train_data,corpus_test_data,vocabulary_src,with_stopwords_removal,use_chi_features,use_raw_tokens):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    from clef_globals import *\n",
      "    from clef_vocabulary_loader import load_vocabulary\n",
      "    \n",
      "    max_ngram_size = 1\n",
      "    \n",
      "    if with_stopwords_removal==False:\n",
      "        stopwords_pattern = ''\n",
      "    else:\n",
      "        stopwords_pattern = '_stopwords'\n",
      "    if use_chi_features==False:\n",
      "        chi_features_pattern = ''\n",
      "    else:\n",
      "        chi_features_pattern = '_chi'\n",
      "    if use_raw_tokens==False:\n",
      "        raw_tokens_pattern = ''\n",
      "        tokenizer = LemmaTokenizer()\n",
      "    else:\n",
      "        raw_tokens_pattern = '_raw'\n",
      "        tokenizer = RawLemmaTokenizer()\n",
      "    \n",
      "    # load vocabulary\n",
      "    vocabulary_tbl_name = 'clef_2010_{0}{1}_lemmas{2}_unigrams{3}_df{4}_tf{5}'.format(vocabulary_src,raw_tokens_pattern,chi_features_pattern,stopwords_pattern,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_train_tfidf_vectors = vectorize_corpus(corpus_train_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    corpus_test_tfidf_vectors = vectorize_corpus(corpus_test_data['corpus'],tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    # classify & evaluate\n",
      "    for i in range(40,56):\n",
      "        classify(corpus_train_tfidf_vectors,corpus_train_data['labels'],\n",
      "                       corpus_test_tfidf_vectors,corpus_test_data['labels'],\n",
      "                       test_set_size,max_labels,\n",
      "                       i/100.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test(vocabulary_src):\n",
      "    from clef_corpus_loader import load_corpus_and_labels\n",
      "    from clef_corpus_loader import load_corpus_with_labels_mappings\n",
      "    \n",
      "    # load clef patents with class lables from DB using only vocabulary_src as main field for vocabulary (e.g., abstract, description, claims...)\n",
      "    corpus_train_data = load_corpus_and_labels(vocabulary_src,'train')\n",
      "    print 'done loading {0} train records and {1} labels.'.format(len(corpus_train_data['corpus']),len(corpus_train_data['labels_dic']))\n",
      "\n",
      "    corpus_test_data = load_corpus_with_labels_mappings(vocabulary_src,'test',corpus_train_data['labels_dic'])\n",
      "    print 'done loading {0} test records.'.format(len(corpus_test_data['corpus']))\n",
      "    \n",
      "    stopwords_removal = False\n",
      "    use_chi_features = False\n",
      "    use_raw_tokens = True\n",
      "    test_lemmatized_unigrams({'corpus':corpus_train_data['corpus'],'labels':corpus_train_data['labels']},\n",
      "                             {'corpus':corpus_test_data['corpus'],'labels':corpus_test_data['labels']},\n",
      "                             vocabulary_src,stopwords_removal,use_chi_features,use_raw_tokens)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test using abstracts vocabulary\n",
      "test('abstract')    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}