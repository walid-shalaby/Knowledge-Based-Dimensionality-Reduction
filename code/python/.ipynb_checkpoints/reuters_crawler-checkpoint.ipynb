{
 "metadata": {
  "name": "",
  "signature": "sha256:985f8ebf70cd80c83fa910be28788afcb046e398a07f5f595b53801c9bf4a3e4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Crawl and Save reuters 21578 data into sqlite DB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "\n",
      "from glob import glob\n",
      "import itertools\n",
      "import os.path\n",
      "import re\n",
      "import tarfile\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import rcParams\n",
      "\n",
      "from sklearn.externals import six\n",
      "from sklearn.externals.six.moves import html_parser\n",
      "from sklearn.externals.six.moves import urllib\n",
      "from sklearn.datasets import get_data_home\n",
      "\n",
      "def _not_in_sphinx():\n",
      "    # Hack to detect whether we are running by the sphinx builder\n",
      "    return '__file__' in globals()\n",
      "\n",
      "\n",
      "###############################################################################\n",
      "# Reuters Dataset related routines\n",
      "###############################################################################\n",
      "\n",
      "\n",
      "class ReutersParser(html_parser.HTMLParser):\n",
      "    \"\"\"Utility class to parse a SGML file and yield documents one at a time.\"\"\"\n",
      "\n",
      "    def __init__(self, encoding='latin-1'):\n",
      "        html_parser.HTMLParser.__init__(self)\n",
      "        self._reset()\n",
      "        self.encoding = encoding\n",
      "\n",
      "    def handle_starttag(self, tag, attrs):\n",
      "        method = 'start_' + tag\n",
      "        getattr(self, method, lambda x: None)(attrs)\n",
      "\n",
      "    def handle_endtag(self, tag):\n",
      "        method = 'end_' + tag\n",
      "        getattr(self, method, lambda: None)()\n",
      "\n",
      "    def _reset(self):\n",
      "        self.in_title = 0\n",
      "        self.in_body = 0\n",
      "        self.in_topics = 0\n",
      "        self.in_topic_d = 0\n",
      "        self.title = \"\"\n",
      "        self.body = \"\"\n",
      "        self.topics = []\n",
      "        self.topic_d = \"\"\n",
      "\n",
      "    def parse(self, fd):\n",
      "        self.docs = []\n",
      "        for chunk in fd:\n",
      "            self.feed(chunk.decode(self.encoding))\n",
      "            for doc in self.docs:\n",
      "                yield doc\n",
      "            self.docs = []\n",
      "        self.close()\n",
      "\n",
      "    def handle_data(self, data):\n",
      "        if self.in_body:\n",
      "            self.body += data\n",
      "        elif self.in_title:\n",
      "            self.title += data\n",
      "        elif self.in_topic_d:\n",
      "            self.topic_d += data\n",
      "\n",
      "    def start_reuters(self, attributes):\n",
      "        pass\n",
      "\n",
      "    def end_reuters(self):\n",
      "        self.body = re.sub(r'\\s+', r' ', self.body)\n",
      "        self.docs.append({'title': self.title,\n",
      "                          'body': self.body,\n",
      "                          'topics': self.topics})\n",
      "        self._reset()\n",
      "\n",
      "    def start_title(self, attributes):\n",
      "        self.in_title = 1\n",
      "\n",
      "    def end_title(self):\n",
      "        self.in_title = 0\n",
      "\n",
      "    def start_body(self, attributes):\n",
      "        self.in_body = 1\n",
      "\n",
      "    def end_body(self):\n",
      "        self.in_body = 0\n",
      "\n",
      "    def start_topics(self, attributes):\n",
      "        self.in_topics = 1\n",
      "\n",
      "    def end_topics(self):\n",
      "        self.in_topics = 0\n",
      "\n",
      "    def start_d(self, attributes):\n",
      "        self.in_topic_d = 1\n",
      "\n",
      "    def end_d(self):\n",
      "        self.in_topic_d = 0\n",
      "        self.topics.append(self.topic_d)\n",
      "        self.topic_d = \"\"\n",
      "\n",
      "\n",
      "def stream_reuters_documents(data_path=None):\n",
      "    \"\"\"Iterate over documents of the Reuters dataset.\n",
      "\n",
      "    The Reuters archive will automatically be downloaded and uncompressed if\n",
      "    the `data_path` directory does not exist.\n",
      "\n",
      "    Documents are represented as dictionaries with 'body' (str),\n",
      "    'title' (str), 'topics' (list(str)) keys.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
      "                    'reuters21578-mld/reuters21578.tar.gz')\n",
      "    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n",
      "\n",
      "    if data_path is None:\n",
      "        data_path = os.path.join(get_data_home(), \"reuters\")\n",
      "    if not os.path.exists(data_path):\n",
      "        \"\"\"Download the dataset.\"\"\"\n",
      "        print(\"downloading dataset (once and for all) into %s\" %\n",
      "              data_path)\n",
      "        os.mkdir(data_path)\n",
      "\n",
      "        def progress(blocknum, bs, size):\n",
      "            total_sz_mb = '%.2f MB' % (size / 1e6)\n",
      "            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n",
      "            if _not_in_sphinx():\n",
      "                print('\\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb),\n",
      "                      end='')\n",
      "\n",
      "        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n",
      "        urllib.request.urlretrieve(DOWNLOAD_URL, filename=archive_path,\n",
      "                                   reporthook=progress)\n",
      "        if _not_in_sphinx():\n",
      "            print('\\r', end='')\n",
      "        print(\"untarring Reuters dataset...\")\n",
      "        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n",
      "        print(\"done.\")\n",
      "\n",
      "    parser = ReutersParser()\n",
      "    for filename in glob(os.path.join(data_path, \"*.sgm\")):\n",
      "        for doc in parser.parse(open(filename, 'rb')):\n",
      "            yield doc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Iterator over parsed Reuters SGML files and save in DB.\n",
      "from reuters_globals import *\n",
      "import sqlite3 as sqlite\n",
      "data_stream = stream_reuters_documents('../../data/reuters21578/data')\n",
      "\n",
      "con = sqlite.connect(db_path)\n",
      "with con:\n",
      "    record_no = 0\n",
      "    topic_no = 0\n",
      "    topic_dic = dict()\n",
      "    for record in data_stream:\n",
      "        record_no = record_no + 1\n",
      "        con.execute(u'insert into reuters21578 values(\"{0}\",lower(\"{1}\"),lower(\"{2}\"))'.format(record_no,record['title'].replace('\"','\\''),record['body'].replace('\"','\\'')))\n",
      "        if len(record['topics'])==0:\n",
      "            topics = ''\n",
      "        else:\n",
      "            for i in range(len(record['topics'])):\n",
      "                if topic_dic.has_key(record['topics'][i])==False:\n",
      "                    topic_no = topic_no + 1\n",
      "                    con.execute('insert into reuters21578_topics values(\"{0}\",lower(\"{1}\"))'.format(topic_no,record['topics'][i]))\n",
      "                    topic_dic[record['topics'][i]] = topic_no\n",
      "                    topic_id = topic_no\n",
      "                else:\n",
      "                    topic_id = topic_dic[record['topics'][i]]\n",
      "                con.execute('insert into reuters21578_topics_join values(\"{0}\",\"{1}\")'.format(record_no,topic_id))\n",
      "print('done with ',record_no,'records')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done with  21578 records\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}