{
 "metadata": {
  "name": "",
  "signature": "sha256:0d3295f962873e82010cd389b80a97e590dd3e3bf70c7a60639515529d691af6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "REUTERS 21578 Vocabulary Builder"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Build stemmed and lemmatized vocabulary (unigrams + bigrams) from reuters 21578 corpus and store into DB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf):    \n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "    # tokenize text\n",
      "    vectorizer = CountVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),stop_words=stop_words)\n",
      "    corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "\n",
      "    # apply minimum term frequency threshold\n",
      "    term_freq = corpus_vectors.sum(axis=0) # sum on culomns to obtain term frequencies\n",
      "    terms_to_remove = []\n",
      "    for k,v in vectorizer.vocabulary_.iteritems():\n",
      "        if(term_freq[0,vectorizer.vocabulary_[k]]<min_tf):\n",
      "            terms_to_remove.append(k)\n",
      "\n",
      "    print 'removing ({0}) terms under tf threshold'.format(len(terms_to_remove))\n",
      "    for k in terms_to_remove:\n",
      "        del vectorizer.vocabulary_[k]\n",
      "\n",
      "    return vectorizer.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_vocabulary(vocabulary,tbl_name):\n",
      "    # save vocabulary in DB for future use\n",
      "    import sqlite3 as sqlitedb\n",
      "    from reuters_globals import *\n",
      "\n",
      "    l = []\n",
      "    l.extend([i] for i in vocabulary)\n",
      "    con = sqlitedb.connect(db_path)\n",
      "    with con:\n",
      "        con.execute('drop table if exists {0}'.format(tbl_name))\n",
      "        con.execute('create table {0}(term text)'.format(tbl_name))\n",
      "        con.executemany('insert into {0}(term) values(?)'.format(tbl_name),l)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build raw unigrams vocabulary\n",
      "def build_raw_unigrams_vocabulary(corpus,target_topic,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,1,1)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_{1}_raw_unigrams'.format(target_topic,vocabulary_src)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build unigrams vocabulary\n",
      "def build_unigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_unigrams_vocabulary(corpus,target_topic,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_{1}_raw_lemmas_unigrams_df{2}_tf{3}'.format(target_topic,vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized test unigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_test_unigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_lemmas_test_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed test unigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_test_unigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_stems_test_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized all unigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_all_unigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_lemmas_all_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed all unigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_all_unigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_stems_all_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_lemmatized_unigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_lemmas_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build unigrams stopwords vocabulary\n",
      "def build_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build bigrams stopwords vocabulary\n",
      "def build_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build raw unigrams stopwords vocabulary\n",
      "def build_raw_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,1,1)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_unigrams_stopwords'.format(vocabulary_src)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build raw bigrams stopwords vocabulary\n",
      "def build_raw_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,1,1)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_bigrams_stopwords'.format(vocabulary_src)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams stopwords vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_lemmas_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams stopwords vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_lemmatized_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_lemmas_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build raw bigrams vocabulary\n",
      "def build_raw_bigrams_vocabulary(corpus,target_topic,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,1,1)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_{1}_raw_bigrams'.format(target_topic,vocabulary_src)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized bigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_bigrams_vocabulary(corpus,target_topic,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_{1}_raw_lemmas_bigrams_df{2}_tf{3}'.format(target_topic,vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized all bigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_all_bigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_lemmas_all_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed all bigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_all_bigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_stems_all_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized test bigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_test_bigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_lemmas_test_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed test bigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_test_bigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_stems_test_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build bigrams vocabulary\n",
      "def build_bigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    \n",
      "    tokenizer = None\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized bigrams vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_lemmatized_bigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_lemmas_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized bigrams stopwords vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_lemmatized_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from lemmatizing_tokenizer import RawLemmaTokenizer\n",
      "    \n",
      "    tokenizer = RawLemmaTokenizer()\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_lemmas_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized bigrams stopwords vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_lemmatized_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_lemmas_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed unigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_unigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer()    \n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_stems_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed unigrams vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_stemmed_unigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer()    \n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_stems_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed unigrams stopwords vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer()    \n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_stems_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed unigrams stopwords vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_stemmed_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer()    \n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_stems_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed bigrams vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_bigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer() \n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2    \n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_stems_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed bigrams vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_stemmed_bigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer() \n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2    \n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_stems_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed bigrams stopwords vocabulary\n",
      "# uses alphanumeric tokenizer\n",
      "def build_raw_stemmed_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from stemming_tokenizer import RawStemmingTokenizer\n",
      "    \n",
      "    tokenizer = RawStemmingTokenizer() \n",
      "    max_ngram_size = 2    \n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_raw_stems_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed bigrams stopwords vocabulary\n",
      "# uses alphabetic tokenizer\n",
      "def build_stemmed_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from reuters_globals import *\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer() \n",
      "    max_ngram_size = 2    \n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'reuters_2010_{0}_stems_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build(target_topic,vocabulary_src):\n",
      "    from reuters_corpus_loader import load_corpus\n",
      "    from stopwords_loader import load_inquiry_stopwords\n",
      "    \n",
      "    # load reuters docs from DB using only vocabulary_src as main field for vocabulary (e.g., abstract, description, claims...)\n",
      "    corpus_train = load_corpus(target_topic,vocabulary_src,'train')\n",
      "    corpus = corpus_train['corpus']\n",
      "    \n",
      "    # build vocabulary without stopwords removal\n",
      "    build_raw_unigrams_vocabulary(corpus,target_topic,vocabulary_src)\n",
      "    #build_unigrams_vocabulary(corpus,vocabulary_src)\n",
      "    #build_lemmatized_unigrams_vocabulary(corpus,vocabulary_src)\n",
      "    build_raw_lemmatized_unigrams_vocabulary(corpus,target_topic,vocabulary_src)    \n",
      "    #build_lemmatized_bigrams_vocabulary(corpus,vocabulary_src)\n",
      "    build_raw_lemmatized_bigrams_vocabulary(corpus,target_topic,vocabulary_src)\n",
      "    build_raw_bigrams_vocabulary(corpus,target_topic,vocabulary_src)\n",
      "    #build_bigrams_vocabulary(corpus,vocabulary_src)\n",
      "    #build_stemmed_unigrams_vocabulary(corpus,vocabulary_src)\n",
      "    #build_raw_stemmed_unigrams_vocabulary(corpus,vocabulary_src)\n",
      "    #build_stemmed_bigrams_vocabulary(corpus,vocabulary_src)\n",
      "    #build_raw_stemmed_bigrams_vocabulary(corpus,vocabulary_src)\n",
      "\n",
      "    # load inquiry stopwords list\n",
      "    #stop_words = load_inquiry_stopwords()\n",
      "\n",
      "    # build vocabulary with stopwords removal\n",
      "    #build_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    #build_raw_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    #build_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    #build_raw_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    #build_lemmatized_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    #build_raw_lemmatized_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    #build_lemmatized_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    #build_raw_lemmatized_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    #build_stemmed_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    #build_raw_stemmed_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    #build_stemmed_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    #build_raw_stemmed_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "        \n",
      "    #corpus_test = load_corpus(vocabulary_src,'test')\n",
      "    #build_raw_lemmatized_test_unigrams_vocabulary(corpus_test['corpus'],vocabulary_src)    \n",
      "    #build_raw_stemmed_test_unigrams_vocabulary(corpus_test['corpus'],vocabulary_src)    \n",
      "    #build_raw_lemmatized_test_bigrams_vocabulary(corpus_test['corpus'],vocabulary_src)            \n",
      "    #build_raw_stemmed_test_bigrams_vocabulary(corpus_test['corpus'],vocabulary_src)            \n",
      "    \n",
      "    #corpus = load_corpus(vocabulary_src,'both')\n",
      "    #build_raw_lemmatized_all_unigrams_vocabulary(corpus['corpus'],vocabulary_src)    \n",
      "    #build_raw_stemmed_all_unigrams_vocabulary(corpus['corpus'],vocabulary_src)    \n",
      "    #build_raw_lemmatized_all_bigrams_vocabulary(corpus['corpus'],vocabulary_src)\n",
      "    #build_raw_stemmed_all_bigrams_vocabulary(corpus['corpus'],vocabulary_src)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build reuters docs vocabulary\n",
      "vocabulary_src = 'all'\n",
      "target_topic = 'earn'\n",
      "build(target_topic,vocabulary_src)\n",
      "\n",
      "# build reuters titles vocabulary\n",
      "vocabulary_src = 'title'\n",
      "#build(target_topic,vocabulary_src)\n",
      "\n",
      "# build reuters body vocabulary\n",
      "vocabulary_src = 'body'\n",
      "#build(target_topic,vocabulary_src)\n",
      "\n",
      "print 'done!'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}