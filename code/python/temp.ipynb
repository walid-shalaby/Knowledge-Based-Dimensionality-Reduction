{
 "metadata": {
  "name": "",
  "signature": "sha256:326503c6120f0d40c759b58e6616759563125b7e4ed5d1a5b3b513895959ad48"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare stemmer\n",
      "from nltk import word_tokenize          \n",
      "from nltk.stem import WordNetLemmatizer \n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import PorterStemmer\n",
      "class StermmingTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stemmer = PorterStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.stemmer.stem(tokens.lower()) for tokens in word_tokenize(doc)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize text\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "vectorizer = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',tokenizer=StermmingTokenizer(),ngram_range=(1,1),stop_words={},min_df=1)#(min_df=0.000001,max_df=0.95)\n",
      "corpus = ['hi cairo here is cairo','here usa s 5.here in usa','hi in']\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "\n",
      "print corpus_vectors.shape\n",
      "print vectorizer.vocabulary_\n",
      "print vectorizer.stop_words_\n",
      "print corpus_vectors.todense()\n",
      "corpus_vectors = corpus_vectors.tolil()\n",
      "term_freq = corpus_vectors.sum(axis=0)\n",
      "#print corpus_vectors.sum(axis=1)\n",
      "keys_to_remove = []\n",
      "for k,v in vectorizer.vocabulary_.iteritems():\n",
      "    print k,v,term_freq[0,vectorizer.vocabulary_[k]]\n",
      "    if(term_freq[0,vectorizer.vocabulary_[k]]<0):\n",
      "        corpus_vectors[:,v] = 0\n",
      "        keys_to_remove.append(k)\n",
      "  \n",
      "print corpus_vectors.todense()\n",
      "print keys_to_remove\n",
      "for k in keys_to_remove:\n",
      "    del vectorizer.vocabulary_[k]\n",
      "    \n",
      "print vectorizer.vocabulary_\n",
      "#print sum(corpus_vectors.getcol(vectorizer.vocabulary_[i])).todense()[0,0]>0\n",
      "#print corpus_vectors[:,vectorizer.vocabulary_['cairo']]\n",
      "#print corpus_vectors\n",
      "#print corpus_vectors.shape\n",
      "\n",
      "#binarizer = Binarizer()\n",
      "#corpus_binary_vectors = binarizer.transform(corpus_vectors)\n",
      "#print corpus_binary_vectors\n",
      "#doc_freq = corpus_binary_vectors.sum(axis=0)\n",
      "#keys_to_remove = []\n",
      "#for k,v in vectorizer.vocabulary_.iteritems():\n",
      "#    print k,v,doc_freq[0,vectorizer.vocabulary_[k]]\n",
      "#    if(doc_freq[0,vectorizer.vocabulary_[k]]<0):\n",
      "#        corpus_vectors[:,v] = 0\n",
      "#        keys_to_remove.append(k)\n",
      "  \n",
      "corpus_vectors = corpus_vectors.tocsr()\n",
      "#print corpus_vectors.todense()\n",
      "#print keys_to_remove\n",
      "#for k in keys_to_remove:\n",
      "#    del vectorizer.vocabulary_[k]\n",
      "    \n",
      "print vectorizer.vocabulary_\n",
      "print len(vectorizer.vocabulary_)\n",
      "# tokenize text\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "transformer = TfidfTransformer()\n",
      "titles_tfidf_vectors = transformer.fit_transform(corpus_vectors)\n",
      "\n",
      "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
      "#vectorizer = TfidfVectorizer(lowercase=False)#(min_df=0.000001,max_df=0.95)\n",
      "#corpus = ['hi this is cairo','hi here in usa']\n",
      "#titles_tfidf_vectors = vectorizer.fit_transform(corpus_vectors)\n",
      "\n",
      "#print vectorizer\n",
      "print titles_tfidf_vectors.shape\n",
      "print titles_tfidf_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(3, 8)\n",
        "{u'usa': 7, u'5.here': 0, u'cairo': 1, u'is': 5, u'here': 2, u's': 6, u'hi': 3, u'in': 4}\n",
        "set([])\n",
        "[[0 2 1 1 0 1 0 0]\n",
        " [1 0 1 0 1 0 1 2]\n",
        " [0 0 0 1 1 0 0 0]]\n",
        "usa 7 2\n",
        "5.here 0 1\n",
        "cairo 1 2\n",
        "is 5 1\n",
        "here 2 2\n",
        "s 6 1\n",
        "hi 3 2\n",
        "in 4 2\n",
        "[[0 2 1 1 0 1 0 0]\n",
        " [1 0 1 0 1 0 1 2]\n",
        " [0 0 0 1 1 0 0 0]]\n",
        "[]\n",
        "{u'usa': 7, u'5.here': 0, u'cairo': 1, u'is': 5, u'here': 2, u's': 6, u'hi': 3, u'in': 4}\n",
        "{u'usa': 7, u'5.here': 0, u'cairo': 1, u'is': 5, u'here': 2, u's': 6, u'hi': 3, u'in': 4}\n",
        "8\n",
        "(3, 8)\n",
        "  (0, 5)\t0.403016210804\n",
        "  (0, 3)\t0.306504216242\n",
        "  (0, 2)\t0.306504216242\n",
        "  (0, 1)\t0.806032421607\n",
        "  (1, 7)\t0.747602230278\n",
        "  (1, 6)\t0.373801115139\n",
        "  (1, 4)\t0.284285382956\n",
        "  (1, 2)\t0.284285382956\n",
        "  (1, 0)\t0.373801115139\n",
        "  (2, 4)\t0.707106781187\n",
        "  (2, 3)\t0.707106781187\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print pred_labels\n",
      "#print probs\n",
      "#metrics.f1_score(y_test, pred)\n",
      "#correct = 0.0\n",
      "#incorrect = 0.0\n",
      "#for i in range(len(y_test)):\n",
      "#    is_correct = False\n",
      "#    for j in range(len(y_test[i])):\n",
      "#        if pred[i]==y_test[i][j]:\n",
      "#            correct = correct + 1\n",
      "#            is_correct = True\n",
      "#            break        \n",
      "#    if is_correct==False:\n",
      "#        incorrect = incorrect + 1\n",
      "        \n",
      "#print 'total=', len(labels), 'test=', len(y_test), 'correct=', correct, 'incorrect=', incorrect, 'accuracy=', correct/len(y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import PorterStemmer\n",
      "from nltk import RegexpTokenizer\n",
      "\n",
      "class StermmingTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stemmer = PorterStemmer()\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b[a-z\\\\-]+\\\\b')\n",
      "    def __call__(self, doc):\n",
      "        return [self.stemmer.stem(tokens.lower()) for tokens in self.tokenizer.tokenize(doc)]\n",
      "    \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "vectorizer = CountVectorizer(tokenizer=StermmingTokenizer(),ngram_range=(1,1),stop_words={},min_df=2)#(min_df=0.000001,max_df=0.95)\n",
      "corpus = ['hi cairo here is CAIRON','here USA s 09.here in u-sa','hi in']\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "\n",
      "# apply minimum term frequency threshold\n",
      "corpus_vectors = corpus_vectors.tolil()\n",
      "term_freq = corpus_vectors.sum(axis=0) # sum on culomns to obtain term frequencies\n",
      "terms_to_remove = []\n",
      "for k,v in vectorizer.vocabulary_.iteritems():\n",
      "    if(term_freq[0,vectorizer.vocabulary_[k]]<3):\n",
      "        corpus_vectors[:,v] = 0\n",
      "        terms_to_remove.append(k)\n",
      "\n",
      "for k in terms_to_remove:\n",
      "    del vectorizer.vocabulary_[k]\n",
      "\n",
      "print corpus_vectors.shape\n",
      "print vectorizer.vocabulary_\n",
      "print vectorizer.stop_words_\n",
      "#tokenizer=StermmingTokenizer(),\n",
      "#token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(3, 3)\n",
        "{u'here': 0}\n",
        "set([u'usa', u'cairo', u'is', u'cairon', u's', u'u-sa'])\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare stemmer\n",
      "from nltk import RegexpTokenizer\n",
      "from nltk.stem import WordNetLemmatizer \n",
      "from sklearn.feature_selection import chi2\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b[a-z]+\\\\-*[a-z]+|\\\\b(?u)\\\\b[a-z]\\\\b')\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc)]\n",
      "    \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "import heapq\n",
      "import numpy\n",
      "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),ngram_range=(1,2),stop_words={'k','x'},min_df=3,vocabulary={u'cairo', u'in', u'here', u'here in', u'class', u'cairo class','hereis','cx'})#(min_df=0.000001,max_df=0.95)\n",
      "#corpus = ['hi k cairo class classifiable classified doing here is CAIRON','here USA s 09-hers in -x u-sa','hi in']\n",
      "corpus = ['hi cairo class here is CAIRON. animals farms','here USA here in usa','here in cairo classes']\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "print len(vectorizer.vocabulary_)\n",
      "print len(vectorizer.stop_words)\n",
      "print vectorizer.get_feature_names()\n",
      "print vectorizer.get_stop_words()\n",
      "print corpus_vectors.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8\n",
        "2\n",
        "[u'here in', u'cairo class', u'cairo', u'here', 'hereis', 'cx', u'in', u'class']\n",
        "set(['x', 'k'])\n",
        "(3, 8)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare stemmer\n",
      "from nltk import RegexpTokenizer\n",
      "from nltk.stem import WordNetLemmatizer \n",
      "from sklearn.feature_selection import chi2\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b[a-z]+\\\\-*[a-z]+|\\\\b(?u)\\\\b[a-z]\\\\b')\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc)]\n",
      "    \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "import heapq\n",
      "import numpy\n",
      "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),ngram_range=(1,2),stop_words={'k','x'},min_df=3,vocabulary={u'hero', u'cairo', u'in', u'here', u'here in', u'class', u'cairo class','hereis','cx'})#(min_df=0.000001,max_df=0.95)\n",
      "#corpus = ['hi k cairo class classifiable classified doing here is CAIRON','here USA s 09-hers in -x u-sa','hi in']\n",
      "corpus = ['hi cairo class here is CAIRON. animals farms','here USA here in usa','here in cairo classes']\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "print len(vectorizer.vocabulary_)\n",
      "print len(vectorizer.stop_words)\n",
      "print vectorizer.get_feature_names()\n",
      "print vectorizer.get_stop_words()\n",
      "print corpus_vectors.shape\n",
      "t = TfidfTransformer()\n",
      "corpus_vectors = t.fit_transform(corpus_vectors)\n",
      "print corpus_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9\n",
        "2\n",
        "[u'here in', u'hero', u'cairo class', u'cairo', u'here', 'hereis', 'cx', u'in', u'class']\n",
        "set(['x', 'k'])\n",
        "(3, 9)\n",
        "  (0, 8)\t0.52682017324\n",
        "  (0, 4)\t0.409122860767\n",
        "  (0, 3)\t0.52682017324\n",
        "  (0, 2)\t0.52682017324\n",
        "  (1, 7)\t0.476062939277\n",
        "  (1, 4)\t0.73941068135\n",
        "  (1, 0)\t0.476062939277\n",
        "  (2, 8)\t0.422460559532\n",
        "  (2, 7)\t0.422460559532\n",
        "  (2, 4)\t0.328078311076\n",
        "  (2, 3)\t0.422460559532\n",
        "  (2, 2)\t0.422460559532\n",
        "  (2, 0)\t0.422460559532\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare stemmer\n",
      "from nltk import RegexpTokenizer\n",
      "from nltk.stem import WordNetLemmatizer \n",
      "from sklearn.feature_selection import chi2\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b[a-z]+\\\\-*[a-z]+|\\\\b(?u)\\\\b[a-z]\\\\b')\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc)]\n",
      "    \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "import heapq\n",
      "import numpy\n",
      "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),ngram_range=(1,2),stop_words={'k','x'},min_df=2,vocabulary={u'cairo', u'in', u'here', u'here in', u'class', u'cairo class','hereis','cx'})#(min_df=0.000001,max_df=0.95)\n",
      "#corpus = ['hi k cairo class classifiable classified doing here is CAIRON','here USA s 09-hers in -x u-sa','hi in']\n",
      "corpus = ['hi cairo class here is CAIRON. animals farms','here USA here in usa','here in cairo classes']\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "y = numpy.array([[0,1,0],[1,1,0],[0,1,1]])\n",
      "y = [0,1,0]\n",
      "chi,pvals = chi2(corpus_vectors,y)\n",
      "print chi\n",
      "print pvals\n",
      "print 'nans',sum(numpy.isnan(chi))\n",
      "largest = heapq.nlargest(3,chi)\n",
      "print largest\n",
      "print min(largest)\n",
      "print chi>=min(largest)\n",
      "chi_ = numpy.array((chi>=min(largest)) & 1)\n",
      "print chi_,sum(chi_)\n",
      "print corpus_vectors.shape\n",
      "print vectorizer.vocabulary_\n",
      "print corpus_vectors\n",
      "ch2 = SelectKBest(chi2, k=3)\n",
      "ch2.fit(corpus_vectors, y)\n",
      "#print 'hoy',ch2.inverse_transform(chi22)\n",
      "print 'he',ch2.get_support() & 1\n",
      "#print 'hi',chi22\n",
      "#print vectorizer.stop_words_\n",
      "#tokenizer=StermmingTokenizer(),\n",
      "#token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.25  1.    1.    0.5    nan   nan  0.25  1.  ]\n",
        "[ 0.61707508  0.31731051  0.31731051  0.47950012         nan         nan\n",
        "  0.61707508  0.31731051]\n",
        "nans 2\n",
        "[1.0, 1.0, 1.0]\n",
        "1.0\n",
        "[False  True  True False False False False  True]\n",
        "[0 1 1 0 0 0 0 1] 3\n",
        "(3, 8)\n",
        "{u'here in': 0, u'cairo class': 1, u'cairo': 2, u'here': 3, 'hereis': 4, 'cx': 5, u'in': 6, u'class': 7}\n",
        "  (0, 1)\t1\n",
        "  (0, 2)\t1\n",
        "  (0, 3)\t1\n",
        "  (0, 7)\t1\n",
        "  (1, 0)\t1\n",
        "  (1, 3)\t2\n",
        "  (1, 6)\t1\n",
        "  (2, 0)\t1\n",
        "  (2, 1)\t1\n",
        "  (2, 2)\t1\n",
        "  (2, 3)\t1\n",
        "  (2, 6)\t1\n",
        "  (2, 7)\t1\n",
        "he [0 1 1 0 0 0 0 1]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/univariate_selection.py:150: RuntimeWarning: invalid value encountered in divide\n",
        "  chisq /= f_exp\n",
        "-c:30: RuntimeWarning: invalid value encountered in greater_equal\n",
        "-c:31: RuntimeWarning: invalid value encountered in greater_equal\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# alphanumeric tokenizer\n",
      "from nltk import RegexpTokenizer\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "class RawLemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc)]\n",
      "    \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "import heapq\n",
      "import numpy\n",
      "vectorizer = CountVectorizer(tokenizer=RawLemmaTokenizer(),ngram_range=(1,2),stop_words={'k','x'},min_df=1)#(min_df=0.000001,max_df=0.95)\n",
      "#corpus = ['hi k cairo class classifiable classified doing here is CAIRON','here USA s 09-hers in -x u-sa','hi in']\n",
      "corpus = ['hi cairo class here is CAIRON. animals farms','here USA here in usa','here in cairo classes']\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "print corpus_vectors.shape\n",
      "print vectorizer.vocabulary_\n",
      "print corpus_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(3, 22)\n",
        "{u'in usa': 17, u'class here': 7, u'cairo': 2, u'is': 18, u'in': 15, u'cairon': 4, u'here is': 11, u'here': 9, u'here in': 10, u'hi': 13, u'animal farm': 1, u'class': 6, u'usa': 20, u'cairo class': 3, u'in cairo': 16, u'cairon animal': 5, u'farm': 8, u'here usa': 12, u'animal': 0, u'usa here': 21, u'is cairon': 19, u'hi cairo': 14}\n",
        "  (0, 13)\t1\n",
        "  (0, 2)\t1\n",
        "  (0, 6)\t1\n",
        "  (0, 9)\t1\n",
        "  (0, 18)\t1\n",
        "  (0, 4)\t1\n",
        "  (0, 0)\t1\n",
        "  (0, 8)\t1\n",
        "  (0, 14)\t1\n",
        "  (0, 3)\t1\n",
        "  (0, 7)\t1\n",
        "  (0, 11)\t1\n",
        "  (0, 19)\t1\n",
        "  (0, 5)\t1\n",
        "  (0, 1)\t1\n",
        "  (1, 9)\t2\n",
        "  (1, 20)\t2\n",
        "  (1, 15)\t1\n",
        "  (1, 12)\t1\n",
        "  (1, 21)\t1\n",
        "  (1, 10)\t1\n",
        "  (1, 17)\t1\n",
        "  (2, 2)\t1\n",
        "  (2, 6)\t1\n",
        "  (2, 9)\t1\n",
        "  (2, 3)\t1\n",
        "  (2, 15)\t1\n",
        "  (2, 10)\t1\n",
        "  (2, 16)\t1\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = ['elephant','lion','cat','dog','whale','ant','bee','peagon','eagle','parrot','human']\n",
      "labels = [['mammals','animals'],['mammals','animals'],['mammals','animals'],['mammals','animals'],['mammals','animals'],['nonmammals','animals'],['nonmammals','animals'],['nonmammals','animals'],['nonmammals','animals'],['nonmammals','animals'],['mammals']]\n",
      "labels_dic = {}\n",
      "labels_arr = []\n",
      "for i in range(len(labels)):\n",
      "    for j in range(len(labels[i])):\n",
      "        #x = labels[i][j]\n",
      "        if labels[i][j] not in labels_dic:\n",
      "            labels_dic[labels[i][j]] = len(labels_arr)\n",
      "            labels_arr.append(labels[i][j])\n",
      "            labels[i][j] = len(labels_arr)-1\n",
      "        else:\n",
      "            labels[i][j] = labels_dic[labels[i][j]]\n",
      "        #print x,labels[i][j]\n",
      "\n",
      "print len(labels_arr)\n",
      "print len(corpus)\n",
      "#print labels\n",
      "print labels_arr\n",
      "print labels_dic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3\n",
        "11\n",
        "['mammals', 'animals', 'nonmammals']\n",
        "{'mammals': 0, 'animals': 1, 'nonmammals': 2}\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare lemmatizer\n",
      "from nltk import RegexpTokenizer\n",
      "from nltk.stem import WordNetLemmatizer \n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b[a-z]+\\\\-*[a-z]+|\\\\b(?u)\\\\b[a-z]\\\\b')\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize text\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "\n",
      "min_term_freq = 1\n",
      "                                           \n",
      "vectorizer = CountVectorizer(min_df=1,tokenizer=LemmaTokenizer(),ngram_range=(1,1),stop_words={})\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "print len(vectorizer.vocabulary_)\n",
      "#print vectorizer.vocabulary_\n",
      "#print vectorizer.stop_words_\n",
      "#print corpus_vectors.todense()\n",
      "corpus_vectors = corpus_vectors.tolil()\n",
      "term_freq = corpus_vectors.sum(axis=0) # sum on culomns to obtain term frequencies\n",
      "terms_to_remove = []\n",
      "for k,v in vectorizer.vocabulary_.iteritems():\n",
      "    #print k,v,term_freq[0,vectorizer.vocabulary_[k]]\n",
      "    if(term_freq[0,vectorizer.vocabulary_[k]]<min_term_freq):\n",
      "        corpus_vectors[:,v] = 0\n",
      "        terms_to_remove.append(k)\n",
      "  \n",
      "#print corpus_vectors.todense()\n",
      "print len(terms_to_remove)\n",
      "for k in terms_to_remove:\n",
      "    del vectorizer.vocabulary_[k]\n",
      "    \n",
      "print vectorizer.vocabulary_\n",
      "#print sum(corpus_vectors.getcol(vectorizer.vocabulary_[i])).todense()[0,0]>0\n",
      "#print corpus_vectors[:,vectorizer.vocabulary_['cairo']]\n",
      "#print corpus_vectors\n",
      "print len(vectorizer.vocabulary_)\n",
      "\n",
      "corpus_vectors = corpus_vectors.tocsr()\n",
      "#print corpus_vectors.todense()\n",
      "#corpus_vectors?\n",
      "    \n",
      "print len(vectorizer.vocabulary_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11\n",
        "0\n",
        "{u'peagon': 9, u'eagle': 4, u'parrot': 8, u'dog': 3, u'cat': 2, u'ant': 0, u'lion': 7, u'human': 6, u'elephant': 5, u'whale': 10, u'bee': 1}\n",
        "11\n",
        "11\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize text\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "\n",
      "transformer = TfidfTransformer()#(min_df=0.000001,max_df=0.95)\n",
      "#corpus = ['hi this is cairo','hi here in usa']\n",
      "corpus_tfidf_vectors = transformer.fit_transform(corpus_vectors)\n",
      "\n",
      "#print vectorizer\n",
      "print corpus_tfidf_vectors.shape\n",
      "print corpus_tfidf_vectors\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(11, 11)\n",
        "  (0, 5)\t1.0\n",
        "  (1, 7)\t1.0\n",
        "  (2, 2)\t1.0\n",
        "  (3, 3)\t1.0\n",
        "  (4, 10)\t1.0\n",
        "  (5, 0)\t1.0\n",
        "  (6, 1)\t1.0\n",
        "  (7, 9)\t1.0\n",
        "  (8, 4)\t1.0\n",
        "  (9, 8)\t1.0\n",
        "  (10, 6)\t1.0\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# binarize the labels\n",
      "from sklearn.preprocessing import MultiLabelBinarizer\n",
      "\n",
      "\n",
      "mlb = MultiLabelBinarizer()\n",
      "labels_binarized = mlb.fit_transform(labels)\n",
      "labels_binarized.shape\n",
      "\n",
      "print labels_binarized\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 1 0]\n",
        " [1 1 0]\n",
        " [1 1 0]\n",
        " [1 1 0]\n",
        " [1 1 0]\n",
        " [0 1 1]\n",
        " [0 1 1]\n",
        " [0 1 1]\n",
        " [0 1 1]\n",
        " [0 1 1]\n",
        " [1 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# given classifier predictions probabilities, return predictions with top n probabilities for each instance\n",
      "import heapq\n",
      "import numpy\n",
      "\n",
      "def get_max_n_pred(pred_proba, n_pred):\n",
      "    max_n_pred = numpy.ndarray(shape=pred_proba.shape)\n",
      "    for i in range(len(pred_proba)):\n",
      "        largest_n_proba = heapq.nlargest(n_pred,pred_proba[i])\n",
      "        print \"1\",(pred_proba[i]>0.5)\n",
      "        print \"2\",(pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1])\n",
      "        print \"3\",numpy.array(((pred_proba[i]>0.5) & (pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1))\n",
      "        max_n_pred[i] = numpy.array(((pred_proba[i]>0.5) & (pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1))\n",
      "        if max_n_pred[i].sum(axis=1)==0:\n",
      "            print \"zeros\"\n",
      "            max_n_pred[i] = numpy.array(((pred_proba[i]>=max(pred_proba[i])) & 1))\n",
      "    \n",
      "    return max_n_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# classify\n",
      "from sklearn.multiclass import OneVsRestClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import metrics\n",
      "x_train, x_test, y_train, y_test = train_test_split(corpus_tfidf_vectors, labels_binarized, test_size=0.1, random_state=87)\n",
      "cls = OneVsRestClassifier(LogisticRegression())\n",
      "#cls = OneVsRestClassifier(MultinomialNB(alpha=0.01))\n",
      "#cls = OneVsRestClassifier(SVC(kernel='linear',probability=True))\n",
      "cls.fit(x_train, y_train)\n",
      "\n",
      "# evaluate\n",
      "pred = cls.predict(x_test)\n",
      "pred_proba = cls.predict_proba(x_test)\n",
      "print x_test\n",
      "print \"pred\", pred\n",
      "print \"pred_proba\", pred_proba\n",
      "#for i in range(len(pred_proba)):\n",
      "#    largest_n_proba = heapq.nlargest(4,pred_proba[i])\n",
      "#    pred_proba[i] = numpy.array((pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1)\n",
      "    \n",
      "pred_labels = mlb.inverse_transform(get_max_n_pred(pred_proba,1))\n",
      "print \"pred_labels\",pred_labels\n",
      "pred_labels = mlb.inverse_transform(pred)\n",
      "print \"pred_labels again\", pred_labels\n",
      "actual_labels = mlb.inverse_transform(y_test)\n",
      "print \"actual_labels\", actual_labels\n",
      "# http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification\n",
      "print metrics.precision_score(actual_labels, pred_labels, average='weighted')\n",
      "print metrics.recall_score(actual_labels, pred_labels, average='weighted')\n",
      "print metrics.f1_score(actual_labels, pred_labels, average='weighted')\n",
      "#pred_probs = cls.predict_proba(x_test)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 2)\t1.0\n",
        "  (1, 9)\t1.0\n",
        "pred [[1 1 0]\n",
        " [1 1 0]]\n",
        "pred_proba [[ 0.53652967  0.74856294  0.46347033]\n",
        " [ 0.53652967  0.74856294  0.46347033]]\n",
        "1 [ True  True False]\n",
        "2 [False  True False]\n",
        "3 [0 1 0]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/__init__.py:89: DeprecationWarning: Function multilabel_ is deprecated; Attribute multilabel_ is deprecated and will be removed in 0.17. Use 'y_type_.startswith('multilabel')' instead\n",
        "  warnings.warn(msg, category=DeprecationWarning)\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "'axis' entry is out of bounds",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-27-b9f60cfd090c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#    pred_proba[i] = numpy.array((pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mpred_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_max_n_pred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_proba\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"pred_labels\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mpred_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-26-a1ee736aba40>\u001b[0m in \u001b[0;36mget_max_n_pred\u001b[1;34m(pred_proba, n_pred)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"3\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpred_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mlargest_n_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlargest_n_proba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mmax_n_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpred_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mlargest_n_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlargest_n_proba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mmax_n_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m\"zeros\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mmax_n_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/dist-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     return um.add.reduce(a, axis=axis, dtype=dtype,\n\u001b[1;32m---> 25\u001b[1;33m                             out=out, keepdims=keepdims)\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: 'axis' entry is out of bounds"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "foo = np.asanyarray(range(5))\n",
      "foo1 = np.asanyarray(range(4))\n",
      "print foo\n",
      "print ((foo>2) & (foo1>31))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import PorterStemmer\n",
      "from nltk import RegexpTokenizer\n",
      "import numpy\n",
      "from scipy import sparse\n",
      "\n",
      "class StermmingTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stemmer = PorterStemmer()\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b[a-z\\\\-]+\\\\b')\n",
      "    def __call__(self, doc):\n",
      "        return [self.stemmer.stem(tokens.lower()) for tokens in self.tokenizer.tokenize(doc)]\n",
      "    \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "min_tf = 3\n",
      "vectorizer = CountVectorizer(tokenizer=StermmingTokenizer(),ngram_range=(1,1),stop_words={},min_df=2)#(min_df=0.000001,max_df=0.95)\n",
      "corpus = ['hi cairo here is CAIRON','here USA s 09.here in u-sa','hi in']\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "corpus_vectors_ = sparse.csr_matrix(corpus_vectors.shape,dtype=bool)#numpy.matrix(numpy.zeros(corpus_vectors.shape))\n",
      "# apply minimum term frequency threshold\n",
      "#corpus_vectors = corpus_vectors.tolil()\n",
      "term_freq = corpus_vectors.sum(axis=0) # sum on culomns to obtain term frequencies\n",
      "terms_to_remove = []\n",
      "row = 0\n",
      "for k,v in vectorizer.vocabulary_.iteritems():\n",
      "    if(term_freq[0,vectorizer.vocabulary_[k]]>=min_tf):\n",
      "        corpus_vectors_[row,v] = 1\n",
      "    else:\n",
      "        terms_to_remove.append(k)        \n",
      "    row = row + 1\n",
      "\n",
      "corpus_vectors = corpus_vectors * corpus_vectors_\n",
      "\n",
      "for k in terms_to_remove:\n",
      "    del vectorizer.vocabulary_[k]\n",
      "\n",
      "print corpus_vectors.shape\n",
      "print vectorizer.vocabulary_\n",
      "print vectorizer.stop_words_\n",
      "#tokenizer=StermmingTokenizer(),\n",
      "#token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import PorterStemmer\n",
      "from nltk import RegexpTokenizer\n",
      "import numpy\n",
      "from scipy import sparse\n",
      "\n",
      "class StermmingTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stemmer = PorterStemmer()\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b[a-z\\\\-]+\\\\b')\n",
      "    def __call__(self, doc):\n",
      "        return [self.stemmer.stem(tokens.lower()) for tokens in self.tokenizer.tokenize(doc)]\n",
      "    \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "corpus = ['hi cairo here is CAIRON','here USA s 09.here in u-sa','hi in']\n",
      "\n",
      "vectorizer = CountVectorizer(tokenizer=StermmingTokenizer(),ngram_range=(1,2),stop_words={})#(min_df=0.000001,max_df=0.95)\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "print vectorizer.vocabulary_\n",
      "\n",
      "vectorizer = CountVectorizer(tokenizer=StermmingTokenizer(),ngram_range=(1,2),stop_words={},vocabulary={'here in','here','in'})#(min_df=0.000001,max_df=0.95)\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "print vectorizer.vocabulary_\n",
      "\n",
      "#tokenizer=StermmingTokenizer(),\n",
      "#token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}